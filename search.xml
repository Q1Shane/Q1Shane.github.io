<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/16107.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><a id="more"></a>

<h2 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World!"></a>Hello World!</h2><h3 id="一个收藏回忆与分享技术的地方！"><a href="#一个收藏回忆与分享技术的地方！" class="headerlink" title="一个收藏回忆与分享技术的地方！"></a>一个收藏回忆与分享技术的地方！</h3>]]></content>
      <categories>
        <category>默认</category>
      </categories>
      <tags>
        <tag>默认</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo 写一篇博客</title>
    <url>/37981.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>欢迎来到 <a href="https://hexo.io/">Hexo</a>！这是您的第一篇文章。查看<a href="(https://hexo.io/docs/)">文档</a>以获取更多信息。如果在使用Hexo时遇到任何问题，可以在<a href="%5D(https://hexo.io/docs/troubleshooting.html)">故障排除</a>中找到答案，或者可以在<a href="https://github.com/hexojs/hexo/issues">GitHub</a>上问我。 </p>
<h2 id="快速开始"><a href="#快速开始" class="headerlink" title="快速开始"></a>快速开始</h2><h3 id="创建一篇新文章"><a href="#创建一篇新文章" class="headerlink" title="创建一篇新文章"></a>创建一篇新文章</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>更多信息：<a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="预览"><a href="#预览" class="headerlink" title="预览"></a>预览</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo server</span><br></pre></td></tr></table></figure>
<p>更多信息： <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="生成静态文件"><a href="#生成静态文件" class="headerlink" title="生成静态文件"></a>生成静态文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo generate</span><br></pre></td></tr></table></figure>
<p>更多信息： <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="部署到远程站点"><a href="#部署到远程站点" class="headerlink" title="部署到远程站点"></a>部署到远程站点</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo deploy</span><br></pre></td></tr></table></figure>
<p>更多信息： <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Blog</tag>
      </tags>
  </entry>
  <entry>
    <title>anaconda3的使用</title>
    <url>/32873.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><a id="more"></a>

<h2 id="1、安装-anaconda3"><a href="#1、安装-anaconda3" class="headerlink" title="1、安装 anaconda3"></a><strong>1、安装 anaconda3</strong></h2><h2 id="安装条件"><a href="#安装条件" class="headerlink" title="    安装条件"></a>    安装条件</h2><ul>
<li>  系统要求：32 位或 64 位系统均可</li>
<li>  下载文件大小：约 500MB</li>
<li>  所需空间大小：3GB 空间大小（Miniconda 仅需 400MB 空间即可）</li>
</ul>
<p>（1）地址 <a href="https://www.anaconda.com/products/individual#linux">anaconda 下载</a>。最新版本为 Anaconda3-2020.02-Linux-x86_64.sh，启动终端，输入如下指令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash ~&#x2F;Downloads&#x2F;Anaconda3-2020.02-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>
<p>（2）安装过程中，看到提示 “In order to continue the installation process, please review the license agreement.”（“请浏览许可证协议以便继续安装。”），点击“Enter” 查看 “许可证协议”，也可以点击空格翻页，这样快一些。在“许可证协议” 界面将屏幕滚动至底，输入 “yes” 表示同意许可证协议内容。然后进行下一步。</p>
<p>（3）安装过程中，提示 “Press Enter to accept the default install location, CTRL-C to cancel the installation or specify an alternate installation directory.”（“按回车键确认安装路径，按’CTRL-C’取消安装或者指定安装目录。”）可以指定目录，输入要安装的目录即可。</p>
<p>（4）安装器若提示 You currently have a PYTHONPATH environment variable set….. 等类似提示时：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200628222025241.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJRUpJTlFVQU5JTA==,size_16,color_FFFFFF,t_70"></p>
<p>这里是将 anaconda3 添加到环境变量中，这里十分建议选 no，否则安装完成后在终端前面始终会有个 (base) 存在，无论用不用 anaconda3。总之选 no 就对了！</p>
<p>        看到 Thank you for installing Anaconda3! 这个提示证明安装成功了！</p>
<h2 id="2、anaconda3-的使用"><a href="#2、anaconda3-的使用" class="headerlink" title="2、anaconda3 的使用"></a><strong>2、anaconda3 的使用</strong></h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd &#x2F;home&#x2F;zydz&#x2F;anaconda3&#x2F;bin</span><br><span class="line">$ . .&#x2F;activate</span><br></pre></td></tr></table></figure>
<p>         这两条指令是进入 conda 环境，其中第 2 条指令第一个点相当于 source，第二个点代表当前路径。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda list</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>         显示已经安装的包名和版本号。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">anaconda-navigator</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>         anaconda 交互界面。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda --version</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>          验证 conda 已被安装，查看 conda 版本号。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda info --envs</span><br><span class="line">$ conda info -e</span><br><span class="line">$ conda env list</span><br></pre></td></tr></table></figure>
<p>         这 3 条指令含义相同，显示已创建环境。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda --help</span><br><span class="line">$ conda -h</span><br></pre></td></tr></table></figure>
<p>         查看 conda 帮助信息。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda create --name &lt;env_name&gt; &lt;package_names&gt;</span><br><span class="line">$ conda create -n &lt;env_name&gt; &lt;package_names&gt;</span><br></pre></td></tr></table></figure>
<p>         创建新环境，以下举例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda create -n conda_test python&#x3D;3.5 numpy</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>         表示创建一个名为 conda_test 的环境，环境中安装版本为 3.5 的 python，同时也安装了 numpy。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda create --name &lt;new_env_name&gt; --clone &lt;copied_env_name&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>         复制环境，以下举例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda create --name new_conda --clone conda_test</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>         表示克隆名为 conda_test 的环境，克隆后的新环境名为 new_conda。此时，环境中将同时存在 conda_test 和 new_conda 环境，且两个环境的配置相同。 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda remove --name &lt;env_name&gt; --all</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>         删除环境，<env_name> 为被删除环境的名称。</env_name></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda search --full python</span><br><span class="line">$ conda search --full-name python</span><br></pre></td></tr></table></figure>
<p>         表示查找全名为 python 的包有哪些版本可供安装。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda install --name &lt;env_name&gt; &lt;package_name&gt;</span><br><span class="line">$ conda install --name conda_test pandas</span><br></pre></td></tr></table></figure>
<p>          表示在名为 conda_test 的环境中安装 pandas 包。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ pip install &lt;package_name&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>         当使用 conda install 无法进行安装时，可以使用 pip 进行安装。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda remove --name &lt;env_name&gt; &lt;package_name&gt;</span><br><span class="line">$ conda remove --name conda_test pandas</span><br></pre></td></tr></table></figure>
<p>         表示卸载名为 conda_test 环境中的 pandas 包。 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda remove &lt;package_name&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>         表示卸载当前环境中的包。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda activate &lt;env_name&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>          切换环境。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda deactivate</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>         退出环境至 root。</p>
<p>         如果在其他电脑上使用，你可以将已经配置好的 xxx 环境，复制到其他电脑上，仍然能够使用。</p>
<h2 id="3、卸载-anaconda3"><a href="#3、卸载-anaconda3" class="headerlink" title="3、卸载 anaconda3"></a><strong>3、卸载 anaconda3</strong></h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ rm -rf ~&#x2F;anaconda3</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>        即删除 Anaconda 的安装目录。根据安装的 Anaconda 版本选择相应的卸载命令。</p>
<h2 id="参考网址"><a href="#参考网址" class="headerlink" title="参考网址"></a><strong>参考网址</strong></h2><p><a href="https://www.jianshu.com/p/62f155eb6ac5">Anaconda 介绍、安装及使用教程</a></p>
]]></content>
      <categories>
        <category>Anaconda</category>
      </categories>
      <tags>
        <tag>毕设</tag>
      </tags>
  </entry>
  <entry>
    <title>Numpy基础用法</title>
    <url>/64862.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><a id="more"></a>

<p> 一个由国外大神制作的 NumPy 可视化教程，直观地介绍 NumPy 的各种用法，很容易就能理解。</p>
<p>话不多说，一睹为快。</p>
<h2 id="数组"><a href="#数组" class="headerlink" title="数组"></a><strong>数组</strong></h2><p>先来介绍最基础的一维数组。</p>
<h3 id="创建数组"><a href="#创建数组" class="headerlink" title="创建数组"></a><strong>创建数组</strong></h3><p>np.array() 直接创建：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/80/v2-90de7812f9fc3169cffb7e39d4c3cfd8_720w.jpg" alt="img"></p>
<p>使用 np.ones()、np.zeros() 等方法：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/80/v2-6e73db4bcf9e406d110da4f2827200ab_720w.jpg" alt="img"></p>
<p>我们在写数组的时候是横着写的，而其实数组是列向量，这样很直观。</p>
<h3 id="数组运算"><a href="#数组运算" class="headerlink" title="数组运算"></a><strong>数组运算</strong></h3><p>加减乘除</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/80/v2-ae2106c929707e5613c6d8b8bb7aab16_720w.png" alt="img"></p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/80/v2-651679ca10deeda9810c5a59de52bac6_720w.jpg" alt="img"></p>
<p>（注意这里的data和ones不是上面的data，ones）</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/80/v2-c2b47f98a0c1ae65eedd798bb222758d_720w.png" alt="img"></p>
<p>数组乘以数值</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/80/v2-e461f0c4e4a66622b487b71f22e3eb7b_720w.png" alt="img"></p>
<h3 id="数组索引"><a href="#数组索引" class="headerlink" title="数组索引"></a><strong>数组索引</strong></h3><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/80/v2-6ab0797b53e61645a9564376ae9f3d0c_720w.jpg" alt="img"></p>
<h3 id="数组聚合"><a href="#数组聚合" class="headerlink" title="数组聚合"></a><strong>数组聚合</strong></h3><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/80/v2-6e8ef73d761be46fdf60aefcf3953821_720w.jpg" alt="img"></p>
<p>上面是一维数组，下面介绍二维维数组也就是矩阵的使用技巧。</p>
<h2 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a><strong>矩阵</strong></h2><h3 id="创建矩阵"><a href="#创建矩阵" class="headerlink" title="创建矩阵"></a><strong>创建矩阵</strong></h3><p>直接创建：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/80/v2-58fe9b45561eed23069a721ec516279b_720w.jpg" alt="img"></p>
<p>使用 np.ones()、np.zeros() 等方法：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/80/v2-63b85180f9ac32626669e0cd662f5d92_720w.jpg" alt="img"></p>
<p>这样就很容易理解括号里 (3,2) 的含义。</p>
<h3 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a><strong>矩阵运算</strong></h3><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/80/v2-651679ca10deeda9810c5a59de52bac6_720w.jpg" alt="img"></p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/80/v2-b1a31f23c0cab9570a3f7b0d2af42031_720w.jpg" alt="img"></p>
<h3 id="矩阵点积"><a href="#矩阵点积" class="headerlink" title="矩阵点积"></a><strong>矩阵点积</strong></h3><p>矩阵点积跟线性代数基本一样，有些抽象，借助示意图能很好理解：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/80/v2-c3f791e0c27505216111dfe9d3f51c74_720w.jpg" alt="img"></p>
<p>进一步拆分解释：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/80/v2-ffeacb71407ec517f64294c30a962159_720w.jpg" alt="img"></p>
<h3 id="矩阵索引"><a href="#矩阵索引" class="headerlink" title="矩阵索引"></a><strong>矩阵索引</strong></h3><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/80/v2-45f4ce6a4ffd5f8af7ab6c7ba717720e_720w.jpg" alt="img"></p>
<h3 id="矩阵聚合"><a href="#矩阵聚合" class="headerlink" title="矩阵聚合"></a><strong>矩阵聚合</strong></h3><p>求最值</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/80/v2-fa64dda84ef6ac60c277446e5dea8029_720w.jpg" alt="img"></p>
<p>按行 / 列聚合</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/80/v2-e5dbfb6c39c9065351628b8e9f22ba35_720w.jpg" alt="img"></p>
<h3 id="矩阵转置"><a href="#矩阵转置" class="headerlink" title="矩阵转置"></a><strong>矩阵转置</strong></h3><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/80/v2-689c92bca6be7d624a473b41fa0fec9b_720w.jpg" alt="img"></p>
<h3 id="矩阵重塑"><a href="#矩阵重塑" class="headerlink" title="矩阵重塑"></a><strong>矩阵重塑</strong></h3><p>reshape() 用法：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/80/v2-676cd66b7083e7507ebec3b69c0b8d3f_720w.jpg" alt="img"></p>
<p>mark</p>
<h2 id="高维数组"><a href="#高维数组" class="headerlink" title="高维数组"></a><strong>高维数组</strong></h2><p>Numpy 不仅可以处理上述的一维数组和二维矩阵，还可以处理任意 N 维的数组，方法也大同小异。</p>
<h3 id="创建多维数组"><a href="#创建多维数组" class="headerlink" title="创建多维数组"></a><strong>创建多维数组</strong></h3><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/80/v2-00e2ef23fefed05864e66318b6c5d107_720w.jpg" alt="img"></p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/80/v2-e5bb94c075e23e20e58cea9e2df47cf1_720w.jpg" alt="img"></p>
<p>掌握了以上基础后，我们可以做个小练习，计算均方误差 MSE：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/80/v2-70d84cd6b6975b2757e5e164249b0cdb_720w.jpg" alt="img"></p>
<p>可以看到有减法、平方、求和等运算：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/80/v2-e52fd0fc193ad26494ee9ce35559753c_720w.jpg" alt="img"></p>
<p>分别假设相应的预测值和真实值：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/80/v2-0462ab48e91ca9081cc652fce298de6c_720w.jpg" alt="img"></p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/80/v2-fb39528fcfd3447639f203df41949b00_720w.jpg" alt="img"></p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/80/v2-a2ed39331e8841397d4908fc2157a46c_720w.jpg" alt="img"></p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/80/v2-56048efa2b5f2a10832388b69c050ebc_720w.jpg" alt="img"></p>
]]></content>
      <categories>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>毕设</tag>
      </tags>
  </entry>
  <entry>
    <title>卷积核（kernels）与滤波器（filters）的关系</title>
    <url>/45549.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><a id="more"></a>

<h2 id="简单理解："><a href="#简单理解：" class="headerlink" title="简单理解："></a><strong>简单理解：</strong></h2><p>卷积核：二维的矩阵<br>滤波器：多个卷积核组成的三维矩阵，多出的一维是通道。</p>
<p>先介绍一些术语：layers（层）、channels（通道）、feature maps（特征图），filters（滤波器），kernels（卷积核）。<br>从层次结构的角度来看，层和滤波器的概念处于同一水平，而通道和卷积核在下一级结构中。通道和特征图是同一个事情。一层可以有多个通道（或者说特征图）。如果输入的是一个 RGB 图像，那么就会有 3 个通道。“channel”通常被用来描述 “layer” 的结构。相似的，“kernel”是被用来描述 “filter” 的结构。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20190915145942491.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMzc1NjA5,size_16,color_FFFFFF,t_70"><br>filter 和 kernel 之间的不同很微妙。很多时候，它们可以互换，所以这可能造成我们的混淆。</p>
<h2 id="那它们之间的不同在于哪里呢？"><a href="#那它们之间的不同在于哪里呢？" class="headerlink" title="那它们之间的不同在于哪里呢？"></a><strong>那它们之间的不同在于哪里呢？</strong></h2><p>一个 “Kernel” 更倾向于是 2D 的权重矩阵。而 “filter” 则是指多个 Kernel 堆叠的 3D 结构。如果是一个 2D 的 filter，那么两者就是一样的。但是一个 3Dfilter，在大多数深度学习的卷积中，它是包含 kernel 的。每个卷积核都是独一无二的，主要在于强调输入通道的不同方面。</p>
<p>参考：[深度学习中的各种卷积](</p>
]]></content>
      <categories>
        <category>默认</category>
      </categories>
      <tags>
        <tag>默认</tag>
      </tags>
  </entry>
  <entry>
    <title>安装Go环境</title>
    <url>/59924.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><a id="more"></a>

<h1 id="安装-Go-语言及搭建-Go-语言开发环境"><a href="#安装-Go-语言及搭建-Go-语言开发环境" class="headerlink" title="安装 Go 语言及搭建 Go 语言开发环境"></a>安装 Go 语言及搭建 Go 语言开发环境</h1><hr>
<h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><h3 id="下载地址"><a href="#下载地址" class="headerlink" title="下载地址"></a>下载地址</h3><ul>
<li><p>Go 官网下载地址：<a href="https://link.zhihu.com/?target=https://golang.org/dl/">https://golang.org/dl/</a></p>
</li>
<li><p>Go 官方镜像站（推荐）：<a href="https://link.zhihu.com/?target=https://golang.google.cn/dl/">https://golang.google.cn/dl/</a></p>
</li>
</ul>
<h3 id="版本的选择"><a href="#版本的选择" class="headerlink" title="版本的选择"></a>版本的选择</h3><p>Windows 平台和 Mac 平台推荐下载可执行文件版，Linux 平台下载压缩文件版。</p>
<p><em>(目前的最新版本可能不是 1.11.5，但是安装过程类似哦。)</em></p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/v2-5b7945253d9729f6d6ceee5f5322072e_r.jpg"></p>
<h1 id="Windows-下安装Go环境"><a href="#Windows-下安装Go环境" class="headerlink" title="Windows 下安装Go环境"></a>Windows 下安装Go环境</h1><hr>
<p>此安装实例以 <code>64位Win10</code>系统安装 <code>Go1.11.5可执行文件版本</code>为例。</p>
<p>将上一步选好的安装包下载到本地。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/v2-86fc9e0126b5d7dd1b820abdda433745_r.jpg"></p>
<p>双击下载好的文件，然后再按照下图步骤安装即可。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/v2-7481d0f0ca16726c5ae9b72af4db7650_r.jpg"><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/v2-dbb182b02a84582bdc13a42f29257ecf_r.jpg"><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/v2-58d7a6598586f787446df4d1b6390216_r.jpg"></p>
<h1 id="Linux下安装Go环境"><a href="#Linux下安装Go环境" class="headerlink" title="Linux下安装Go环境"></a>Linux下安装Go环境</h1><hr>
<h2 id="登录-Linux"><a href="#登录-Linux" class="headerlink" title="登录 Linux"></a>登录 Linux</h2><p>Mac 或 Linux 的用户可以用命令<code>ssh root@xxx.xxx.xxx.xxx</code>登录主机<br>Window 的用户可以使用 SecureCRT 登录主机<br>虚拟机用户直接打开你的虚拟机</p>
<h2 id="安装-Go-环境"><a href="#安装-Go-环境" class="headerlink" title="安装 Go 环境"></a>安装 Go 环境</h2><blockquote>
<p>Golang 官网下载地址：<a href="https://golang.org/dl/">https://golang.org/dl/</a></p>
</blockquote>
<ol>
<li>打开官网下载地址选择对应的系统版本, 复制下载链接<br> 这里我选择的是<br> <code>go1.11.5.linux-amd64.tar.gz</code>：<a href="https://dl.google.com/go/go1.11.5.linux-amd64.tar.gz">https://dl.google.com/go/go1.11.5.linux-amd64.tar.gz</a></li>
</ol>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="http://upload-images.jianshu.io/upload_images/1277729-7d5e14c2c8c994bf.png"> image.png</p>
<ol start="2">
<li> <code>cd</code>进入你用来存放安装包的目录，我习惯在<code>~</code>下面创建个<code>go</code>文件夹。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在 ~ 下创建 go 文件夹，并进入 go 文件夹</span><br><span class="line">mkdir ~&#x2F;go &amp;&amp; cd ~&#x2F;go</span><br><span class="line">下载的 go 压缩包</span><br><span class="line">wget https:&#x2F;&#x2F;dl.google.com&#x2F;go&#x2F;go1.11.5.linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="3">
<li> 下载完成</li>
</ol>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="http://upload-images.jianshu.io/upload_images/1277729-60625ab3615c752e.png"></p>
<ol start="4">
<li> 执行<code>tar</code>解压到<code>/usr/loacl</code>目录下（官方推荐），得到<code>go</code>文件夹等</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -C &#x2F;usr&#x2F;local -zxvf  go1.11.5.linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="5">
<li> 添加<code>/usr/loacl/go/bin</code>目录到 PATH 变量中。添加到<code>/etc/profile</code> 或<code>$HOME/.profile</code>都可以</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 习惯用vim，没有的话可以用命令&#96;sudo apt-get install vim&#96;安装一个</span><br><span class="line">vim &#x2F;etc&#x2F;profile</span><br><span class="line"># 在最后一行添加</span><br><span class="line">export GOROOT&#x3D;&#x2F;usr&#x2F;local&#x2F;go</span><br><span class="line">export PATH&#x3D;$PATH:$GOROOT&#x2F;bin</span><br><span class="line"># 保存退出后source一下（vim 的使用方法可以自己搜索一下）</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="6">
<li>执行<code>go version</code>，如果现实版本号，则 Go 环境安装成功。是不是很简单呢？<br> <img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="http://upload-images.jianshu.io/upload_images/1277729-bd36e8e538989c62.png"></li>
</ol>
<h2 id="运行第一个程序"><a href="#运行第一个程序" class="headerlink" title="运行第一个程序"></a>运行第一个程序</h2><ol>
<li> 先创建你的工作空间 (<a href="%5Bworkspace%5D(https://golang.org/doc/code.html#Workspaces)">Workspaces</a>)，官方建议目录<code>$HOME/go</code>。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir $HOME&#x2F;go</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="2">
<li> 将你的工作空间路径声明到环境变量中。和上一部分的第 5 步相似。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 编辑 ~&#x2F;.bash_profile 文件</span><br><span class="line">vim ~&#x2F;.bash_profile</span><br><span class="line"># 在最后一行添加下面这句。$HOME&#x2F;go 为你工作空间的路径，你也可以换成你喜欢的路径</span><br><span class="line">export GOPATH&#x3D;$HOME&#x2F;go</span><br><span class="line"># 保存退出后source一下（vim 的使用方法可以自己搜索一下）</span><br><span class="line">source ~&#x2F;.bash_profile</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="3">
<li> 在你的工作空间创建你的第一个工程目录</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建并进入你的第一个工程目录</span><br><span class="line">mkdir -p $GOPATH&#x2F;src&#x2F;hello &amp;&amp; cd $GOPATH&#x2F;src&#x2F;hello</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="4">
<li> 在你的工程目录下创建名为<code>hello.go</code>的文件</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim hello.go</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="5">
<li> 将下面内容粘贴到 hello.go 文件</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import &quot;fmt&quot;</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">    fmt.Printf(&quot;hello, world\n&quot;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="6">
<li> 好了，工程目录和工程文件都准备好了。现在我们到我们的工程目录 (<code>$GOPATH/src/hello</code>) 下构建我们的工程</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 如果你当前的目录不在 $GOPATH&#x2F;src&#x2F;hello， 需要先执行 &quot;cd $GOPATH&#x2F;src&#x2F;hello&quot; 进入该目录</span><br><span class="line"># 执行构建工程的命令</span><br><span class="line">go build</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="7">
<li> 等一会，命令执行完之后你可以看到目录下会多出一个 hello 的文件，这就是我们编译之后的文件啦。怎么执行我们的程序呢？只需要在当前目录下执行<code>./xxx</code>就可以啦！是不是敲鸡煎蛋呢！</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;hello</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="http://upload-images.jianshu.io/upload_images/1277729-94425080feff1bbd.png"></p>
<h1 id="Mac-下安装Go环境"><a href="#Mac-下安装Go环境" class="headerlink" title="Mac 下安装Go环境"></a>Mac 下安装Go环境</h1><hr>
<p>下载可执行文件版，直接点击<strong>下一步</strong>安装即可，默认会将 go 安装到<code>/usr/local/go</code>目录下。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/v2-45081f19819b3c3a69e4c78fed850d47_r.jpg"></p>
<h2 id="检查"><a href="#检查" class="headerlink" title="检查"></a>检查</h2><p>上一步安装过程执行完毕后，可以打开终端窗口，输入<code>go version</code>命令，查看安装的 Go 版本。</p>
<h1 id="关于-Go-的一些介绍"><a href="#关于-Go-的一些介绍" class="headerlink" title="关于 Go 的一些介绍"></a>关于 Go 的一些介绍</h1><hr>
<h2 id="环境变量："><a href="#环境变量：" class="headerlink" title="环境变量："></a>环境变量：</h2><ul>
<li>$GOROOT:<br>  表示 Go 的安装目录。也就是上面我们解压出来的文件夹里面的<code>go</code>文件夹。</li>
<li>$GOPATH:<br>  表示我们的工作空间。用来存放我们的工程目录的地方。</li>
</ul>
<h2 id="GOPATH-目录："><a href="#GOPATH-目录：" class="headerlink" title="GOPATH 目录："></a>GOPATH 目录：</h2><p>一般来说 GOPATH 下面会有三个文件夹：<code>bin</code>、<code>pkg</code>、<code>src</code>，没有的话自己创建。每个文件夹都有其的作用。</p>
<ul>
<li>  bin：编译后可的执行文件的存放路径</li>
<li>  pkg：编译包时，生成的. a 文件的存放路径</li>
<li>  src：源码路径，一般我们的工程就创建在<code>src</code>下面。</li>
</ul>
<p>注意：如果要用<code>Go Mod</code>(Go1.11 及以上支持) 进行包管理，则需要在 GOPATH 以外的目录创建工程。关于<code>Go Mod</code>的使用，可以自行 Google 一下，这里就不赘述了。</p>
<h2 id="配置-GOPATH"><a href="#配置-GOPATH" class="headerlink" title="配置 GOPATH"></a>配置 GOPATH</h2><p><code>GOPATH</code>是一个环境变量，用来表明你写的 go 项目的存放路径（工作目录），本文就选择使用 <code>D:\code\go</code> 这个目录作为 <code>GOPATH</code> 目录。</p>
<p><code>GOPATH</code>路径最好只设置一个，我们写的所有 Go 项目代码都放到<code>GOPATH</code>的<code>src</code>目录下。</p>
<p><em>补充说明：Go1.11 版本之后，开启<code>go mod</code>模式之后就不再强制需要配置<code>GOPATH</code>了。</em></p>
<p>Linux 和 Mac 平台就参照上面配置环境变量的方式将自己的工作目录添加到环境变量中即可。</p>
<p>Windows 平台按下面的步骤将<code>D:\code\go</code>添加到环境变量：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/v2-5e4fa7826f177c0d3e8aaaf645559fd6_r.jpg"><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/v2-07f79095623e958b07a518dcb35cef24_r.jpg"><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/v2-c4f08c411d25b622f3577dd5cf72196f_r.jpg"><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/v2-4a875a0e2dca47c4c813aae850a5ed2a_r.jpg"><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/v2-d563ad97c44aaf2adfa4b3231b0ee834_r.jpg"><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/v2-f04b1d2b9f17c73675c07b98b2b4f21d_r.jpg"><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/v2-00eef37d36fe47cd68c8cd0e320a7405_r.jpg"></p>
<p>在 Go 1.8 版本之前，<code>GOPATH</code>环境变量默认是空的。从 Go 1.8 版本开始，Go 开发包在安装完成后会为 <code>GOPATH</code>设置一个默认目录，参见下图。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/v2-b8e098f6ce03c4d2947f6b3dd33c03eb_r.jpg"></p>
<p>同时，我们将 <code>GOROOT</code>下的 bin 目录及<code>GOPATH</code>下的 bin 目录都添加到环境变量中。</p>
<p>配置环境变量之后需要重启已经打开的终端才能让新配置的环境变量生效。（例如 cmd、VS Code 里面的终端和其他编辑器的终端等）。</p>
<h2 id="Go-项目结构"><a href="#Go-项目结构" class="headerlink" title="Go 项目结构"></a>Go 项目结构</h2><p>在进行 Go 语言开发的时候，我们的代码总是会保存在<code>$GOPATH/src</code>目录下。在工程经过<code>go build</code>、<code>go install</code>或<code>go get</code>等指令后，会将下载的第三方包源代码文件放在<code>$GOPATH/src</code>目录下， 产生的二进制可执行文件放在 <code>$GOPATH/bin</code>目录下，生成的中间缓存文件会被保存在 <code>$GOPATH/pkg</code> 下。</p>
<p>如果我们使用版本管理工具（Version Control System，VCS。常用如 Git）来管理我们的项目代码时，我们只需要添加<code>$GOPATH/src</code>目录的源代码即可。<code>bin</code> 和 <code>pkg</code> 目录的内容无需版本控制。</p>
<h3 id="适合个人开发者"><a href="#适合个人开发者" class="headerlink" title="适合个人开发者"></a>适合个人开发者</h3><p>我们知道源代码都是存放在<code>GOPATH</code>的<code>src</code>目录下，那我们可以按照下图来组织我们的代码。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/v2-8d441d70a1caebb5f69197686a3b1e5f_r.jpg"></p>
<h3 id="目前流行的项目结构"><a href="#目前流行的项目结构" class="headerlink" title="目前流行的项目结构"></a>目前流行的项目结构</h3><p>Go 语言中也是通过包来组织代码文件，我们可以引用别人的包也可以发布自己的包，但是为了防止不同包的项目名冲突，我们通常使用<code>顶级域名</code>来作为包名的前缀，这样就不担心项目名冲突的问题了。</p>
<p>因为不是每个个人开发者都拥有自己的顶级域名，所以目前流行的方式是使用个人的 github 用户名来区分不同的包。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/v2-01248b3f1900f995a437395cdec5e756_r.jpg"></p>
<p>举个例子：张三和李四都有一个名叫<code>studygo</code>的项目，那么这两个包的路径就会是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import &quot;github.com&#x2F;zhangsan&#x2F;studygo&quot;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>和</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import &quot;github.com&#x2F;lisi&#x2F;studygo&quot;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>以后我们从 github 上下载别人包的时候，如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">go get github.com&#x2F;jmoiron&#x2F;sqlx</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>那么，这个包会下载到我们本地<code>GOPATH</code>目录下的<code>src/github.com/jmoiron/sqlx</code>。</p>
<h3 id="适合企业开发者"><a href="#适合企业开发者" class="headerlink" title="适合企业开发者"></a>适合企业开发者</h3><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/v2-da6bd50401d8c030f1b79e9b3131ed04_r.jpg"></p>
<h1 id="Goland-官网下载"><a href="#Goland-官网下载" class="headerlink" title="Goland 官网下载"></a>Goland 官网下载</h1><hr>
<p>Goland 官网地址：<a href="https://www.jetbrains.com/go/specials/go/go.html?gclid=EAIaIQobChMI9Y2H-vDV4AIV2CCtBh0nrAPYEAAYASAAEgJ-cfD_BwE&gclsrc=aw.ds">https://www.jetbrains.com/go/specials/go/go.html?gclid=EAIaIQobChMI9Y2H-vDV4AIV2CCtBh0nrAPYEAAYASAAEgJ-cfD_BwE&amp;gclsrc=aw.ds</a></p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/2019022522171329.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuZ2VsaWE2MjA=,size_16,color_FFFFFF,t_70"></p>
<h2 id="配置-GoLand-的-GOPATH-和-GOROOT"><a href="#配置-GoLand-的-GOPATH-和-GOROOT" class="headerlink" title="配置 GoLand 的 GOPATH 和 GOROOT"></a>配置 GoLand 的 GOPATH 和 GOROOT</h2><p>打开 Goland 软件，点击右下角 Configure，然后点击 Setting，如下图所示：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20190225222635750.png"></p>
<p>在打开的界面中，点击 GO 列表，然后找到 GOPATH 和 GOROOT</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20190225222831313.png"></p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20190225222847786.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuZ2VsaWE2MjA=,size_16,color_FFFFFF,t_70"></p>
<h2 id="测试软件是否安装成功"><a href="#测试软件是否安装成功" class="headerlink" title="测试软件是否安装成功"></a>测试软件是否安装成功</h2><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20190225223126694.png"></p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20190225223141314.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuZ2VsaWE2MjA=,size_16,color_FFFFFF,t_70"></p>
]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习(1)——特征工程</title>
    <url>/32820.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h3 id="机器学习的流程"><a href="#机器学习的流程" class="headerlink" title="机器学习的流程"></a>机器学习的流程</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/32820/1.png" style="zoom:25%;">

<center>预处理  ——> 特征工程  ——> 机器学习  ——> 模型评估</center>

<p>如果未达到要求，重新循环</p>
<h3 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h3><p>将<strong>原始数据</strong>转换为更好地<strong>代表预测模型的潜在问题的特征</strong>的工程，从而提高了模型对位置数据预测的准确性（ <strong>属于数据预处理阶段的工作</strong>）</p>
<p>以scikit-learn为例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer, TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler, StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dictvec</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    字典数据抽取</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 实例化</span></span><br><span class="line">    <span class="comment"># sparse改为True,输出的是每个不为零位置的坐标，稀疏矩阵可以节省存储空间</span></span><br><span class="line">    <span class="built_in">dict</span> = DictVectorizer(sparse=<span class="literal">False</span>)  <span class="comment"># 把sparse改为True看看</span></span><br><span class="line">    <span class="comment">#矩阵中存在大量的0，sparse存储只记录非零位置，节省空间</span></span><br><span class="line">    <span class="comment"># 调用fit_transform</span></span><br><span class="line">    data = <span class="built_in">dict</span>.fit_transform([&#123;<span class="string">&#x27;city&#x27;</span>: <span class="string">&#x27;北京&#x27;</span>, <span class="string">&#x27;temperature&#x27;</span>: <span class="number">100</span>&#125;,</span><br><span class="line">                               &#123;<span class="string">&#x27;city&#x27;</span>: <span class="string">&#x27;上海&#x27;</span>, <span class="string">&#x27;temperature&#x27;</span>: <span class="number">60</span>&#125;,</span><br><span class="line">                               &#123;<span class="string">&#x27;city&#x27;</span>: <span class="string">&#x27;深圳&#x27;</span>, <span class="string">&#x27;temperature&#x27;</span>: <span class="number">30</span>&#125;])</span><br><span class="line">    print(data)</span><br><span class="line">    print(<span class="string">&#x27;-&#x27;</span> * <span class="number">50</span>)</span><br><span class="line">    print(<span class="built_in">dict</span>.get_feature_names_out())  <span class="comment"># 字典中的一些类别数据，分别进行转换成特征</span></span><br><span class="line">    print(<span class="string">&#x27;-&#x27;</span> * <span class="number">50</span>)</span><br><span class="line">    print(<span class="built_in">dict</span>.inverse_transform(data))  <span class="comment">#去看每个特征代表的含义，逆转回去</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>DictVectorizer.fit_transform：（在训练集上）将字典数据<strong>转换</strong>为<strong>特征值</strong>数组（默认是one-hot编码）</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/32820/2.png" style="zoom:50%;">

<p>DictVectorizer.inverse_transform：上述过程逆转换</p>
<p>sklearn.feature_extraction.text.CountVectorizer：对文本进行特征值化,单个汉字单个字母不统计，因为单个汉字字母没有意义</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countvec</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对文本进行特征值化,单个汉字单个字母不统计，因为单个汉字字母没有意义,(“我”没有统计)</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    cv = CountVectorizer()</span><br><span class="line">    data = cv.fit_transform([<span class="string">&quot;人生苦短，我 喜欢 python python&quot;</span>, <span class="string">&quot;人生漫长，不用 python&quot;</span>])</span><br><span class="line"></span><br><span class="line">    print(cv.get_feature_names())</span><br><span class="line">    print(data)</span><br><span class="line">    print(data.toarray())</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">countvec()</span><br></pre></td></tr></table></figure>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/32820/3.png" style="zoom:50%;">

<h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><p>如果某个词或短语在一篇文章中出现的概率高,并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。</p>
<p>​        <strong>sklearn.feature extraction.text.TfidfVectorizer</strong></p>
<p>Tf：term frequency:词的频率出现的次数<br>idf：逆文档频率inverse document frequency：log(总文档数量/该词出现的文档数量)<br>tf*idf 来代表重要性程度</p>
<h4 id="（1）TF是词频-Term-Frequency"><a href="#（1）TF是词频-Term-Frequency" class="headerlink" title="（1）TF是词频(Term Frequency)"></a><strong>（1）TF是词频(Term Frequency)</strong></h4><p>        <strong>词频（TF）**</strong>表示词条（关键字）在文本中出现的频率**。</p>
<p>        这个数字通常会被归一化(一般是词频除以文章总词数), 以防止它偏向长的文件。</p>
<p>        <strong>公式：<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/32820/4.png" style="zoom:50%;"></strong>           **即：<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/32820/5.png" style="zoom:50%;"></p>
<p>其中 <strong>ni,j</strong> 是该词在文件 <strong>dj</strong> 中出现的次数，分母则是文件 dj 中所有词汇出现的次数总和；</p>
<h4 id="（2）-IDF是逆向文件频率-Inverse-Document-Frequency"><a href="#（2）-IDF是逆向文件频率-Inverse-Document-Frequency" class="headerlink" title="（2） IDF是逆向文件频率(Inverse Document Frequency)"></a><strong>（2） IDF是逆向文件频率(Inverse Document Frequency)</strong></h4><p>        <strong>逆向文件频率 (IDF)</strong> ：某一特定词语的IDF，可以由<strong>总文件数目除以包含该词语的文件的数目</strong>，<strong>再将得到的商取对数得到</strong>。</p>
<p>如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。</p>
<p>      <strong>公式：<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/32820/6.png" style="zoom:50%;"></strong> </p>
<p>其中，**|D|** <strong>是语料库中的文件总数</strong>。 <strong>|{j:ti∈dj}| 表示包含词语 ti 的文件数目</strong>（即 ni,j≠0 的文件数目）。如果该词语不在语料库中，就会导致分母为零，因此<strong>一般情况下使用 1+|{j:ti∈dj}|</strong></p>
<p> <strong>即：<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/32820/7.png" style="zoom:50%;"></strong></p>
<h4 id="（3）TF-IDF实际上是：TF-IDF"><a href="#（3）TF-IDF实际上是：TF-IDF" class="headerlink" title="*（3）TF-IDF实际上是：TF * IDF*"></a>*<em>（3）TF-IDF实际上是：TF * IDF*</em></h4><p>       某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。</p>
<p>       <strong>公式：<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/32820/8.png" style="zoom:50%;"></strong></p>
<p><strong>注：</strong>  TF-IDF算法非常容易理解，并且很容易实现，但是其简单结构并没有考虑词语的语义信息，无法处理一词多义与一义多词的情况。</p>
<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-11-——逻辑回归</title>
    <url>/30190.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>逻辑回归（Logistic Regression）是机器学习中的一种分类模型，逻辑回归是一种分类算法，虽然名字中带有回归，但是它与回归之间有一定的联系。由于算法的简单和高效，在实际中应用非常广泛。<strong>是解决二分类问题的利器。</strong></p>
<h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/1.png" style="zoom:40%;">

<p>逻辑回归的输入就是一个线性回归的结果。</p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>sigmoid函数</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/2.png" style="zoom:50%;">

<p>逻辑回归公式：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/3.png" style="zoom:60%;">

<p>输出：[0,1]区间的概率值，默认0.5作为阀值<br>注：g(z)为sigmoid函数<br>e: 2.71<br>Z =回归的结果</p>
<p><strong>逻辑回归即使用线性回归的思想去做二分类问题，也即将分类问题转化为概率问题，用sigmoid的目的是为了保证每一处都是连续可导的。</strong></p>
<p>输出结果解释(重要)：假设有两个类别A，B，并且假设我们的概率值为属于A(1)这个类别的概率值。现在有一个样本的输入到逻辑回归输出结果0.6，那么这个概率值超过0.5，意味着我们训练或者预测的结果就是A(1)类别。那么反之，如果得出结果为0.3那么，训练或者预测结果就为B(0)类别。</p>
<p>所以接下来我们回忆之前的线性回归预测结果我们用均方误差衡量，那如果对于逻辑回归，我们预测的结果不对该怎么去衡量这个损失呢？我们来看这样一张图</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/4.png" style="zoom:50%;">

<p>那么如何去衡量逻辑回归的预测结果与真实结果的差异呢？</p>
<h3 id="损失以及优化"><a href="#损失以及优化" class="headerlink" title="损失以及优化"></a>损失以及优化</h3><p>逻辑回归的损失，称之为<strong>对数似然损失</strong>，与线性回归原理相同，但由于是分类问题，损失函数不一样，只能通过梯度下降求解。公式如下：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/5.png" style="zoom:60%;">

<p>完整的损失函数：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/6.png" style="zoom:60%;">

<ul>
<li>cost损失的值越小，那么预测的类别准确度更高</li>
<li>yi就是真实值</li>
</ul>
<p>怎么理解单个的式子呢？这个要根据log的函数图像来理解</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/7.png" style="zoom:60%;">

<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/8.png" style="zoom:50%;">

<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/9.png" style="zoom:47%;">

<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/10.png" style="zoom:49%;">

<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/11.png" style="zoom:49%;">

<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/12.png" style="zoom:49%;">

<h3 id="逻辑回归api"><a href="#逻辑回归api" class="headerlink" title="逻辑回归api"></a>逻辑回归api</h3><ul>
<li>sklearn.linear_model.LogisticRegression(solver=’liblinear’, penalty=‘l2’, C = 1.0)<ul>
<li>solver：优化算法的选择，可选参数:{‘liblinear’, ‘sag’, ‘saga’,’newton-cg’, ‘lbfgs’}，<ul>
<li>默认: ‘liblinear’；用于优化问题的算法。</li>
<li>对于小数据集来说，“liblinear”是个不错的选择，而“sag”和’saga’对于大型数据集会更快。</li>
<li>对于多类问题，只有’newton-cg’， ‘sag’， ‘saga’和’lbfgs’可以处理多项损失;“liblinear”仅限于“one-versus-rest”分类。</li>
</ul>
</li>
<li>penalty：正则化项的类型，L1或L2正则化</li>
<li>C：正则化强度的倒数，C越小表示正则化强度越高。C越大，高阶项系数越小。</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>默认将类别数量少的当做正例</strong></p>
</blockquote>
<p>LogisticRegression方法相当于 SGDClassifier(loss=”log”, penalty=” “),SGDClassifier实现了一个普通的随机梯度下降学习。而使用LogisticRegression(实现了SAG)</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>应用: 广告点击率预测、电商购物搭配推荐，是否患病</li>
<li>优点: 适合需要得到一个分类概率的场景，简单，速度快</li>
<li>缺点: 当特征空间很大时，逻辑回归的性能不是很好(看硬件能力)，不好处理多分类问题</li>
</ul>
<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-10-——拟合与正则化</title>
    <url>/6552.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="欠拟合和过拟合"><a href="#欠拟合和过拟合" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><ul>
<li>过拟合：一个假设<strong>在训练数据上能够获得比其他假设更好的拟合， 但是在测试数据集上却不能很好地拟合数据</strong>，此时认为这个假设出现了过拟合的现象。(模型过于复杂)</li>
<li>欠拟合：一个假设<strong>在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好地拟合数据</strong>，此时认为这个假设出现了欠拟合的现象。(模型过于简单)</li>
</ul>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/1.png" style="zoom:40%;">

<h3 id="原因以及解决办法"><a href="#原因以及解决办法" class="headerlink" title="原因以及解决办法"></a>原因以及解决办法</h3><ul>
<li>欠拟合原因以及解决办法<ul>
<li>原因：学习到数据的特征过少</li>
<li>解决办法：<ul>
<li><strong>1）添加其他特征项，</strong>有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。例如，“组合”、“泛化”、“相关性”三类特征是特征添加的重要手段，无论在什么场景，都可以照葫芦画瓢，总会得到意想不到的效果。除上面的特征之外，“上下文特征”、“平台特征”等等，都可以作为特征添加的首选项。</li>
<li><strong>2）添加多项式特征</strong>，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。</li>
</ul>
</li>
</ul>
</li>
<li>过拟合原因以及解决办法<ul>
<li>原因：原始特征过多，存在一些嘈杂特征， 模型过于复杂是因为模型尝试去兼顾各个测试数据点</li>
<li>解决办法：<ul>
<li>1）重新清洗数据，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。</li>
<li>2）增大数据的训练量，还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。</li>
<li><strong>3）正则化</strong></li>
<li>4）减少特征维度，防止<strong>维灾难</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><h3 id="什么是正则化"><a href="#什么是正则化" class="headerlink" title="什么是正则化"></a>什么是正则化</h3><p>在解决回归过拟合中，我们选择正则化。但是对于其他机器学习算法如分类算法来说也会出现这样的问题，除了一些算法本身作用之外（决策树、神经网络），我们更多的也是去自己做特征选择，包括之前说的删除、合并一些特征</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/2.png" style="zoom:50%;">

<p><strong>在学习的时候，数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽量减少这个特征的影响（甚至删除某个特征的影响），这就是正则化</strong></p>
<p>注：调整时候，算法并不知道某个特征影响，而是去调整参数得出优化的结果</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/3.png" style="zoom:40%;">

<h3 id="正则化类别"><a href="#正则化类别" class="headerlink" title="正则化类别"></a>正则化类别</h3><ul>
<li><strong>L2正则化</strong><ul>
<li>作用：可以使得其中一些W的都很小，都接近于0，削弱某个特征的影响</li>
<li>优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象</li>
<li><strong>Ridge回归(岭回归)</strong></li>
</ul>
</li>
<li><strong>L1正则化</strong><ul>
<li>作用：可以使得其中一些W的值直接为0，删除这个特征的影响</li>
<li><strong>LASSO回归</strong></li>
</ul>
</li>
</ul>
<h3 id="L1-与-L2-的区别"><a href="#L1-与-L2-的区别" class="headerlink" title="L1 与 L2 的区别"></a>L1 与 L2 的区别</h3><p>L1正则化产生稀疏的权值,L2正则化产生平滑的权值<br><strong>L1正则化偏向于稀疏，它会自动进行特征选择，去掉一些没用的特征，也就是将这些特征对应的权重置为0</strong>.<br>L2主要功能是为了防止过拟合，<strong>当要求参数越小时，说明模型越简单，而模型越简单则，越趋向于平滑</strong>，从而防止过拟合。</p>
<p><strong>正则化力度:</strong><br><strong>大: 参数趋近于0</strong><br><strong>小: 参数变化小(高阶项权重没变)</strong></p>
<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>欠拟合与过拟合</tag>
        <tag>L1和L2正则化</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-12-——聚类</title>
    <url>/27590.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><h3 id="聚类算法的概念"><a href="#聚类算法的概念" class="headerlink" title="聚类算法的概念"></a>聚类算法的概念</h3><p>一种典型的<strong>无监督</strong>学习算法，主要用于将相似的样本自动归到一个类别中。</p>
<p>在聚类算法中根据样本之间的相似性，将样本划分到不同的类别中，对于不同的相似度计算方法，会得到不同的聚类结果，常用的相似度计算方法有欧式距离法。</p>
<h3 id="聚类算法实现流程"><a href="#聚类算法实现流程" class="headerlink" title="聚类算法实现流程"></a>聚类算法实现流程</h3><p><strong>k-means</strong>其实包含两层内容：</p>
<p> K : 初始中心点个数（计划聚类数）<br>means：求中心点到其他数据点距离的平均值</p>
<p>k-means聚类步骤：</p>
<ul>
<li>1、随机设置K个特征空间内的点作为初始的聚类中心</li>
<li>2、对于其他每个点计算到K个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别</li>
<li>3、接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值）</li>
<li>4、如果计算得出的新中心点与原中心点一样（质心不再移动），那么结束，否则重新进行第二步过程</li>
</ul>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/27590/1.png" style="zoom:40%;">

<h3 id="案例练习"><a href="#案例练习" class="headerlink" title="案例练习"></a>案例练习</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/27590/2.png" style="zoom:50%;">

<p>1、随机设置K个特征空间内的点作为初始的聚类中心（本案例中设置p1和p2）</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/27590/3.png" style="zoom:50%;">

<p>2、对于其他每个点计算到K个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/27590/4.png" style="zoom:50%;">

<p>3、接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值）</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/27590/5.png" style="zoom:50%;">

<p>4、如果计算得出的新中心点与原中心点一样（质心不再移动），那么结束，否则重新进行第二步过程【经过判断，需要重复上述步骤，开始新一轮迭代】</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/27590/6.png" style="zoom:50%;">

<p>5、当每次迭代结果不变时，认为算法收敛，聚类完成，<strong>K-Means一定会停下，不可能陷入一直选质心的过程。</strong></p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/27590/7.png" style="zoom:50%;">

<h3 id="api介绍"><a href="#api介绍" class="headerlink" title="api介绍"></a>api介绍</h3><ul>
<li>sklearn.cluster.KMeans(n_clusters=8)<ul>
<li>参数:<ul>
<li>n_clusters:开始的聚类中心数量<ul>
<li>整型，缺省值=8，生成的聚类数，即产生的质心（centroids）数。</li>
</ul>
</li>
</ul>
</li>
<li>方法:<ul>
<li>estimator.fit(x)</li>
<li>estimator.predict(x)</li>
<li>estimator.fit_predict(x)<ul>
<li>计算聚类中心并预测每个样本属于哪个类别,相当于先调用fit(x),然后再调用predict(x)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Kmeans性能评估指标"><a href="#Kmeans性能评估指标" class="headerlink" title="Kmeans性能评估指标"></a>Kmeans性能评估指标</h3><p>轮廓系数：</p>
<p>结合了聚类的凝聚度（Cohesion）和分离度（Separation），用于评估聚类的效果：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/27590/8.png" style="zoom:65%;">

<p>注：对于每个点i 为已聚类数据中的样本 ，<strong>bi 为i 到最近族群的所有样本的平均距离</strong>，<strong>ai 为i 到本身簇的距离平均值</strong>，<strong>max是bi或者ai的中较大的那个</strong>！最终计算出所有的样本点的轮廓系数平均值</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/27590/9.png" style="zoom:50%;">

<p>对于每一个样本<br>1、计算蓝1到自身类别的点距离的平均值a_i<br>2、计算蓝1分别到红色类别，绿色类别所有的点的距离，求出平均值b1, b2，取其中最小的值当做b_i<br><strong>极端：–这样容易理解</strong><br>b_ i&gt;&gt;a_ i: 1完美<br>a_ i&gt;&gt;b_ i:-1最差<br>蓝1:轮廓系数[-1, 1]<br>超过0.1，说明聚类效果很好—经验</p>
<ul>
<li>sklearn.metrics.silhouette_score(X, labels)<ul>
<li>计算所有样本的平均轮廓系数</li>
<li>X：特征值</li>
<li>labels：被聚类标记的目标值</li>
</ul>
</li>
</ul>
<p><strong>流程</strong>:</p>
<ul>
<li>事先<strong>确定常数K</strong>，常数K意味着最终的聚类类别数;</li>
<li>首先随机<strong>选定初始点为质心</strong>，并通过计算每一个样本与质心之间的相似度(这里为欧式距离)，将样本点归到最相似的类中，</li>
<li>接着，<strong>重新计算</strong>每个类的质心(即为类中心)，重复这样的过程，直到<strong>质心不再改变</strong>，</li>
<li>最终就确定了每个样本所属的类别以及每个类的质心。</li>
</ul>
<p><strong>注意</strong>:</p>
<ul>
<li>由于每次都要计算所有的样本与每一个质心之间的相似度，故在大规模的数据集上，K-Means算法的收敛速度比较慢。</li>
</ul>
<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>无监督学习</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习(3)——数据压缩</title>
    <url>/6552.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="数据压缩"><a href="#数据压缩" class="headerlink" title="数据压缩"></a>数据压缩</h2><h3 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h3><p>主成分分析（Principal Component Analysis, PCA） 也称为 卡尔胡宁-勒夫变换（Karhunen-Loeve Transform），是一种用于探索高维数据结构的技术。PCA 通常用于高维数据集的探索与可视化；还可以用于数据压缩，数据预处理等。PCA 可以把可能具有相关性的高维变量合成线性无关的低维变量，称为主成分（ principal components）。新的低维数据集会尽可能的保留原始数据的变量。</p>
<p>作用：可以<strong>削减回归分析</strong>或者<strong>聚类分析</strong>中<strong>特征的数量</strong>，特征数量达到<strong>上百</strong>的时候，开始使用PCA</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/1.png" style="zoom:50%;">

<p>如图所示，第四张二维的平面图，最能体现出其三维的特性，因此，我们可以使用PCA，将三维的数据，转换为二维的数据，虽然会损失一部分信息，但不影响我们对特征的提取与预测。</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/2.png" style="zoom:50%;">

<p>直观就是，所有点到线的垂直距离和最短的一条线。也确定了每个点在新的维度上的特征值</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/3.png" style="zoom:50%;">

<p>(业界常使用90%～95%)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pca</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    主成分分析进行特征降维</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># n_ components:小数 0~1 90% 业界选择 90~95%</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># 整数   减少到的特征数量</span></span><br><span class="line"></span><br><span class="line">    pca = PCA(n_components=<span class="number">0.9</span>) <span class="comment">#成分保留</span></span><br><span class="line"></span><br><span class="line">    data = pca.fit_transform([[<span class="number">2</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">8</span>], [<span class="number">5</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">    print(data)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">pca()</span><br></pre></td></tr></table></figure>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/4.png" style="zoom:50%;">

<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习(4)——算法</title>
    <url>/6552.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="机器学习算法分类"><a href="#机器学习算法分类" class="headerlink" title="机器学习算法分类"></a>机器学习算法分类</h2><ol>
<li><strong>监督学习</strong>：（英语：Supervised learning），可以由输入数据中学到或建立一个模型，并依此模式推测新的结果。输入数据是由输入特征值和目标值所组成。函数的输出可以是一个连续的值（称为回归），或是输出是有限个离散值（称作分类）分为：<ul>
<li><strong>分类：k-近邻算法、贝叶斯分类、决策树与随机森林、逻辑回归、神经网络</strong></li>
<li><strong>回归：线性回归、岭回归、神经网络</strong></li>
</ul>
</li>
<li><strong>无监督学习</strong>：（英语：Supervised learning），可以由输入数据中学到或建立一个模型，并依此模式推测新的结果。输入数据是由输入特征值所组成。<ul>
<li><strong>聚类：k-means（有几十种聚类）</strong></li>
</ul>
</li>
</ol>
<p>监督学习与非监督学习区别：</p>
<ul>
<li>监督学习:特征值+目标值</li>
<li>非监督学习:特征值1000个样本</li>
</ul>
<p>分类与回归的区别：</p>
<ul>
<li><p>分类:目标值离散型</p>
</li>
<li><p>回归:目标值连续型</p>
</li>
</ul>
<h2 id="分类模型评估指标"><a href="#分类模型评估指标" class="headerlink" title="分类模型评估指标"></a>分类模型评估指标</h2><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/1.png" style="zoom:50%;">

<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/2.png" style="zoom:50%;">

<h3 id="精确率-Precision-与召回率-Recall"><a href="#精确率-Precision-与召回率-Recall" class="headerlink" title="精确率(Precision)与召回率(Recall)"></a>精确率(Precision)与召回率(Recall)</h3><p><strong>精确率</strong>： 预测结果为正例样本中真实为正例的比例（查得准） TP/(TP+FP)</p>
<p><strong>召回率</strong>： 真实为正例的样本中预测结果为正例的比例（查的全， 对正样本的区分能力）TP/(TP+FN)， <strong>医院非常重视 召回率</strong></p>
<h3 id="F1-Score"><a href="#F1-Score" class="headerlink" title="F1-Score"></a>F1-Score</h3><p>既考虑了精确率， 也考虑了召回率的一个评估指标</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/3.png" style="zoom:50%;">

<h3 id="ROC-曲线与-AUC-指标"><a href="#ROC-曲线与-AUC-指标" class="headerlink" title="ROC 曲线与 AUC 指标"></a>ROC 曲线与 AUC 指标</h3><p><strong>TPR（True Positive Rate）</strong> 可以理解为所有正类中， 有多少被预测成正类（正类预测正确） ， 即召回率， 给出定义如下：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/4.png" style="zoom:50%;">

<p><strong>FPR（False Positive Rate）</strong> 可以理解为所有反类中， 有多少被预测成正类（正类预测错误） ， 给出定义如下： 这个值是越小越好</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/5.png" style="zoom:50%;">

<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/6.png" style="zoom:50%;">

<p><strong>AUC 则是 ROC 曲线下的面积</strong></p>
<p>ROC 曲线每个点对应着一个分类器的不同阈值。 通过改变阈值， 可以得到不同的真阳性率（TPR） 和假阳性率（FPR） ， 进而绘制出一条完整的 ROC 曲线。</p>
<p>分类器的阈值是指当分类器输出的概率（或得分） 大于该阈值时， 将样本划分为正类； 否则将其划分为负类。 不同阈值的设置会影响到分类器的性能和预测结果。</p>
<h4 id="在实际应用中，-选取不同阈值所代表的含义如下："><a href="#在实际应用中，-选取不同阈值所代表的含义如下：" class="headerlink" title="在实际应用中， 选取不同阈值所代表的含义如下："></a>在实际应用中， 选取不同阈值所代表的含义如下：</h4><ul>
<li>阈值=0： 分类器将所有样本都预测为正例。 此时， TPR=1， FPR=1， ROC 曲线的最右上角的点。</li>
<li>0 &lt; 阈值 &lt; 1： 分类器将样本分为正例和负例。 随着阈值逐渐增大， TPR 和 FPR 也会变化。</li>
<li>当阈值= 1 时， 分类器将所有样本都预测为负例。 此时， TPR=0， FPR=0， ROC 曲线的最左下角的点。</li>
</ul>
<p>需要注意的是， 通过调整分类器阈值可以改变模型的预测结果和分类性能， 但也可能引入一些新的问题， 例如过拟合、 欠拟合等。 因此， 在实际应用中， 需要综合考虑多种指标和方法，并进行适当的调参和优化。</p>
<h4 id="在实际应用中，-ROC-曲线和-AUC-常被用于以下几个方面："><a href="#在实际应用中，-ROC-曲线和-AUC-常被用于以下几个方面：" class="headerlink" title="在实际应用中， ROC 曲线和 AUC 常被用于以下几个方面："></a>在实际应用中， ROC 曲线和 AUC 常被用于以下几个方面：</h4><ol>
<li><strong>对比不同模型的性能</strong>： 在比较不同分类器的性能时， 可以通过绘制 ROC 曲线并计算 AUC 来确定哪个模型具有更好的分类性能。 AUC 值越高的模型通常被认为是更优秀的模型。</li>
<li><strong>选择最佳阈值</strong>： 在特定场景下， 可能需要根据业务需求选择特定的分类阈值，这时可以通过 ROC 曲线来选择最佳的分类阈值。 例如， 在医疗领域中， 可能更倾向于将假阳性率降到最低， 而在金融领域中， 则可能更关注漏报率。</li>
<li><strong>检测模型的稳定性</strong>： 在数据集或分类器发生变化或者存在很多噪声的情况下，ROC 曲线和 AUC 可以帮助评估模型的稳健性， 即模型对这些变化的敏感程度。ROC 曲线和 AUC 是机器学习中重要的评价指标， 可以帮助我们更全面地了解模型的性能， 并进行模型选择、 调参和优化等工作。</li>
</ol>
<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
        <tag>评估指标</tag>
        <tag>ROC</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习(2)——归一化与标准化</title>
    <url>/11448.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h2><h3 id="归一化Normalization"><a href="#归一化Normalization" class="headerlink" title="归一化Normalization"></a>归一化Normalization</h3><p>由于每一组特征数据的量纲是不同的，我们需要先将他们的量纲统一，之后再去训练学习，每一组特征数据的变化对预测结果的影响。</p>
<p><strong>特点:通过对原始数据进行变换把数据映射到(默认为[0,1])之间</strong></p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/11448/1.png" style="zoom:50%;">

<p>def MinMaxScaler(feature_range=(0, 1…))：归一化。每个特征缩放到给定范围(默认[0,1])</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler, StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mm</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    归一化处理</span></span><br><span class="line"><span class="string">    :return: NOne</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 归一化容易受极值的影响</span></span><br><span class="line">    mm = MinMaxScaler(feature_range=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">		</span><br><span class="line">    <span class="comment">#通过fit_transform进行转换</span></span><br><span class="line">    data = mm.fit_transform([[<span class="number">90</span>, <span class="number">2</span>, <span class="number">10</span>, <span class="number">40</span>], [<span class="number">60</span>, <span class="number">4</span>, <span class="number">15</span>, <span class="number">45</span>], [<span class="number">75</span>, <span class="number">3</span>, <span class="number">13</span>, <span class="number">46</span>]])</span><br><span class="line"></span><br><span class="line">    print(data)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#如果使用transform进行测试集的时候，量纲还是fit_transform的</span></span><br><span class="line">    out = mm.fit_transform([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">6</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">7</span>]])</span><br><span class="line"></span><br><span class="line">    print(out)</span><br><span class="line">    <span class="comment"># [[-1.96666667 0.		-1.4		-6		]</span></span><br><span class="line">    <span class="comment">#  [-1.8				1.5		-0.4		-5.5	]]</span></span><br><span class="line">		<span class="comment"># 相当于是用 (1-90/90-60) *1 + 1 = -1.966667</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">mm()</span><br></pre></td></tr></table></figure>


<ol>
<li><strong>归一化的好处</strong>：容易更快地通过梯度下降找到最优解</li>
<li>归一化的弊端：由于是使用max和min数据进行处理，会很容易受极值的影响，如果max很大，而其他数据很小，那归一化后的其他数据则很接近0</li>
</ol>
<p><strong>注意</strong>：在特定场景下最大值最小值是变化的，另外，最大值与最小值非常容易受<strong>异常点</strong>影响，所以这种方法鲁棒性较差，<strong>只适合传统精确小数据场景</strong>。</p>
<h3 id="标准化Standardization"><a href="#标准化Standardization" class="headerlink" title="标准化Standardization"></a>标准化Standardization</h3><p>z-score 标准化(zero-mean normalization)，最常见的标准化方法就是Z标准化，也叫<strong>标准差标准化</strong>。通过对原始数据进行变换<strong>把数据变换到均值为0,标准差为1范围内</strong>，不是标准正态分布，只是均值为0，标准差为1的均匀分布</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/11448/2.png" style="zoom:25%;">

<p>def  StandardScaler(…)：标准化</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler, StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stand</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    标准化缩放，不是标准正太分布，只均值为0，方差为1的分布</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    std = StandardScaler()</span><br><span class="line">    data = std.fit_transform([[<span class="number">1.</span>, -<span class="number">1.</span>, <span class="number">3.</span>], [<span class="number">2.</span>, <span class="number">4.</span>, <span class="number">2.</span>], [<span class="number">4.</span>, <span class="number">6.</span>, -<span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line">    print(data)</span><br><span class="line">    print(std.mean_)</span><br><span class="line">    print(std.var_)</span><br><span class="line">    print(std.n_samples_seen_)  <span class="comment"># 样本数</span></span><br><span class="line">    print(<span class="string">&#x27;-&#x27;</span> * <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">stand()</span><br></pre></td></tr></table></figure>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/11448/3.png" style="zoom:50%;">

<ul>
<li>对于归一化来说：如果出现异常点，影响了<strong>最大值和最小值</strong>，那么结果显然会发生改变</li>
<li>对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于<strong>平均值的影响</strong>并不大，从而方差改变<strong>较小</strong>。</li>
</ul>
<p><strong>注意</strong>：在已有<strong>样本足够多</strong>的情况下比较稳定，适合现代嘈杂大数据场景。</p>
<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-5-——转换器与估计器</title>
    <url>/6552.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="转换器（Transformer）"><a href="#转换器（Transformer）" class="headerlink" title="转换器（Transformer）"></a>转换器（Transformer）</h2><p>转换器是一种用于数据预处理的工具，它可以将原始数据转换为机器学习算法可以处理的格式。转换器通常用于特征工程，这是一种从原始数据中提取有意义特征的过程。</p>
<p>在sklearn中，转换器通常实现了一个名为<code>fit_transform</code>的方法。这个方法首先通过<code>fit</code>步骤学习数据的特性（如平均值、标准差等），然后通过<code>transform</code>步骤应用这些学习到的特性来转换数据。</p>
<p>转换器是一种实现特征工程操作的一组<a href="https://so.csdn.net/so/search?q=API&spm=1001.2101.3001.7020">API</a>，可以较方便的完成常用的特征工程操作，包括：</p>
<ul>
<li>fit：计算一些数据的平均数、方差等</li>
<li>transform：进行一些数据转换</li>
<li>fit_transform：相当于fit+transform，既实现了fit的功能，又实现了transform的功能。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">std1=StandardScaler()</span><br><span class="line">a=[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]</span><br><span class="line">std1.fit_transform(a)</span><br><span class="line"><span class="comment"># 结果如下</span></span><br><span class="line">array([[-<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>], [ <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line">print(std1.fit(a))<span class="comment">#计算每一列的平均值与标准差</span></span><br><span class="line">StandardScaler(copy=<span class="literal">True</span>, with_mean=<span class="literal">True</span>, with_std=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">std1.transform(a)<span class="comment">#进行最终的标准化</span></span><br><span class="line"><span class="comment"># 结果如下</span></span><br><span class="line">array([[-<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>],</span><br><span class="line">[ <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure>
<p>例如，<code>StandardScaler</code>是一个常用的转换器，它可以将特征数据标准化，即减去平均值并除以标准差。这样做的好处是，标准化后的数据具有零均值和单位方差，这对于许多机器学习算法来说是非常有益的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化转换器</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用fit_transform方法对数据进行转换</span></span><br><span class="line">X_scaled = scaler.fit_transform(X)</span><br></pre></td></tr></table></figure>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/3.png" style="zoom:45%;">



<h2 id="估计器（Estimator）"><a href="#估计器（Estimator）" class="headerlink" title="估计器（Estimator）"></a>估计器（Estimator）</h2><p>估计器是机器学习算法的主要实现者。在sklearn中，估计器是一个实现了特定机器学习算法的类。例如，<code>LogisticRegression</code>是一个实现了逻辑回归算法的估计器，<code>RandomForestClassifier</code>是一个实现了随机森林算法的估计器。包括：</p>
<p><strong>1、用于分类的估计器：</strong></p>
<ul>
<li>sklearn.neighbors k-近邻算法</li>
<li>sklearn.naive_bayes 贝叶斯</li>
<li>sklearn.linear_model.LogisticRegression 逻辑回归</li>
<li>sklearn.tree 决策树与随机森林</li>
</ul>
<p>2、用于回归的估计器：</p>
<ul>
<li>sklearn.linear_model.LinearRegression 线性回归</li>
<li>sklearn.linear_model.Ridge 岭回归</li>
</ul>
<p><strong>3、用于无监督学习的估计器：</strong></p>
<ul>
<li>sklearn.cluster.KMeans 聚类</li>
</ul>
<p>估计器通常提供了<code>fit</code>、<code>predict</code>等方法。<code>fit</code>方法用于训练模型，它接受训练数据作为输入并学习数据的特性。<code>predict</code>方法则用于对新数据进行预测。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化估计器</span></span><br><span class="line">clf = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用fit方法对模型进行训练</span></span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用predict方法对测试数据进行预测</span></span><br><span class="line">y_pred = clf.predict(X_test)</span><br></pre></td></tr></table></figure>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/2.jpg" style="zoom:45%;">

<h3 id="估计器工作流程"><a href="#估计器工作流程" class="headerlink" title="估计器工作流程"></a>估计器工作流程</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/1.jpg" style="zoom:25%;">



<h2 id="转换器与估计器的结合"><a href="#转换器与估计器的结合" class="headerlink" title="转换器与估计器的结合"></a>转换器与估计器的结合</h2><p>在sklearn中，转换器和估计器可以非常方便地结合使用。这是因为sklearn提供了一个名为<code>Pipeline</code>的工具，它可以将转换器和估计器组合成一个整体。</p>
<p>例如，我们可以创建一个包含标准化转换器和逻辑回归估计器的管道：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建管道</span></span><br><span class="line">pipe = Pipeline([</span><br><span class="line">    (<span class="string">&#x27;scaler&#x27;</span>, StandardScaler()),</span><br><span class="line">    (<span class="string">&#x27;clf&#x27;</span>, LogisticRegression())</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用fit方法对模型进行训练</span></span><br><span class="line">pipe.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用predict方法对测试数据进行预测</span></span><br><span class="line">y_pred = pipe.predict(X_test)</span><br></pre></td></tr></table></figure>
<p>在这个例子中，<code>Pipeline</code>会自动处理数据的转换和模型的训练。我们只需要调用<code>fit</code>和<code>predict</code>方法，<code>Pipeline</code>就会按照我们指定的顺序执行转换和预测。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>转换器和估计器是机器学习中的两个核心概念。转换器负责数据的预处理，而估计器则负责实现具体的机器学习算法。在sklearn中，这两个概念得到了很好的实现和整合，使得我们可以方便地进行机器学习任务。通过理解并熟练使用转换器和估计器，我们可以更好地掌握机器学习的精髓，并在实际应用中取得更好的效果。</p>
<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-7-——朴素贝叶斯算法</title>
    <url>/6552.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><p>贝叶斯公式（两个特征之间独立）：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/1.png" style="zoom:70%;">

<p>换个表达形式就会明朗很多，如下：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/2.png" style="zoom:40%;">

<p>我们最终求的p(类别|特征)即可！就相当于完成了我们的任务。</p>
<p>公式分为三个部分：</p>
<ul>
<li>P(C):每个文档类别的概率(某文档类别数/总文档数</li>
<li>P(WIC):<strong>给定某个类别</strong>下<strong>特征(被预测文档中出现的词)的概率</strong></li>
</ul>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/3.png" style="zoom:50%;">

<h3 id="拉普拉斯平滑"><a href="#拉普拉斯平滑" class="headerlink" title="拉普拉斯平滑"></a>拉普拉斯平滑</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/4.png" style="zoom:50%;">

<p>属于某个类别为 0， 合适吗？</p>
<p><strong>问题</strong>： 从上面的例子我们得到娱乐概率为 0， 这是不合理的， 如果词频列表里面有很多出现次数都为 0， 很可能计算结果都为零<br><strong>解决方法</strong>： 拉普拉斯平滑系数</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/5.png" style="zoom:40%;">

<p>α 为指定的系数一般为 1， m 为代表类别的数目（标签的数目） (我们分为两类,科技类与娱乐类)</p>
<h3 id="sklearn-朴素贝叶斯实现-API"><a href="#sklearn-朴素贝叶斯实现-API" class="headerlink" title="sklearn 朴素贝叶斯实现 API"></a>sklearn 朴素贝叶斯实现 API</h3><p>sklearn.naive_bayes.MultinomialNB</p>
<p>MultinomialNB</p>
<ul>
<li>sklearn.naive_bayes.MultinomialNB(alpha = 1.0)</li>
<li>朴素贝叶斯分类</li>
<li>alpha： 拉普拉斯平滑系数</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">朴素贝叶斯进行文本分类</span></span><br><span class="line"><span class="string">:return: None</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">news = fetch_20newsgroups(subset=<span class="string">&#x27;all&#x27;</span>, data_home=<span class="string">&#x27;data&#x27;</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="built_in">len</span>(news.data))  <span class="comment">#样本数，包含的特征</span></span><br><span class="line">print(<span class="string">&#x27;-&#x27;</span>*<span class="number">50</span>)</span><br><span class="line">print(news.data[<span class="number">0</span>]) <span class="comment">#第一个样本 特征</span></span><br><span class="line">print(<span class="string">&#x27;-&#x27;</span>*<span class="number">50</span>)</span><br><span class="line">print(news.target) <span class="comment">#标签</span></span><br><span class="line">print(np.unique(news.target)) <span class="comment">#标签的类别</span></span><br><span class="line">print(news.target_names) <span class="comment">#标签的名字</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;-&#x27;</span>*<span class="number">50</span>)</span><br><span class="line"><span class="comment"># 进行数据分割</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=<span class="number">0.25</span>, random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对数据集进行特征抽取</span></span><br><span class="line">tf = TfidfVectorizer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以训练集当中的词的列表进行每篇文章重要性统计[&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;]</span></span><br><span class="line">x_train = tf.fit_transform(x_train)</span><br><span class="line"><span class="comment">#针对特征内容，可以自行打印，下面的打印可以得到特征数目</span></span><br><span class="line">print(<span class="built_in">len</span>(tf.get_feature_names_out()))</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment"># 进行朴素贝叶斯算法的预测,alpha是拉普拉斯平滑系数，分子和分母加上一个系数，分母加alpha*特征词数目</span></span><br><span class="line">mlt = MultinomialNB(alpha=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">print(x_train.toarray())</span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">start=time.time()</span><br><span class="line">mlt.fit(x_train, y_train)</span><br><span class="line">end=time.time()</span><br><span class="line">end-start <span class="comment">#统计训练时间</span></span><br><span class="line"></span><br><span class="line">x_test = tf.transform(x_test)  <span class="comment">#特征数目不发生改变</span></span><br><span class="line">print(<span class="built_in">len</span>(tf.get_feature_names_out()))</span><br><span class="line"></span><br><span class="line">start=time.time()</span><br><span class="line">y_predict = mlt.predict(x_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;预测的前面10篇文章类别为：&quot;</span>, y_predict[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得出准确率,这个是很难提高准确率，为什么呢？</span></span><br><span class="line">print(<span class="string">&quot;准确率为：&quot;</span>, mlt.score(x_test, y_test))</span><br><span class="line">end=time.time()</span><br><span class="line">end-start</span><br><span class="line"></span><br><span class="line"><span class="comment"># 目前这个场景我们不需要召回率，support是真实的为那个类别的有多少个样本</span></span><br><span class="line">print(<span class="string">&quot;每个类别的精确率和召回率：&quot;</span>,</span><br><span class="line">      classification_report(y_test, y_predict,</span><br><span class="line">      target_names=news.target_names))</span><br></pre></td></tr></table></figure>


<h3 id="朴素贝叶斯分类优缺点"><a href="#朴素贝叶斯分类优缺点" class="headerlink" title="朴素贝叶斯分类优缺点"></a>朴素贝叶斯分类优缺点</h3><p><strong>优点：</strong></p>
<ul>
<li>朴素贝叶斯模型发源于古典数学理论， 有稳定的分类效率。</li>
<li>对缺失数据不太敏感， 算法也比较简单， 常用于文本分类。</li>
<li>分类准确度高， 速度快</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>需要知道先验概率 P(F1,F2,…|C)， 因此在某些时候会由于假设的先验模型的原因 导致预测效果不佳</li>
<li>假设了文章当中一些词语另外一些是独立没关系—-如果有关系， 会造成不太靠谱</li>
<li>训练集当中去进行统计词这些工作 文章收集的不好， 比如有作弊文章， 充斥某个词会对结果造成干扰</li>
</ul>
<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>分类算法</tag>
        <tag>朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-6-——K近邻算法</title>
    <url>/6552.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="k近邻（k-NN）算法"><a href="#k近邻（k-NN）算法" class="headerlink" title="k近邻（k-NN）算法"></a>k近邻（k-NN）算法</h2><p>k近邻算法是一种<strong>基本分类和回归方法</strong>。K近邻算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例<strong>最邻近</strong>的K个实例，<strong>这K个实例的多数属于某个类</strong>，就把该输入实例分类到这个类中。（<strong>这就类似于现实生活中少数服从多数的思想</strong>）根据这个说法，咱们来看下引自维基百科上的一幅图：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/1.png" style="zoom:50%;">

<p>如上图所示，有<strong>两类</strong>不同的<a href="https://zhida.zhihu.com/search?content_id=2584561&content_type=Article&match_order=1&q=%E6%A0%B7%E6%9C%AC%E6%95%B0%E6%8D%AE&zhida_source=entity">样本数据</a>，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的那个绿色的圆所标示的数据则是<strong>待分类的数据</strong>。这也就是我们的目的，来了一个新的数据点，我要得到它的类别是什么？好的，下面我们根据k近邻的思想来给绿色圆点进行分类。</p>
<ul>
<li>如果K=3，绿色圆点的最邻近的3个点是2个红色小三角形和1个蓝色小正方形，<strong>少数从属于多数，</strong>基于统计的方法，判定绿色的这个待分类点属于红色的三角形一类。</li>
<li>如果K=5，绿色圆点的最邻近的5个邻居是2个红色三角形和3个蓝色的正方形，<strong>还是少数从属于多数，</strong>基于统计的方法，判定绿色的这个待分类点属于蓝色的正方形一类。</li>
</ul>
<h3 id="sklearn-k-近邻算法-API"><a href="#sklearn-k-近邻算法-API" class="headerlink" title="sklearn k-近邻算法 API"></a>sklearn k-近邻算法 API</h3><ul>
<li>sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm=’auto’)</li>
<li>n_neighbors： int,可选（默认= 5） ， k_neighbors 查询默认使用的邻居数</li>
<li>algorithm： {‘auto’ ， ‘ball_tree’ ， ‘kd_tree’ ， ‘brute’ }， 可选用于计算最近邻居的算法： ‘ball_tree’ 将会使用 BallTree， ‘kd_tree’ 将使用 KDTree。 ‘auto’ 将尝试根据传递给 fit 方法的值来决定最合适的算法。 (不同实现方式影响效率)</li>
</ul>
<h3 id="一般来说，KNN-分类算法的计算过程-预测-："><a href="#一般来说，KNN-分类算法的计算过程-预测-：" class="headerlink" title="一般来说，KNN 分类算法的计算过程(预测)："></a>一般来说，KNN 分类算法的计算过程(预测)：</h3><ol>
<li>计算待分类点与已知类别的点之间的距离</li>
<li>按照距离递增次序排序</li>
<li>选取与待分类点距离最小的 K 个点</li>
<li>确定前 K 个点所在类别的出现次数</li>
<li>返回前 K 个点出现次数最高的类别作为待分类点的预测分类</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行数据的分割训练集合测试集</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.25</span>, random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征工程（标准化）,下面3行注释，一开始我们不进行标准化，看下效果，目标值要不要标准化？</span></span><br><span class="line">std = StandardScaler()</span><br><span class="line"><span class="comment"># #</span></span><br><span class="line"><span class="comment"># # # 对测试集和训练集的特征值进行标准化,服务于knn fit</span></span><br><span class="line">x_train = std.fit_transform(x_train)</span><br><span class="line"><span class="comment"># # transform返回的是copy，不在原有的输入对象中去修改</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 进行算法流程 # 超参数，可以通过设置n_neighbors=5，来调整结果好坏</span></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # fit， predict,score，训练，knn的fit是不训练的，只是把训练集的特征值和目标值放入到内存中</span></span><br><span class="line">knn.fit(x_train, y_train)</span><br><span class="line"><span class="comment"># # #</span></span><br><span class="line"><span class="comment"># # # 得出预测结果</span></span><br><span class="line">y_predict = knn.predict(x_test)</span><br><span class="line"><span class="comment"># #</span></span><br><span class="line">print(<span class="string">&quot;预测的目标签到位置为：&quot;</span>, y_predict[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"><span class="comment"># # #</span></span><br><span class="line"><span class="comment"># # # # 得出准确率</span></span><br><span class="line">print(<span class="string">&quot;预测的准确率:&quot;</span>, knn.score(x_test, y_test))</span><br><span class="line"><span class="comment"># print(y_predict)</span></span><br><span class="line"><span class="comment"># y_test</span></span><br></pre></td></tr></table></figure>


<h3 id="特征归一化的必要性"><a href="#特征归一化的必要性" class="headerlink" title="特征归一化的必要性"></a>特征归一化的必要性</h3><p>首先举例如下，我用一个人身高(cm)与脚码（尺码）大小来作为<a href="https://zhida.zhihu.com/search?content_id=2584561&content_type=Article&match_order=1&q=%E7%89%B9%E5%BE%81%E5%80%BC&zhida_source=entity">特征值</a>，类别为男性或者女性。我们现在如果有5个训练样本，分布如下：</p>
<p>A [(179,42),男] B [(178,43),男] C [(165,36)女] D [(177,42),男] E [(160,35),女]</p>
<p>很容易看到第一维身高特征是第二维脚码特征的4倍左右，那么在进行距离度量的时候，<strong>我们就会偏向于第一维特征。</strong>这样造成俩个特征并不是等价重要的，最终可能会导致距离计算错误，从而导致预测错误。口说无凭，举例如下：</p>
<p>现在我来了一个测试样本 F(167,43)，让我们来预测他是男性还是女性，我们采取k=3来预测。</p>
<p>下面我们用欧式距离分别算出F离训练样本的欧式距离，然后选取最近的3个，多数类别就是我们最终的结果，计算如下：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/2.png" style="zoom:45%;">

<p>由计算可以得到，最近的前三个分别是C,D,E三个样本，那么由C,E为女性，D为男性，女性多于男性得到我们要预测的结果为<strong>女性</strong>。</p>
<p><strong>这样问题就来了，一个女性的脚43码的可能性，远远小于男性脚43码的可能性，那么为什么算法还是会预测F为女性呢？那是因为由于各个特征量纲的不同，在这里导致了身高的重要性已经远远大于脚码了，这是不客观的。</strong>所以我们应该让每个特征都是同等重要的！这也是我们要归一化的原因！</p>
<h3 id="k近邻算法中k的选取"><a href="#k近邻算法中k的选取" class="headerlink" title="k近邻算法中k的选取"></a>k近邻算法中k的选取</h3><ol>
<li><strong>选取k值以及它的影响</strong></li>
</ol>
<p>k近邻的k值我们应该怎么选取呢？</p>
<p><strong>如果我们选取较小的k值，那么就会意味着我们的整体模型会变得复杂，容易发生过拟合！</strong>恩<del>结论说完了，太抽象了吧你，不上图讲解号称通俗讲解的都是流氓**</del>好吧，那我就上图来讲解**</p>
<p><strong>假设我们选取k=1这个极端情况，怎么就使得模型变得复杂，又容易过拟合了呢？</strong></p>
<p><strong>假设我们有训练数据和待分类点如下图：</strong></p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/3.png" style="zoom:50%;">

<p>上图中有俩类，一个是<strong>黑色的圆点</strong>，一个是<strong>蓝色的长方形</strong>，现在我们的待分类点是<strong>红色的五边形。</strong></p>
<p>好，根据我们的<a href="https://zhida.zhihu.com/search?content_id=2584561&content_type=Article&match_order=6&q=k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95&zhida_source=entity">k近邻算法</a>步骤来决定待分类点应该归为哪一类。我们由图中可以得到，<strong>很容易我们能够看出来五边形离黑色的圆点最近，k又等于1，那太好了</strong>，我们最终判定待分类点是黑色的圆点。</p>
<p>由这个处理过程我们很容易能够感觉出问题了，如果k太小了，比如等于1，那么模型就太复杂了，<strong>我们很容易学习到噪声</strong>，也就非常容易判定为噪声类别，而在上图，如果，k大一点，k等于8，<strong>把长方形都包括进来</strong>，我们很容易得到我们正确的分类应该是蓝色的长方形！如下图：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/4.png" style="zoom:50%;">

<p>所谓的<a href="https://zhida.zhihu.com/search?content_id=2584561&content_type=Article&match_order=3&q=%E8%BF%87%E6%8B%9F%E5%90%88&zhida_source=entity">过拟合</a>就是在训练集上准确率非常高，而在测试集上准确率低，经过上例，我们可以得到k太小会导致<strong>过拟合</strong>，<strong>很容易将一些噪声（如上图离五边形很近的黑色圆点）学习到模型中，而忽略了数据真实的分布！</strong></p>
<p><strong>如果我们选取较大的k值，就相当于用较大邻域中的训练数据进行预测，这时与输入实例较远的（不相似）训练实例也会对预测起作用，使预测发生错误，k值的增大意味着整体模型变得简单。</strong></p>
<p>k值增大怎么就意味着模型变得简单了？</p>
<p><strong>我们想，如果k=N（N为<a href="https://zhida.zhihu.com/search?content_id=2584561&content_type=Article&match_order=1&q=%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC&zhida_source=entity">训练样本</a>的个数）,那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型是不是非常简单，这相当于你压根就没有训练模型呀！</strong>直接拿训练数据统计了一下各个数据的类别，找最大的而已！这好像下图所示：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/5.png" style="zoom:40%;">

<p>我们统计了黑色圆形是8个，长方形个数是7个，那么哈哈，如果k=N，我就得出结论了，红色五边形是属于黑色圆形的。<strong>这个时候，模型过于简单，完全忽略训练数据实例中的大量有用信息，是不可取的。</strong></p>
<h3 id="距离的度量"><a href="#距离的度量" class="headerlink" title="距离的度量"></a>距离的度量</h3><p>在上文中说到，k近邻算法是在训练数据集中找到与该实例<strong>最邻近</strong>的K个实例，这K个实例的多数属于某个类，我们就说预测点属于哪个类。</p>
<p>定义中所说的最邻近是如何度量呢？我们怎么知道谁跟测试点最邻近。这里就会引出我们几种度量俩个点之间距离的标准。</p>
<p>我们可以有以下几种度量方式：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/6.png" style="zoom:40%;">

<p>其中当p=2的时候，就是我们最常见的欧式距离，我们也一般都用欧式距离来衡量我们<a href="https://zhida.zhihu.com/search?content_id=2584561&content_type=Article&match_order=1&q=%E9%AB%98%E7%BB%B4%E7%A9%BA%E9%97%B4&zhida_source=entity">高维空间</a>中俩点的距离。在实际应用中，<a href="https://zhida.zhihu.com/search?content_id=2584561&content_type=Article&match_order=1&q=%E8%B7%9D%E7%A6%BB%E5%87%BD%E6%95%B0&zhida_source=entity">距离函数</a>的选择应该根据数据的特性和分析的需要而定，一般选取p=2欧式距离表示，这不是本文的重点。</p>
<h3 id="k-近邻算法优缺点"><a href="#k-近邻算法优缺点" class="headerlink" title="k-近邻算法优缺点"></a>k-近邻算法优缺点</h3><p><strong>优点：</strong></p>
<ol>
<li>算法简单，理论成熟，既可以用来做分类也可以用来做回归。</li>
<li>可用于非线性分类。(Y=kx 是线性）</li>
<li>没有明显的训练过程，而是在程序开始运行时，把数据集加载到内存后，不需要进行训练，直接进行预测，所以训练时间复杂度为 0。</li>
<li>由于 KNN 方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属的类别，因此对于类域的交叉或重叠较多的待分类样本集来说，KNN 方法较其他方法更为适合。</li>
<li>该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量比较小的类域采用这种算法比较容易产生误分类情况。</li>
</ol>
<p><strong>缺点：</strong></p>
<ol>
<li>需要算每个测试点与训练集的距离，当训练集较大时，计算量相当大，时间复杂度高，特别是特征数量比较大的时候。（预测的时间复杂度高）</li>
<li>需要大量的内存，空间复杂度高。</li>
<li>样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少），对稀有类别的预测准确度低。</li>
</ol>
<h3 id="KNN-算法适用于以下场景："><a href="#KNN-算法适用于以下场景：" class="headerlink" title="KNN 算法适用于以下场景："></a>KNN 算法适用于以下场景：</h3><ol>
<li>数据集具有明显的类别划分；</li>
<li>样本数量比较大， 样本特征维度较小；</li>
<li>类别边界比较清晰， 即不同类别的数据在特征空间中分离明显；</li>
<li>预测目标的类别或值受到周围邻居影响较大。</li>
</ol>
<p>需要注意的是， KNN 算法在处理高维稀疏数据时可能会出现“维数灾难” 问题， 同时也对噪<br>声敏感， 并且需要选择合适的距离度量方法和 K 值大小。</p>
<h2 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h2><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><p>交叉验证：将拿到的训练数据，分为训练和验证集。以下图为例：将数据分成5份，其中一份作为验证集。然后经过5次(组)的测试，每次都更换不同的验证集。即得到5组模型的结果，取平均值作为最终结果。又称5折交叉验证。</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/7.png" style="zoom:40%;">

<h3 id="网格搜索-1"><a href="#网格搜索-1" class="headerlink" title="网格搜索"></a>网格搜索</h3><p>通常情况下，有很多参数是需要手动指定的（如k-近邻算法中的K值），这种叫超参数。但是手动过程繁杂，所以需要对模型预设几种超参数组合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。</p>
<ul>
<li>sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None)<ul>
<li>对估计器的指定参数值进行详尽搜索</li>
<li>estimator：估计器对象</li>
<li>param_grid：估计器参数(dict){“n_neighbors”:[1,3,5]}</li>
<li>cv：指定几折交叉验证</li>
<li>fit：输入训练数据</li>
<li>score：准确率</li>
<li>结果分析：<ul>
<li>best_score_:在交叉验证中验证的最好结果*</li>
<li>best_estimator_：最好的参数模型</li>
<li>cv_results_:每次交叉验证后的验证集准确率结果和训练集准确率结果</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="comment">#网格搜索时讲解</span></span><br><span class="line"><span class="comment"># # 构造一些参数的值进行搜索</span></span><br><span class="line">param = &#123;<span class="string">&quot;n_neighbors&quot;</span>: [<span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">15</span>],<span class="string">&#x27;weights&#x27;</span>:[<span class="string">&#x27;uniform&#x27;</span>, <span class="string">&#x27;distance&#x27;</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行网格搜索，cv=3是3折交叉验证，用其中2折训练，1折验证</span></span><br><span class="line">gc = GridSearchCV(knn, param_grid=param, cv=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">gc.fit(x_train, y_train)  <span class="comment">#你给它的x_train，它又分为训练集，验证集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测准确率，为了给大家看看</span></span><br><span class="line">print(<span class="string">&quot;在测试集上准确率：&quot;</span>, gc.score(x_test, y_test))</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;在交叉验证当中最好的结果：&quot;</span>, gc.best_score_) <span class="comment">#最好的结果</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;选择最好的模型是：&quot;</span>, gc.best_estimator_) <span class="comment">#最好的模型,告诉你用了哪些参数</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;每个超参数每次交叉验证的结果：&quot;</span>, gc.cv_results_)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果如图所示：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/8.png" style="zoom:50%;">



<h2 id="参数与超参数"><a href="#参数与超参数" class="headerlink" title="参数与超参数"></a>参数与超参数</h2><p><strong>参数</strong>：就是模型可以根据数据可以自动学习出的变量，应该就是参数。比如，深度学习的权重，偏差等</p>
<p><strong>超参数</strong>：就是用来确定模型的一些参数，超参数不同，模型是不同的(这个模型不同的意思就是有微小的区别，比如假设都是CNN模型，如果层数不同，模型不一样，虽然都是CNN模型哈。)，超参数一般就是根据经验确定的变量。在深度学习中，超参数有：学习速率，迭代次数，层数，每层神经元的个数等等。</p>
<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
        <tag>K近邻</tag>
        <tag>网格搜索</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-8-——决策树算法</title>
    <url>/6552.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a>决策树算法</h2><p><strong>决策树：是一种树形结构，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，最后每个叶节点代表一种分类结果，本质是一颗由多个判断节点组成的树</strong>。</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/1.png" style="zoom:50%;">

<p>树的节点如何划分？很明显，从图中是按照重要程度进行，逐个划分，越往上重要程度更高，但如何定义重要程度呢？这就需要引入<strong>信息熵</strong>的概念</p>
<h3 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h3><p>物理学上，<strong>熵 Entropy</strong> 是“混乱”程度的量度。</p>
<p><strong>系统越有序，熵值越低；系统越混乱或者分散，熵值越高</strong>。</p>
<ul>
<li><strong>信息理论</strong>：</li>
</ul>
<p>1、<strong>从信息的完整性上进行的描述:</strong></p>
<p>当<strong>系统的有序状态一致时</strong>，**数据越集中的地方熵值越小，数据越分散的地方熵值越大。</p>
<p>2、<strong>从信息的有序性上进行的描述:</strong></p>
<p>当<strong>数据量一致时</strong>，<strong>系统越有序，熵值越低；系统越混乱或者分散，熵值越高</strong>。</p>
<p>1948年香农提出了<strong>信息熵</strong>（Entropy）的概念。</p>
<p>假如事件A的分类划分是（A1,A2,…,An），每部分发生的概率是(p1,p2,…,pn)，那信息熵定义为公式如下：（log是以2为底，lg是以10为底）</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/2.png" style="zoom:50%;">

<h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">课堂案例1：</span><br><span class="line">如果一颗骰子的六个面都是1 ，投掷它不会给你带来任何新信息，因为你知道它的结果肯定是1，它的信息熵为？？</span><br><span class="line"></span><br><span class="line">答案：</span><br><span class="line"> - log(1) = 0 。</span><br></pre></td></tr></table></figure>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">课堂案例2：</span><br><span class="line">假设我们没有看世界杯的比赛，但是想知道哪支球队会是冠军，</span><br><span class="line">我们只能猜测某支球队是或不是冠军，然后观众用对或不对来回答，</span><br><span class="line">我们想要猜测次数尽可能少，你会用什么方法？</span><br><span class="line"></span><br><span class="line">答案：</span><br><span class="line">二分法：</span><br><span class="line">假如有 16 支球队，分别编号，先问是否在 1-8 之间，如果是就继续问是否在 1-4 之间，</span><br><span class="line">以此类推，直到最后判断出冠军球队是哪只。</span><br><span class="line">如果球队数量是 16，我们需要问 4 次来得到最后的答案。那么世界冠军这条消息的信息熵就是 4。</span><br><span class="line"></span><br><span class="line">如果有32个球队，准确的信息量应该是： </span><br><span class="line">H = -（p1 * logp1 + p2 * logp2 + ... + p32 * logp32），</span><br><span class="line">其中 p1, ..., p32 分别是这 32 支球队夺冠的概率。</span><br><span class="line">当每支球队夺冠概率相等都是 1/32 的时：H = -（32 * 1/32 * log1/32） = 5</span><br><span class="line">每个事件概率相同时，熵最大，这件事越不确定。</span><br></pre></td></tr></table></figure>
<h3 id="决策树的划分依据一——信息增益——ID3"><a href="#决策树的划分依据一——信息增益——ID3" class="headerlink" title="决策树的划分依据一——信息增益——ID3"></a>决策树的划分依据一——信息增益——ID3</h3><p><strong>信息增益：</strong>以某特征划分数据集前后的熵的差值。熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。因此可以<strong>使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏</strong>。</p>
<p><strong>信息增益 = entroy(前) - entroy(后)</strong></p>
<ul>
<li>定义与公式</li>
</ul>
<p>特征A对训练数据集D的信息增益g(D,A),定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差，即公式为：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/3.png" style="zoom:35%;">

<p>公式的详细解释：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/4.png" style="zoom:40%;">

<h4 id="案例-1"><a href="#案例-1" class="headerlink" title="案例"></a>案例</h4><p>如下左图，第一列为论坛号码，第二列为性别，第三列为活跃度，最后一列用户是否流失。</p>
<p>我们要解决一个问题：<strong>性别和活跃度两个特征，哪个对用户流失影响更大</strong>？</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/5.png" style="zoom:50%;">

<p>通过计算信息增益可以解决这个问题，统计上右表信息</p>
<p>其中Positive为正样本（已流失），Negative为负样本（未流失），下面的数值为不同划分下对应的人数。可得到三个熵：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/6.png" style="zoom:45%;">

<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/7.png" style="zoom:45%;">

<p>活跃度的信息增益比性别的信息增益大，也就是说，活跃度对用户流失的影响比性别大。</p>
<p>在做<strong>特征选择</strong>或者<strong>数据分析</strong>的时候，我们应该<strong>重点考察活跃度这个指标</strong>。</p>
<p>从极限角度来理解： 加入某个特征只有一个类别， 那么信息增益为零， 我们会删除这个特征</p>
<p><strong>缺点</strong>：一个特征中的两个特征值， 某个特征值的熵很大， 乘以的是 1/2， 而三个特征值， 某个特征值的熵很大， 乘 1/3， 所以增益就大。比如上述的E(a2)在只有两个的时候，只是1/2，但三个特征的时候1/3更大了，这是不好的，<strong>导致偏向特征值较多的特征</strong></p>
<h3 id="决策树的划分依据二—-信息增益率——C4-5"><a href="#决策树的划分依据二—-信息增益率——C4-5" class="headerlink" title="决策树的划分依据二—-信息增益率——C4.5"></a>决策树的划分依据二—-信息增益率——C4.5</h3><p><strong>增益率：</strong>增益比率度量是用前面的增益度量Gain(S，A)和所分离信息度量SplitInformation(如上例的性别，活跃度等)的比值来共同定义的。</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/8.png" style="zoom:45%;">

<p>这里需要注意，信息增益率对可取值较少的特征有所偏好**(分母越小，整体越大)**</p>
<p><strong>缺点</strong>： <strong>偏向于特征值较小的特征</strong>， HA(D)特征的特征值越多， 它就越大， 特征值越少， 就越小，与上面的刚好相反。</p>
<h3 id="决策树的划分依据三——基尼值和基尼指数"><a href="#决策树的划分依据三——基尼值和基尼指数" class="headerlink" title="决策树的划分依据三——基尼值和基尼指数"></a>决策树的划分依据三——基尼值和基尼指数</h3><p><strong>基尼值Gini（D）：</strong>从数据集D中随机抽取两个样本，其类别标记不一致的概率。故，Gini（D）值越小，数据集D的纯度越高。</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/9.png" style="zoom:45%;">

<h4 id="案例-2"><a href="#案例-2" class="headerlink" title="案例"></a>案例</h4><p>请根据下图列表，按照基尼指数的划分依据，做出决策树。</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/10.png" style="zoom:45%;">

<ol>
<li><p>对数据集非类标号属性{是否有房，婚姻状况，年收入}分别计算它们的Gini系数增益，<strong>取Gini系数增益值最大的属性作为决策树的根节点属性。</strong></p>
</li>
<li><p>根节点的Gini系数为(基尼整体熵)：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/11.png" style="zoom:45%;">
</li>
<li><p>当根据是否有房来进行划分时，Gini系数增益计算过程为(基尼条件熵)：</p>
<p>左子节点：有房的，是否有拖欠贷款</p>
<p>右子节点：没房的，是否有拖欠贷款</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/12.png" style="zoom:45%;">
</li>
<li><p>同理可得年收入Gini：</p>
<p>对于年收入属性为<strong>数值型属性</strong>，首先需要对数据按升序排序，然后从小到大依次用相邻值的中间值作为分隔将样本划分为两组。例如当面对年收入为60和70这两个值时，我们算得其中间值为65。以中间值65作为分割点求出Gini系数增益。</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/13.png" style="zoom:45%;">

</li>
</ol>
<p><strong>最大化增益等价于最小化子女结点的不纯性度量（Gini系数）的加权平均值</strong>，现在我们希望最大化Gini系数的增益。根据计算知道，三个属性划分根节点的增益最大的有两个：年收入属性和婚姻状况，他们的增益都为0.12。此时，选取首先出现的属性作为第一次划分。</p>
<p><strong>一，决策树构建的基本步骤如下</strong>：</p>
<ol>
<li>开始将所有记录看作一个节点</li>
<li>遍历每个变量的每一种分割方式，找到最好的分割点</li>
<li>分割成两个节点N1和N2</li>
<li>对N1和N2分别继续执行2-3步，直到每个节点足够“纯”为止。</li>
</ol>
<p><strong>二，决策树的变量可以有两种</strong>：</p>
<ol>
<li>数字型（Numeric）：变量类型是整数或浮点数，如前面例子中的“年收入”。用“&gt;=”，“&gt;”,“&lt;”或“&lt;=”作为分割条件（排序后，利用已有的分割情况，可以优化分割算法的时间复杂度）。</li>
<li>名称型（Nominal）：类似编程语言中的枚举类型，变量只能从有限的选项中选取，比如前面例子中的“婚姻情况”，只能是“单身”，“已婚”或“离婚”，使用“=”来分割。</li>
</ol>
<p><strong>三，如何评估分割点的好坏？</strong></p>
<p> 如果一个分割点可以将当前的所有节点分为两类，使得每一类都很“纯”，也就是同一类的记录较多，那么就是一个好分割点。</p>
<p> 比如上面的例子，“拥有房产”，可以将记录分成了两类，“是”的节点全部都可以偿还债务，非常“纯”；“否”的节点，可以偿还贷款和无法偿还贷款的人都有，不是很“纯”，但是两个节点加起来的纯度之和与原始节点的纯度之差最大，所以按照这种方法分割。构建决策树采用贪心算法，只考虑当前纯度差最大的情况作为分割点。</p>
<h3 id="代码公式："><a href="#代码公式：" class="headerlink" title="代码公式："></a>代码公式：</h3><h4 id="sklearn-决策树-API-DecisionTreeClassifier"><a href="#sklearn-决策树-API-DecisionTreeClassifier" class="headerlink" title="sklearn 决策树 API DecisionTreeClassifier"></a>sklearn 决策树 API DecisionTreeClassifier</h4><ul>
<li>class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)<ul>
<li>criterion<ul>
<li>特征选择标准</li>
<li>“gini”或者”entropy”，前者代表基尼系数，后者代表信息增益。一默认”gini”，即CART算法。</li>
</ul>
</li>
<li>min_samples_split<ul>
<li>内部节点再划分所需最小样本数</li>
<li>这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。我之前的一个项目例子，有大概10万样本，建立决策树时，我选择了min_samples_split=10。可以作为参考。</li>
</ul>
</li>
<li>min_samples_leaf<ul>
<li>叶子节点最少样本数</li>
<li>这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。之前的10万样本项目使用min_samples_leaf的值为5，仅供参考。</li>
</ul>
</li>
<li>max_depth<ul>
<li>决策树最大深度</li>
<li>决策树的最大深度，默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间</li>
</ul>
</li>
<li>random_state<ul>
<li>随机数种子</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier, export_graphviz</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用决策树进行预测，修改max_depth试试</span></span><br><span class="line">dec = DecisionTreeClassifier()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">dec.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;预测的准确率： &quot;</span>, dec.score(x_test,y_test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导出决策树的结构</span></span><br><span class="line">export_graphviz(dec, out_file=<span class="string">&quot;tree.dot&quot;</span>,</span><br><span class="line">                feature_names=[<span class="string">&#x27;年龄&#x27;</span>, <span class="string">&#x27;pclass=1st&#x27;</span>, <span class="string">&#x27;pclass=2nd&#x27;</span>, <span class="string">&#x27;pclass=3rd&#x27;</span>, <span class="string">&#x27;女性&#x27;</span>, <span class="string">&#x27;男性&#x27;</span>])</span><br></pre></td></tr></table></figure>


<h3 id="决策树调参"><a href="#决策树调参" class="headerlink" title="决策树调参"></a>决策树调参</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/14.png" style="zoom:30%;">

<p>横轴表示在决策树创建过程中树的结点总数，纵轴表示决策树的预测精度。<br>实线显示的是决策树在训练集上的精度，虚线显示的则是在一个独立的测试集上测量出来的精度。</p>
<p>随着树的增长，在训练样集上的精度是单调上升的， 然而在独立的测试样例上测出的精度先上升后下降。也即是<strong>当决策树很深的时候，就可能出现过拟合的情况</strong></p>
<p>出现这种情况的原因：</p>
<ul>
<li>原因1：噪声、样本冲突，即错误的样本数据。</li>
<li>原因2：特征即属性不能完全作为分类标准。</li>
<li>原因3：巧合的规律性，数据量不够大。</li>
</ul>
<h4 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h4><p>（1） 每一个结点所包含的最小样本数目， 例如 10， 则该结点总样本数小于 10时， 则不再分；<strong>（min_samples_split）</strong><br>（2） 指定树的高度或者深度， 例如树的最大深度为 4；<strong>（max_depth）</strong><br>（3） 指定结点的熵小于某个值， 不再划分。 随着树的增长， 在训练样集上的精度是单调上升的， 然而在独立的测试样例上测出的精度先上升后下降。<strong>对应超参数是 min_impurity_decrease</strong></p>
<h4 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h4><p>后剪枝， 在已生成过拟合决策树上进行剪枝， 可以得到简化版的剪枝决策树。</p>
<h3 id="决策树的优缺点以及改进"><a href="#决策树的优缺点以及改进" class="headerlink" title="决策树的优缺点以及改进"></a>决策树的优缺点以及改进</h3><h4 id="优点："><a href="#优点：" class="headerlink" title="优点："></a><strong>优点：</strong></h4><ul>
<li>简单的理解和解释， <strong>树木可视化</strong>。</li>
<li>需要很少的数据准备， 其他技术通常需要数据归一化， 标准化（<strong>决策树不需要进行归一化和标准化</strong>）</li>
</ul>
<h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a><strong>缺点：</strong></h4><ul>
<li>决策树学习者可以创建不能很好地推广数据的过于复杂的树，这被称为<strong>过拟合</strong>。</li>
<li>决策树<strong>可能不稳定</strong>， 因为数据的小变化可能会导致完全不同的树被生成<strong>（弱分类器）</strong></li>
</ul>
<h4 id="改进："><a href="#改进：" class="headerlink" title="改进："></a><strong>改进：</strong></h4><ul>
<li>减枝 cart(Classification and regression tree)算法—这里我们来看下源码实现还有 png 图</li>
<li><strong>随机森林—解决过拟合</strong></li>
</ul>
<p>过拟合： 我们在训练集可以达到 100%的效果， 但是在测试集只能达到 70%， 80%的效果，<br>称为过拟合</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#调整决策树的参数</span></span><br><span class="line"><span class="comment"># 分割数据集到训练集合测试集</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.25</span>, random_state=<span class="number">4</span>)</span><br><span class="line"><span class="comment"># 进行处理（特征工程）特征-》类别-》one_hot编码</span></span><br><span class="line"><span class="built_in">dict</span> = DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这一步是对字典进行特征抽取</span></span><br><span class="line">x_train = <span class="built_in">dict</span>.fit_transform(x_train.to_dict(orient=<span class="string">&quot;records&quot;</span>))</span><br><span class="line">x_test = <span class="built_in">dict</span>.transform(x_test.to_dict(orient=<span class="string">&quot;records&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(x_train)</span></span><br><span class="line"><span class="comment"># # 用决策树进行预测，修改max_depth为10，发现提升了,min_impurity_decrease带来的增益要大于0.01才会进行划分</span></span><br><span class="line">dec = DecisionTreeClassifier(max_depth=<span class="number">7</span>,min_impurity_decrease=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">dec.fit(x_train, y_train)</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># # 预测准确率</span></span><br><span class="line">print(<span class="string">&quot;预测的准确率：&quot;</span>, dec.score(x_test, y_test))</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># # 导出决策树的结构</span></span><br><span class="line">export_graphviz(dec, out_file=<span class="string">&quot;tree1.dot&quot;</span>,</span><br><span class="line">                feature_names=<span class="built_in">dict</span>.get_feature_names_out())</span><br></pre></td></tr></table></figure>


<h3 id="集成学习方法-随机森林"><a href="#集成学习方法-随机森林" class="headerlink" title="集成学习方法-随机森林"></a>集成学习方法-随机森林</h3><p>集成学习通过建立几个模型来解决单一预测问题。它的工作原理是<strong>生成多个分类器/模型</strong>，各自独立地学习和作出预测。<strong>这些预测最后结合成组合预测，因此优于任何一个单分类的做出预测。</strong></p>
<h4 id="Bagging集成原理"><a href="#Bagging集成原理" class="headerlink" title="Bagging集成原理"></a>Bagging集成原理</h4><p>目标：把下面的圈和方块进行分类</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/15.png" style="zoom:40%;">

<p>实现过程：</p>
<p>1.采样不同数据集</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/16.png" style="zoom:40%;">

<p>2.训练分类器</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/17.png" style="zoom:40%;">

<p>3.平权投票，获取最终结果</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/18.png" style="zoom:40%;">

<h4 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h4><p>定义： 在机器学习中， 随机森林是一个包含多个决策树的分类器， 并且其输出的类别是由个别树输出的类别的众数而定。</p>
<p><strong>随机森林</strong> <strong>= Bagging +</strong> <strong>决策树</strong></p>
<p>例如, 如果你训练了5个树, 其中有4个树的结果是True, 1个树的结果是False, 那么最终投票结果就是True</p>
<p><strong>随机森林够造过程中的关键步骤</strong>（用N来表示训练用例（样本）的个数，M表示特征数目）：</p>
<p> <strong>1）一次随机选出一个样本，有放回的抽样，重复N次（有可能出现重复的样本）</strong></p>
<p> <strong>2） 随机去选出m个特征, m &lt;&lt;M，建立决策树</strong></p>
<ul>
<li><p>1.为什么要随机抽样训练集？　　</p>
<p>如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的</p>
</li>
<li><p>2.为什么要有放回地抽样？</p>
<p>如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是“有偏的”，都是绝对“片面的”（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决。</p>
</li>
</ul>
<h4 id="随机森林api"><a href="#随机森林api" class="headerlink" title="随机森林api"></a>随机森林api</h4><p>sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2)</p>
<ul>
<li><p>n_estimators：integer，optional（default = 10）森林里的树木数量120,200,300,500,800,1200</p>
</li>
<li><p>Criterion：string，可选（default =“gini”）分割特征的测量方法</p>
</li>
<li><p>max_depth：integer或None，可选（默认=无）树的最大深度 5,8,15,25,30</p>
</li>
<li><p>max_features=”auto”,每个决策树的最大特征数量</p>
<ul>
<li>If “auto”, then <code>max_features=sqrt(n_features)</code>.</li>
<li>If “sqrt”, then <code>max_features=sqrt(n_features)</code>(same as “auto”).</li>
<li>If “log2”, then <code>max_features=log2(n_features)</code>.</li>
<li>If None, then <code>max_features=n_features</code>.</li>
</ul>
</li>
<li><p>bootstrap：boolean，optional（default = True）是否在构建树时使用放回抽样</p>
</li>
<li><p>min_samples_split:节点划分最少样本数</p>
</li>
<li><p>min_samples_leaf:叶子节点的最小样本数</p>
</li>
<li><p>超参数：n_estimator, max_depth, min_samples_split,min_samples_leaf</p>
</li>
</ul>
<h4 id="bagging集成优点"><a href="#bagging集成优点" class="headerlink" title="bagging集成优点"></a>bagging集成优点</h4><p> <strong>Bagging + 决策树/线性回归/逻辑回归/深度学习… = bagging集成学习方法</strong></p>
<p>经过上面方式组成的集成学习方法:</p>
<ol>
<li><strong>均可在原有算法上提高约2%左右的泛化正确率</strong></li>
<li><strong>简单, 方便, 通用</strong></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 随机森林进行预测 （超参数调优），n_jobs充分利用多核的一个参数</span></span><br><span class="line">rf = RandomForestClassifier(n_jobs=-<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 120, 200, 300, 500, 800, 1200,n_estimators森林中决策树的数目，也就是分类器的数目</span></span><br><span class="line"><span class="comment"># max_samples  是最大样本数</span></span><br><span class="line"><span class="comment">#bagging类型</span></span><br><span class="line">param = &#123;<span class="string">&quot;n_estimators&quot;</span>: [<span class="number">1500</span>, <span class="number">2000</span>, <span class="number">5000</span>], <span class="string">&quot;max_depth&quot;</span>: [<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">8</span>,<span class="number">15</span>,<span class="number">25</span>]&#125;</span><br><span class="line"></span><br><span class="line">gc= GridSearchCV(rf, param_grid=param, cv=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">gc.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;准确率：&quot;</span>, gc.score(x_test, y_test))</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;查看选择的参数模型：&quot;</span>, gc.best_params_)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;选择最好的模型是：&quot;</span>, gc.best_estimator_)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;每个超参数每次交叉验证的结果：&quot;</span>, gc.cv_results_)</span><br></pre></td></tr></table></figure>


<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>分类算法</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title>论文解读：GPipe</title>
    <url>/58735.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><a id="more"></a>

<h2 id="背景："><a href="#背景：" class="headerlink" title="背景："></a><strong>背景：</strong></h2><p>增大模型规模通常是提高模型效果的有效方案，但是更多的模型参数，更复杂的模型计算逻辑，导致模型的训练过程需要更多的内存。同时计算设备 (GPU) 的内存资源往往非常非常有限，单台设备不足以支持超大规模模型的训练，而将模型分布在多台设备上则是一种可能的解决方案。</p>
<h2 id="简介："><a href="#简介：" class="headerlink" title="简介："></a><strong>简介：</strong></h2><p>GPipe 使用模型并行方案，将模型切分成一连串 stage，每个 stage 放在独立的设备（GPU/TPU）上，实现对超大规模模型的支持，同时利用 Pipeline 的方案，提高了模型并行模式下的设备利用率。最终 GPipe 通过更大规模的模型和更大的 batch_size，在图像和 NLP 的模型上都得到更好的模型效果。</p>
<h2 id="方案设计"><a href="#方案设计" class="headerlink" title="方案设计"></a><strong>方案设计</strong></h2><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/v2-dea46cace07195b41da36a71cfdf6169_r.jpg"></p>
<h3 id="1-划分-stage"><a href="#1-划分-stage" class="headerlink" title="1. 划分 stage"></a><strong>1. 划分 stage</strong></h3><p>如图 1.(a) 左图所示，将模型划分为连续的几个 stage，每个 stage 各自对应一个设备。这样就使得模型的大小可以突破单个设备内存的大小，因为一台设备只需要能够容纳部分模型的参数和计算。</p>
<h3 id="2-划分-Micro-Batch-和-Pipeline"><a href="#2-划分-Micro-Batch-和-Pipeline" class="headerlink" title="2. 划分 Micro-Batch 和 Pipeline"></a><strong>2. 划分 Micro-Batch 和 Pipeline</strong></h3><p>图 1.(b) 为一般的模型并行的运算模式，在每个时间点只有一台设备在处理计算逻辑，完成计算后将结果发送给下一台设备，其中设备空闲的时间称为 Bubble。GPipe 将 mini-batch 进一步划分成更小的 micro-batch，同时利用 pipipline 方案，每次处理一个 micro-batch 的数据，得到结果后，将该 micro-batch 的结果发送给下游设备，同时开始处理后一个 micro-batch 的数据，通过这套方案减小设备中的 Bubble。</p>
<p>GPipe 实验发现，当增加 micro_batch 数目的时候，设备上的 bubble 可忽略 (一定条件下)，一方面是由于如图 1.(c) 所示的的运行模式导致的，另一方面是由于 Re-Materializaition 方案中，计算梯度过程中的前向计算部分可以不依赖前一部分数据的计算结果 (即在图 1.(c) 中的 Bubble 阶段执行。)</p>
<h3 id="3-Re-Materialization"><a href="#3-Re-Materialization" class="headerlink" title="3. Re-Materialization"></a><strong>3. Re-Materialization</strong></h3><p>基于现有的训练框架，在前向计算时，训练框架会记录每一个算子的计算结果，用于 backward 时的梯度计算。在很多模型中，这部分的内存消耗可能大于模型参数，是限制模型大小（以及 batch-size）的一个重要原因。</p>
<p>Re-Materialization 具体是指在前向计算过程中，GPipe 只记录 stage 划分处的输出，在计算梯度时，GPipe 会重新执行前向计算逻辑，从而得到各个算子的前向结果，然后再计算梯度结果。</p>
<p>Re-Materialization 的好处是大大减少了内存需求，可以有效增大训练的 batch_size，比如正常的训练模式下 (忽略输出所占的内存，这部分一般占比较小)，内存能够支持 mini-batch=16，在 Gpipe 的模式下，可以将 micro-batch 设置为 16, 这样实际的 batch_size 就扩大了 min-batch/micor_batch 倍。</p>
<h2 id="方案效果"><a href="#方案效果" class="headerlink" title="方案效果"></a><strong>方案效果</strong></h2><h3 id="1-更大的模型"><a href="#1-更大的模型" class="headerlink" title="1. 更大的模型"></a><strong>1. 更大的模型</strong></h3><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/v2-a250b74bdb6cefb01ad66396d7d3b89c_r.jpg"></p>
<p>Gpipe 给出的结果中，AmoebaNet-D 模型最大的是~ 25GB，使用了 8 块 GPU,</p>
<p>对于 Transformer-L 模型，在使用 128 块 GPU 的情况下，模型最大达到 937.9GB，使用了 128 块 GPU。</p>
<h3 id="2-更大的-batch-size"><a href="#2-更大的-batch-size" class="headerlink" title="2. 更大的 batch_size"></a><strong>2. 更大的 batch_size</strong></h3><p>Transformer Big 的训练 batch size 从之前的 400k 提高到 4M。</p>
<h3 id="3-更好的模型效果"><a href="#3-更好的模型效果" class="headerlink" title="3. 更好的模型效果"></a><strong>3. 更好的模型效果</strong></h3><p>结合大模型和大 batch 的优势，大部分图像和 NLP 模型效果都得到了比之前更好的结果。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/v2-9b67b5ad569d20766eb810469912c311_r.jpg"><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/v2-572fc0b4f52ac7b0a8a7149bab83e6cc_b.jpg"></p>
<h3 id="4-训练效率"><a href="#4-训练效率" class="headerlink" title="4. 训练效率"></a><strong>4. 训练效率</strong></h3><p>按照 paper 所说，在增加设备个数的情况下，最优能达到接近线性的加速效果。不过没有看到与相同设备下的数据并行方案的对比。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/v2-f2da3e9dd8fdf1cadc88bf5bd8d86d41_b.jpg"><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/v2-f655032c4a5a7cfe663b756c73405c5c_b.jpg"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>这是谷歌出品的一个可扩展的模型并行库，用于训练巨型神经网络。通过批量分隔流水线并行方法，取得了几乎线性加速，并且按照论文表述，可支持各种深度网络，有很高的可靠性（同步梯度下降模式下，无论分区数量多少，都可以保证一致的训练）。</p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>毕设</tag>
      </tags>
  </entry>
  <entry>
    <title>毕业设计-日志</title>
    <url>/45623.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><a id="more"></a>

<h2 id="2021-3-3"><a href="#2021-3-3" class="headerlink" title="2021/3/3"></a>2021/3/3</h2><ul>
<li><p>阅读论文： <strong><em>PipeDream: Fast and Effiffifficient Pipeline Parallel DNN Training</em></strong> ，</p>
<p>​                   <strong><em>GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism</em></strong></p>
</li>
<li><p>学习PyTorch分布式架构的使用</p>
</li>
</ul>
<h2 id="2021-3-2"><a href="#2021-3-2" class="headerlink" title="2021/3/2"></a>2021/3/2</h2><ul>
<li>配置服务器环境，安装anaconda, pytorch, 导入torchgpipe。学习其使用</li>
<li>翻译论文：**<em>torchgpipe: On-the-ﬂy Pipeline Parallelism for Training Giant Models**</em></li>
</ul>
<h2 id="2021-2-2021-2-7"><a href="#2021-2-2021-2-7" class="headerlink" title="2021/2/-2021/2/7"></a>2021/2/-2021/2/7</h2><ul>
<li><p><strong>1.查阅分布式深度学习的相关知识</strong></p>
<p>（在对分布式深度学习有一定了解后，再研读论文，有一定的思路。）</p>
<p>‘’它使用同步随机梯度下降和流水线并行的方式进行训练，适用于任何由多个有序的层组成的深度神经网络(Deep Neural Networks, DNN)。 Gpipe通过跨不同的加速器来分割模型，并自动将一小批训练示例划分成更小的批量。 该模型允许GPipe的加速器进行并行操作，最大限度地提高了训练过程中的可扩展性。‘’层序</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/45623/Users\HP\AppData\Roaming\Typora\typora-user-images\image-20210115040032282.png" alt="image-20210115040032282"></p>
<p><strong>2.阅读论文–torchgpipe: On-the-flfly Pipeline Parallelism for Training Giant Models</strong></p>
<p>通过设置参数将一个计算划分为相同的几个小批次，实现并行。通过超参数调优，有效的将训练时间减少到一定大小的批次。</p>
<p>文中解决的问题：</p>
<p>每个分布式机器做完自己的任务，需要更新参数，用于同步网络参数。大量的参数需要同步时，可能会导致大量的通信负载</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/45623/Users\HP\AppData\Roaming\Typora\typora-user-images\image-20210115041557350.png" alt="image-20210115041557350"></p>
<p>1：</p>
<p>​    不同任务所占据的CPU时间不定，可长可短，解决方法：引入确定性时间周期，给出任务总排序</p>
<p>2：</p>
<p>​    由于pytorch的特点是，需要时调用，用完即弃，所以不保存前向图逻辑，只保留计算梯度等参数。所以反馈时不一定会顺序正确，所以需要引入fork，join原语标注顺序</p>
<p>3：</p>
<p>​    出现双向同步的问题，导致使用不足，出现双方相互等待。所以需要使用非默认C UDA流来避免这个问题，这样副本就不会阻止计算，除非计算必须等待数据。</p>
<p>4：</p>
<p>​    考虑有些任务会跳过一些批次，所以放宽顺序执行，增加设计一个接口来跳过中间张量。</p>
</li>
</ul>
<p>  <strong>提出的问题</strong></p>
<p>  ​    不同批次的大小该如何划分以达到最优？</p>
<p>  <strong>近期计划</strong></p>
<p>  1.开始在pytorch上实践torchpipe，尽早发现问题，解决问题。前期工作主要是实现torchpipe的思想，后期再进一步优化。</p>
<p>  2.继续研读相关论文，正确理解其含义，误解的开始</p>
<p>  介于假期不能线下开会，计划每两周开一次小会，将有关问题汇总和研究生学长讨论。</p>
]]></content>
      <categories>
        <category>日志</category>
      </categories>
      <tags>
        <tag>毕设</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-9-——线性回归</title>
    <url>/6552.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><h3 id="定义与公式"><a href="#定义与公式" class="headerlink" title="定义与公式"></a>定义与公式</h3><p><strong>线性回归(Linear regression)**是利用</strong>回归方程(函数)<strong>对</strong>一个或多个自变量(特征值)和因变量(目标值)之间**关系进行建模的一种分析方式。</p>
<ul>
<li>特点：只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归</li>
</ul>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/1.png" style="zoom:40%;">

<ul>
<li>线性回归用矩阵表示举例</li>
</ul>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/2.png" style="zoom:40%;">

<p>那么怎么理解呢？我们来看几个例子</p>
<ul>
<li>期末成绩：0.7×考试成绩+0.3×平时成绩</li>
<li>房子价格 = 0.02×中心区域的距离 + 0.04×城市一氧化氮浓度 + (-0.12×自住房平均房价) + 0.254×城镇犯罪率</li>
</ul>
<p>上面两个例子，<strong>我们看到特征值与目标值之间建立了一个关系，这个关系可以理解为线性模型</strong>。</p>
<h3 id="线性回归的损失和优化"><a href="#线性回归的损失和优化" class="headerlink" title="线性回归的损失和优化"></a>线性回归的损失和优化</h3><p><strong>假设刚才的房子例子，真实的数据之间存在这样的关系</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">真实关系：真实房子价格 = <span class="number">0.02</span>×中心区域的距离 + <span class="number">0.04</span>×城市一氧化氮浓度 + (-<span class="number">0.12</span>×自住房平均房价) + <span class="number">0.254</span>×城镇犯罪率</span><br></pre></td></tr></table></figure>
<p>那么现在呢，我们随意指定一个关系（猜测）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">随机指定关系：预测房子价格 = <span class="number">0.25</span>×中心区域的距离 + <span class="number">0.14</span>×城市一氧化氮浓度 + <span class="number">0.42</span>×自住房平均房价 + <span class="number">0.34</span>×城镇犯罪率</span><br></pre></td></tr></table></figure>
<p>请问这样的话，会发生什么？真实结果与我们预测的结果之间是不是存在一定的误差呢？类似这样样子</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/3.png" style="zoom:40%;">

<p>既然存在这个误差，那我们就将这个误差给衡量出来</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>总损失定义为：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/4.png" style="zoom:50%;">

<ul>
<li>yi为第i个训练样本的真实值</li>
<li>h(xi)为第i个训练样本特征值组合预测函数</li>
<li>又称最小二乘法</li>
</ul>
<p>如何去减少这个损失，使我们预测的更加准确些？既然存在了这个损失，我们一直说机器学习有自动学习的功能，在线性回归这里更是能够体现。这里可以通过一些优化方法去优化（其实是数学当中的求导功能）回归的总损失！！！<strong>目的是找到最小损失对应的W值，这是重点</strong></p>
<h3 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h3><p><strong>如何去求模型当中的W，使得损失最小？（目的是找到最小损失对应的W值）</strong></p>
<p>线性回归经常使用的两种优化算法：即<strong>正规方程</strong>和<strong>梯度下降</strong></p>
<h3 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/6.png" style="zoom:50%;">

<p><strong>推导过程：</strong></p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/5.png" style="zoom:50%;">

<ul>
<li><strong>推导方式二【拓展】：</strong></li>
</ul>
<p><a href="https://www.jianshu.com/p/2b6633bd4d47">https://www.jianshu.com/p/2b6633bd4d47</a></p>
<h4 id="正规方程接口api："><a href="#正规方程接口api：" class="headerlink" title="正规方程接口api："></a>正规方程接口api：</h4><ul>
<li>sklearn.linear_model.LinearRegression(fit_intercept=True)<ul>
<li>通过正规方程优化</li>
<li>fit_intercept：是否计算偏置</li>
<li>LinearRegression.coef_：回归系数</li>
<li>LinearRegression.intercept_：偏置</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error,classification_report,roc_auc_score</span><br><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">线性回归直接预测房子价格</span></span><br><span class="line"><span class="string">:return: None</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 获取数据</span></span><br><span class="line">lb = fetch_california_housing(data_home=<span class="string">&#x27;data&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分割数据集到训练集和测试集</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(lb.data, lb.target, test_size=<span class="number">0.25</span>, random_state=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#</span></span><br><span class="line">print(x_train.shape)</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># # 进行标准化处理(?) 目标值处理？</span></span><br><span class="line"><span class="comment"># # 特征值和目标值是都必须进行标准化处理, 实例化两个标准化API</span></span><br><span class="line">std_x = StandardScaler()</span><br><span class="line"></span><br><span class="line">x_train = std_x.fit_transform(x_train)</span><br><span class="line">x_test = std_x.transform(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 目标值进行了标准化</span></span><br><span class="line">std_y = StandardScaler()</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># temp = y_train.reshape(-1, 1) #-1代表把剩余的元素都堆到哪一维</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># #标签进行标准化</span></span><br><span class="line"><span class="comment"># # 目标值是一维的，这里需要传进去2维的</span></span><br><span class="line">y_train = std_y.fit_transform(y_train.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># print(y_train.shape)</span></span><br><span class="line">y_test = std_y.transform(y_test.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># print(y_test.shape)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # estimator预测</span></span><br><span class="line"><span class="comment"># # # 正规方程求解方式预测结果，正规方程进行线性回归</span></span><br><span class="line">lr = LinearRegression()</span><br><span class="line"><span class="comment"># fit是耗时的</span></span><br><span class="line">lr.fit(x_train, y_train)</span><br><span class="line"><span class="comment">#回归系数可以看特征与目标之间的相关性</span></span><br><span class="line">print(<span class="string">&#x27;回归系数&#x27;</span>, lr.coef_)</span><br><span class="line"></span><br><span class="line">y_predict = lr.predict(x_test)</span><br><span class="line"><span class="comment"># 预测测试集的房子价格，通过inverse得到真正的房子价格</span></span><br><span class="line">y_lr_predict = std_y.inverse_transform(y_predict)</span><br><span class="line"><span class="comment"># 保存训练好的模型，模型中保存的是w的值，也保存了模型结构</span></span><br><span class="line"><span class="comment">#保存模型放在fit之后即可</span></span><br><span class="line"></span><br><span class="line">joblib.dump(lr, <span class="string">&quot;./tmp/test.pkl&quot;</span>)</span><br><span class="line">print(<span class="string">&quot;正规方程测试集里面每个房子的预测价格：&quot;</span>, y_predict[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"><span class="comment">#下面是求测试集的损失</span></span><br><span class="line">print(<span class="string">&quot;正规方程的均方误差：&quot;</span>, mean_squared_error(y_test, y_predict))</span><br></pre></td></tr></table></figure>


<h3 id="梯度下降-Gradient-Descent"><a href="#梯度下降-Gradient-Descent" class="headerlink" title="梯度下降(Gradient Descent)"></a>梯度下降(Gradient Descent)</h3><p>梯度下降法的基本思想可以类比为一个下山的过程。</p>
<p>假设这样一个场景：一个人被困在山上，需要从山上下来(i.e. 找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用梯度下降算法来帮助自己下山。具体来说就是，以他当前的所处的位置为基准，<strong>寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走</strong>，（同理，如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝着最陡峭的方向往上走）。然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷。</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/7.png" style="zoom:30%;">

<p>梯度下降的基本过程就和下山的场景很类似。</p>
<p>首先，我们有一个<strong>可微分的函数</strong>。这个函数就代表着一座山。</p>
<p>我们的目标就是找到<strong>这个函数的最小值</strong>，也就是山底。</p>
<p>根据之前的场景假设，最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到函数中，就是<strong>找到给定点的梯度</strong> ，然后朝着梯度相反的方向，就能让函数值下降的最快！因为梯度的方向就是函数之变化最快的方向。 所以，我们重复利用这个方法，反复求取梯度，最后就能到达局部的最小值，这就类似于我们下山的过程。而求取梯度就确定了最陡峭的方向，也就是场景中测量方向的手段。</p>
<h4 id="梯度下降（Gradient-Descent）公式"><a href="#梯度下降（Gradient-Descent）公式" class="headerlink" title="梯度下降（Gradient Descent）公式"></a>梯度下降<strong>（</strong>Gradient Descent）公式</h4><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/8.png" style="zoom:50%;">

<ul>
<li>α在梯度下降算法中被称作为<strong>学习率</strong>或者<strong>步长</strong>，意味着我们可以通过α来控制每一步走的距离，以保证不要步子跨的太大扯着蛋，哈哈，其实就是不要走太快，错过了最低点。同时也要保证不要走的太慢，导致太阳下山了，还没有走到山下。所以α的选择在梯度下降法中往往是很重要的！α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点！</li>
<li>梯度前加一个负号，就意味着朝着梯度相反的方向前进！我们在前文提到，梯度的方向实际就是函数在此点上升最快的方向！而我们需要朝着下降最快的方向走，自然就是负的梯度的方向，所以此处需要加上负号</li>
</ul>
<h4 id="梯度下降接口api："><a href="#梯度下降接口api：" class="headerlink" title="梯度下降接口api："></a>梯度下降接口api：</h4><ul>
<li>sklearn.linear_model.SGDRegressor(loss=”squared_loss”, fit_intercept=True, learning_rate =’invscaling’, eta0=0.01)<ul>
<li>SGDRegressor类实现了随机梯度下降学习，它支持不同的<strong>loss函数和正则化惩罚项</strong>来拟合线性回归模型。</li>
<li>loss:损失类型<ul>
<li><strong>loss=”squared_loss”: 普通最小二乘法</strong></li>
</ul>
</li>
<li>fit_intercept：是否计算偏置</li>
<li>learning_rate : string, optional<ul>
<li>学习率填充</li>
<li><strong>‘constant’: eta = eta0</strong></li>
<li><strong>‘optimal’: eta = 1.0 / (alpha * (t + t0)) [default]</strong></li>
<li>‘invscaling’: eta = eta0 / pow(t, power_t)<ul>
<li><strong>power_t=0.25:存在父类当中</strong></li>
</ul>
</li>
<li><strong>对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用eta0来指定学习率。</strong></li>
</ul>
</li>
<li>SGDRegressor.coef_：回归系数</li>
<li>SGDRegressor.intercept_：偏置</li>
</ul>
</li>
</ul>
<h3 id="梯度下降和正规方程的对比"><a href="#梯度下降和正规方程的对比" class="headerlink" title="梯度下降和正规方程的对比"></a>梯度下降和正规方程的对比</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/9.png" style="zoom:50%;">

<p>选择：</p>
<ul>
<li>小规模数据：<ul>
<li><strong>LinearRegression(不能解决拟合问题)</strong></li>
<li>岭回归</li>
</ul>
</li>
<li>大规模数据：SGDRegressor</li>
</ul>
<h3 id="回归性能评估"><a href="#回归性能评估" class="headerlink" title="回归性能评估"></a>回归性能评估</h3><p>均方误差(Mean Squared Error)MSE)评价机制：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/10.png" style="zoom:40%;">

<p><strong>注：yi为预测值，¯y为真实值</strong></p>
<ul>
<li>sklearn.metrics.mean_squared_error(y_true, y_pred)<ul>
<li>均方误差回归损失</li>
<li>y_true:真实值</li>
<li>y_pred:预测值</li>
<li>return:浮点数结果</li>
</ul>
</li>
</ul>
<h2 id="正则化线性模型"><a href="#正则化线性模型" class="headerlink" title="正则化线性模型"></a>正则化线性模型</h2><h2 id="Ridge-Regression-岭回归，又名-Tikhonov-regularization"><a href="#Ridge-Regression-岭回归，又名-Tikhonov-regularization" class="headerlink" title="Ridge Regression (岭回归，又名 Tikhonov regularization)"></a>Ridge Regression (岭回归，又名 Tikhonov regularization)</h2><p>岭回归是线性回归的正则化版本，即在原来的线性回归的 cost function 中添加正则项（regularization term）:</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/11.png" style="zoom:50%;">

<h3 id="岭回归接口api："><a href="#岭回归接口api：" class="headerlink" title="岭回归接口api："></a>岭回归接口api：</h3><ul>
<li>sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True,solver=”auto”, normalize=False)<ul>
<li>具有l2正则化的线性回归</li>
<li>alpha:正则化力度，也叫 λ<ul>
<li><strong>λ取值：0<del>1 1</del>10</strong></li>
</ul>
</li>
<li>solver:会根据数据自动选择优化方法<ul>
<li><strong>sag:如果数据集、特征都比较大，选择该随机梯度下降优化</strong></li>
</ul>
</li>
<li>normalize:数据是否进行标准化<ul>
<li>normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据</li>
</ul>
</li>
<li>Ridge.coef_:回归权重</li>
<li>Ridge.intercept_:回归偏置</li>
</ul>
</li>
</ul>
<p><strong>Ridge方法相当于SGDRegressor(penalty=’l2’, loss=”squared_loss”),只不过SGDRegressor实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG)</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># # # 岭回归去进行房价预测</span></span><br><span class="line"><span class="comment">#岭回归是对线性回归加入L2正则化，L2正则化是对系数的平方和进行惩罚</span></span><br><span class="line"><span class="comment">#alpha就是补偿的系数</span></span><br><span class="line"><span class="comment">#正规方程求解，加补偿就可以让正规方程可逆</span></span><br><span class="line">rd = Ridge(alpha=<span class="number">0.02</span>)</span><br><span class="line"></span><br><span class="line">rd.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">print(rd.coef_)</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># # 预测测试集的房子价格</span></span><br><span class="line">print(rd.predict(x_test).shape)</span><br><span class="line"><span class="comment"># y_rd_predict = std_y.inverse_transform(rd.predict(x_test))</span></span><br><span class="line">y_predict = rd.predict(x_test)</span><br><span class="line"><span class="comment"># print(&quot;岭回归里面每个房子的预测价格：&quot;, y_rd_predict)</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;岭回归的均方误差：&quot;</span>, mean_squared_error(y_test, y_predict))</span><br><span class="line"><span class="comment"># print(&quot;岭回归的均方误差：&quot;, mean_squared_error(std_y.inverse_transform(y_test), y_rd_predict))</span></span><br></pre></td></tr></table></figure>
<h3 id="观察正则化程度的变化，对结果的影响？"><a href="#观察正则化程度的变化，对结果的影响？" class="headerlink" title="观察正则化程度的变化，对结果的影响？"></a>观察正则化程度的变化，对结果的影响？</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/14.png" style="zoom:45%;">

<ul>
<li><p>正则化力度越大，权重系数会越小</p>
</li>
<li><p>正则化力度越小，权重系数会越大</p>
</li>
</ul>
<h2 id="Lasso-Regression-Lasso-回归"><a href="#Lasso-Regression-Lasso-回归" class="headerlink" title="Lasso Regression(Lasso 回归)"></a>Lasso Regression(Lasso 回归)</h2><p>Lasso 回归是线性回归的另一种正则化版本，正则项为权值向量的 L1 范数。</p>
<p>Lasso回归的代价函数 ：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/12.png" style="zoom:60%;">

<p>【注意 】</p>
<ul>
<li>Lasso Regression 的代价函数在 θi=0处是不可导的.</li>
<li>解决方法：在θi=0处用一个次梯度向量(subgradient vector)代替梯度，如下式</li>
<li>Lasso Regression 的次梯度向量</li>
</ul>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/13.png" style="zoom:45%;">

<p>Lasso Regression 有一个很重要的性质是：倾向于完全消除不重要的权重。</p>
<p>例如：当α 取值相对较大时，高阶多项式退化为二次甚至是线性：高阶多项式特征的权重被置为0。</p>
<p>也就是说，Lasso Regression 能够自动进行特征选择，并输出一个稀疏模型（只有少数特征的权重是非零的）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># # # Lasso回归去进行房价预测</span></span><br><span class="line"><span class="comment">#alpha就是补偿的系数</span></span><br><span class="line">print(x_train.shape)</span><br><span class="line">print(y_train.shape)</span><br><span class="line">ls = Lasso(alpha=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">ls.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">print(ls.coef_)</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># # 预测测试集的房子价格</span></span><br><span class="line">print(ls.predict(x_test).shape)</span><br><span class="line">print(<span class="string">&#x27;-&#x27;</span>*<span class="number">50</span>)</span><br><span class="line"><span class="comment"># y_ls_predict = std_y.inverse_transform(ls.predict(x_test).reshape(-1,1))</span></span><br><span class="line">y_predict = ls.predict(x_test)</span><br><span class="line"><span class="comment"># print(&quot;Lasso回归里面每个房子的预测价格：&quot;, y_rd_predict)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">print(<span class="string">&quot;Lasso回归的均方误差：&quot;</span>, mean_squared_error(y_test, y_predict))</span><br><span class="line"><span class="comment"># print(&quot;Lasso回归的均方误差：&quot;, mean_squared_error(std_y.inverse_transform(y_test), y_ls_predict))</span></span><br></pre></td></tr></table></figure>


<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>回归算法</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title>论文解读：MuCGEC</title>
    <url>/42505.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="MuCGEC-a-Multi-Reference-Multi-Source-Evaluation-Dataset-for-Chinese-Grammatical-Error-Correction"><a href="#MuCGEC-a-Multi-Reference-Multi-Source-Evaluation-Dataset-for-Chinese-Grammatical-Error-Correction" class="headerlink" title="MuCGEC- a Multi-Reference Multi-Source Evaluation Dataset for Chinese Grammatical Error Correction"></a>MuCGEC- a Multi-Reference Multi-Source Evaluation Dataset for Chinese Grammatical Error Correction</h2><hr>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><ul>
<li>用于中文语法纠错（CGEC）的多参考多源评估数据集,包含收录自CSL中的7063个句子。其中每个句子由三名annotator更正，他们的更正由senior annotator仔细审查，每个句子产生 2.3 条参考。</li>
<li>使用两个主流CGEC模型进行实验，并用一个大规模预训练模型进行提升，在数据集上达到了有竞争力的基准表现</li>
<li>使用基于字符的指标。</li>
</ul>
<h3 id="1-introduction"><a href="#1-introduction" class="headerlink" title="1. introduction"></a>1. introduction</h3><ul>
<li>GEC：表示对有可能有语法错误的输入文字句子（生句子）进行检测以及纠正所有的错误并且生成正确干净的句子。</li>
<li>英文数据集(EGEC)有着丰富的高质量人工标注评估数据，但中文数据集(CGEC)相对欠缺。主流的CGEC是NLPCC18和CGED。</li>
<li>现存的CGEC缺少多参考的数据，大部分来自上述两个数据集，这可能是由于采用了不同的标注工作流。</li>
</ul>
<p>MuCGEC：解决下述的两个问题：单参考问题和单文本源问题</p>
<ul>
<li><p><strong>多参考的标注</strong>对整个<strong>GEC模型以及GEC数据标注</strong>都很重，因为对于一个不正确的句子，通常存在不止一个具有相似含义的可接受引用。</p>
</li>
<li><ol>
<li>如果 GEC 模型输出正确的参考，但与评估数据中给出的参考不同，那么模型性能将被不公平地低估 –&gt;增加引用数量</li>
<li>施加单引用约束会使数据标注出现问题，比如annotator提交不同的但同样是可以接受的修正，senior不知道如何决定哪个更好</li>
</ol>
<p>（认为是一句话的歧义问题以及更改在单参考的环境下会出现错误评估以及评估失效）</p>
</li>
<li><p>现有的 CGEC 数据集还从单一文本源收集句子，这可能不足以进行稳健的模型评估</p>
</li>
</ul>
<p>总结：基于基于浏览器的在线标注工具，每个句子分配给三名标注员进行独立批改，并分配给一名高级标注员进行终审。 标注者可以提交多个引用，高级标注者除了拒绝不正确的提交外，还可以补充新的引用。 通过这种方式，我们的目标是产生尽可能多的参考资料。 总之，这项工作做出了以下贡献。</p>
<p>1）我们新构建的 MuCGEC 由 7,063 个句子组成，这些句子来自CSL文本的三个代表性来源。 每个句子平均获得 2.3 个参考。 我们对新数据集进行详细分析以获得更多见解。</p>
<p>2）我们使用Seq2Edit和Seq2Seq模型进行基准实验，这两种模型都通过预训练语言模型 (PLM) 进行了增强，我们还尝试了一种非常有效的集成策略。使用比基于单词的评估指标更简单的基于字符的评估指标。</p>
<h3 id="2-Data-Annotation"><a href="#2-Data-Annotation" class="headerlink" title="2. Data Annotation"></a>2. Data Annotation</h3><h4 id="2-1-Multi-Source-Data-Selection"><a href="#2-1-Multi-Source-Data-Selection" class="headerlink" title="2.1 Multi-Source Data Selection"></a>2.1 Multi-Source Data Selection</h4><p>在CSL学习者文本上调查各种各样的中文语法错误。</p>
<ul>
<li>数据源：<ol>
<li>重新标注NLPCC18(包含北大语料库)</li>
<li>重新标注CGED-2018 和 CGED-2020 测试数据集(来自HSK的写作章节)，从总共 5,006 个句子中删除标记为正确的句子后，我们获得了 3,137 个可能错误的句子用于重新标注。</li>
<li>NLPCC2018 共享任务组织者从 Lang8 收集了约 717,000 个中文句子及其更正。我们随机选择 2,000 个 30 到 60 个字符的句子进行重新标注。</li>
</ol>
</li>
</ul>
<p>放弃所有原始更正，并按照我们新的标注指南和工作流程直接从头开始重新标注。</p>
<h4 id="2-2-Annotation-Paradigm-Direct-Rewriting"><a href="#2-2-Annotation-Paradigm-Direct-Rewriting" class="headerlink" title="2.2 Annotation Paradigm: Direct Rewriting"></a>2.2 Annotation Paradigm: Direct Rewriting</h4><p>构建 GEC 数据的标注范式主要有两种类型，即<strong>错误编码</strong>和<strong>直接重写</strong>。</p>
<ol>
<li>错误编码范式：要求标注者明确标记原始句子中的错误跨度，然后选择其错误类型，最后进行更正。(单词纠正)</li>
</ol>
<p>存在的问题：</p>
<ul>
<li>不同的标注者很难就错误跨度的边界及其错误类型达成一致，尤其是当有许多类别需要考虑时-&gt;导致标注工作量的增加和 标注质量下降</li>
<li>在复杂的标注范式下，标注者忽视结果引用的流畅性，有时甚至会导致表达不自然。</li>
</ul>
<ol start="2">
<li>直接重写范式：要求标注者直接重写输入句子并生成相应的语法正确的句子，而不改变原始含义。这种标注范式已被证明是高效且具有成本效益的（全句重写）</li>
</ol>
<p>我们采用直接重写范式。 除了上述优点外，我们相信这种范式可以帮助提高引用的多样性，因为标注者可以更自由地纠正错误。</p>
<h4 id="2-3-Annotation-Guidelines"><a href="#2-3-Annotation-Guidelines" class="headerlink" title="2.3 Annotation Guidelines"></a>2.3 Annotation Guidelines</h4><p>编写了 30 页的 CGEC 标注综合指南，并根据反馈逐步完善。指南采用了两层层次的错误分类法，包括5种主要错误类型和14种次要错误类型</p>
<ul>
<li>标点：丢失; 冗余; 滥用</li>
<li>拼写：拼音混乱； 字形混乱； 语无伦次</li>
<li>单词：丢失; 冗余; 滥用</li>
<li>句法：词序；混合语法模式</li>
<li>语用学：逻辑不一致； 歧义； 常识性错误</li>
</ul>
<p><strong>处理word-missing errors遇到的问题</strong>：以往的CGEC 数据集会直接插入特定词，但漏词本身是与上下文有关的。导致语义错误，能会给 GEC 模型评估带来麻烦，因为还有很多其他可接受的候选词。-&gt; 使用missing components (MC)标签标注，因为“[MC]”不包含在现有的训练数据和词汇表中。 我们将这个问题留作未来的工作。</p>
<h4 id="2-4-Annotation-Workflow-and-Tool"><a href="#2-4-Annotation-Workflow-and-Tool" class="headerlink" title="2.4 Annotation Workflow and Tool"></a>2.4 Annotation Workflow and Tool</h4><ul>
<li><p>将每个句子分配给三个随机标注者进行独立标注。 然后将他们提交的内容汇总并发送给随机的高级标注者（审阅者）进行审阅。</p>
</li>
<li><p>高级标注者的工作包括： 1）将不正确的引用修改为正确的引用（有时只是拒绝它们）； 2）根据指南添加新的正确引用。</p>
</li>
<li><p>评审后，被接受的引用被定义为<strong>最终黄金饮用</strong></p>
</li>
<li><p>采用一种自学机制，允许标注者在提交不正确的引用时从错误中吸取教训。：简而言之，就是标注者提交的引用不是黄金引用，则可以对其进行修改成一个正确的引用。另加上投诉机制，对自学机制进行优化。我们发现自学和投诉机制可以引发非常有益的讨论。</p>
</li>
</ul>
<h4 id="2-5-Annotation-Process"><a href="#2-5-Annotation-Process" class="headerlink" title="2.5 Annotation Process"></a>2.5 Annotation Process</h4><p>聘请了21名母语为汉语、熟悉汉语语法的本科生作为兼职标注员。所有参与者都被要求每天至少标注 1 小时。 整个标注过程历时约3个月。</p>
<h4 id="2-6-Ethical-Issues"><a href="#2-6-Ethical-Issues" class="headerlink" title="2.6 Ethical Issues"></a>2.6 Ethical Issues</h4><p>这三个来源的所有数据都是公开的。 同时，我们已获得 NLPCC-2018 和 CGED 共享任务的组织者的许可</p>
<h3 id="3-Analysis-of-Our-Annotated-Data"><a href="#3-Analysis-of-Our-Annotated-Data" class="headerlink" title="3 Analysis of Our Annotated Data"></a>3 Analysis of Our Annotated Data</h3><ol>
<li>与原标注相比，新标注中的错句比例有一定减少，体现了我们对<strong>过度修正现象</strong>的严格控制。</li>
<li>关于句子长度，NLPCC18 的句子最短，而 CGED 的句子要长得多。因为考生喜欢写长句子…</li>
<li>重新标注的 NLPCC18 中的每个句子平均收到 2.5 个引用，这是原始 NLPCC18 数据中的两倍多。可以使我们的数据集在评估时更可靠</li>
<li>比较不同数据集中每个引用的基于字符的编辑数量。我们可以看到编辑次数与句子长度密切相关。平均句子长度和编辑次数的差异表明三个数据源在质量和难度上可能存在系统性差异，我们认为这有助于评估模型的泛化能力。</li>
<li>标注者更倾向于提供单一引用，因为更经济，他们觉得应该让更多标注着提供他们的最佳饮用。</li>
<li>我们通过根据最终的黄金参考评估所有标注提交来计算基于字符的 F0.5 分数。平均 F0.5 为 72.12。</li>
<li>最佳标注者获得 82.34 F0.5 分数，而完成最多任务的标注者仅获得 68.32 分。 这表明我们在计算工资时应该更加关注标注质量，避免标注者过于关注标注速度。</li>
<li>标注者常犯的错误：最常见的错误是由于更正不完整造成的，占无效引用的 56.7%。很难无遗漏地纠正所有错误，第二类错误是错误更正，即旧错误的更正导致新错误，此外，有 11.0% 的投稿因意思改变而被拒绝。</li>
</ol>
<h3 id="4-Benchmark-Models"><a href="#4-Benchmark-Models" class="headerlink" title="4 Benchmark Models"></a>4 Benchmark Models</h3><p>为了了解前沿的 GEC 模型在我们的数据上的表现如何，我们采用了两种主流的 GEC 方法，即 Seq2Edit 和 Seq2Seq。</p>
<ul>
<li><p><strong>Seq2Edit 模型</strong>将 GEC 视为序列标记任务，并通过一系列令牌级别的编辑（包括插入、删除和替换）执行错误更正（Malmi 等人，2019）。 一个token在英文中对应一个词或子词，在中文中对应一个字。 我们采用 GECToR（Omelianchuk 等人，2020 年）进行微小修改以适应中文，从而在 EGEC 数据集上实现了最好的表现(SOTA)。</p>
</li>
<li><p>使用StructBERT （流行的PLM，因为它在微调后具有优越的性能）作为encoder以提升GECToR</p>
</li>
<li><p><strong>Seq2Seq 模型</strong>直接将 GEC 视为单语翻译任务。 最近的工作建议使用 T5 或 BART 等 PLM 来增强基于 Transformer 的 Seq2Seq EGEC 模型。 与 BERT不同，T5 和 BART 是专门为文本生成而设计的。 因此，继续使用 GEC 数据对它们进行培训很简单。 我们遵循这些工作并利用 Shao 等人最近提出的中文 BART。初始化我们的 Seq2Seq 模型。</p>
</li>
<li><p><strong>集成模型</strong>。 之前的几项工作已经证明了模型集成对 CGEC 的有效性。 在这项工作中，我们清楚地观察到上述两个模型在修复不同错误类型方面的互补能力，因此尝试将它们结合起来。<strong>Seq2Seq 模型 善于处理 词序 错误 ，而Seq2Edit 模型 在 冗余错误上表现较好</strong></p>
</li>
<li><p>我们用两种集成设定进行实验：</p>
<p>1）一个 Seq2Edit 和一个 Seq2Seq，用“1×Seq2Edit+1×Seq2Seq”表示</p>
<p>2）三个 Seq2Edit 和三个 Seq2Seq，用“3×Seq2Edit+3×Seq2Seq”表示。</p>
<p>这三个 Seq2Edit模型是使用不同的随机种子进行初始化得到的，Seq2Seq也是如此。</p>
</li>
</ul>
<ul>
<li><strong>其他设置</strong>。 为了获得两种模型的单模型性能，我们使用不同的随机种子分别运行它们三次进行初始化并计算平均指标。 对于“1×Seq2Edit+1×Seq2Seq”，我们随机选择一对单一模型。 对于“3×Seq2Edit+3×Seq2Seq”，我们汇总了所有六个单一模型的结果。</li>
</ul>
<h3 id="5-Experiments-on-NLPCC18-Orig-Data"><a href="#5-Experiments-on-NLPCC18-Orig-Data" class="headerlink" title="5 Experiments on NLPCC18-Orig Data"></a>5 Experiments on NLPCC18-Orig Data</h3><p>训练数据：将训练数据严格限制为公共资源，即 Lang8 数据 和 HSK 数据，<strong>按照 Junczys-Dowmunt 等人 (2018) 的重新加权程序，我们将 HSK 数据复制五次，并将它们与 Lang8 数据合并。</strong> 问题？？</p>
<ul>
<li>当仅使用 Lang8 进行训练时，我们的单个 Seq2Seq 模型已经具有相当的竞争力。 它的性能在 F0.5 中仅比 MaskGEC (Zhao and Wang, 2020) 低 1 个百分点。</li>
<li>添加 HSK 训练数据可将我们所有模型的性能提高约 4 个百分点。 我们的两个基准模型已经在单模型设置下实现了 SOTA 性能。</li>
<li>与单个模型相比，模型集成技术带来了明显的性能提升（超过 5 分）。 然而，增加组件模型数量的收益似乎相当小。</li>
</ul>
<h3 id="6-Experiments-on-MuCGEC"><a href="#6-Experiments-on-MuCGEC" class="headerlink" title="6 Experiments on MuCGEC"></a>6 Experiments on MuCGEC</h3><h4 id="6-1-Data-Splits"><a href="#6-1-Data-Splits" class="headerlink" title="6.1 Data Splits"></a>6.1 Data Splits</h4><p>在这项工作中，我们建议通过从 CGED 源中随机选择 1,125 个句子，为我们新标注的数据集提供一个固定的开发集，表示为 CGED-dev。 剩下的5938个句子作为测试集，其中每个数据源的句子数量大致相等，即NLPCC18-test 1996个句子，CGED-test 2000个句子，Lang8-test 1942个句子。（以往是随机选择）</p>
<h4 id="6-2-Evaluation-Metrics"><a href="#6-2-Evaluation-Metrics" class="headerlink" title="6.2 Evaluation Metrics"></a>6.2 Evaluation Metrics</h4><ul>
<li><strong>基于单词的指标存在问题：</strong>以前的 CGEC 数据集是根据单词序列进行标注的，并采用基于单词的度量标准进行评估。 在标注和评估之前，需要使用中文分词 (CWS) 模型将句子分割成单词。因为1）CWS 模型不可避免地会产生分词错误。 2）存在多种异构的CWS标准。3）我们发现正确的编辑可能会因为词边界不匹配而被判断为错误的。</li>
</ul>
<p>这项工作改为采用基于字符的跨度级别评估指标。 首先，给定一个输入句子和一个修正，我们获得了具有最小编辑距离的最佳字符级编辑序列。 我们考虑三种类型的字符级编辑，对应三种错误类型：<br>• 为冗余错误，删除一个字符；<br>• 为缺失的错误，插入一个字符；<br>• 为替换错误，将一个字符替换为另一个；</p>
<p>然后，我们按照之前在 EGEC 和 CGEC 中的做法，通过合并相同类型的连续编辑，将所有字符级别的编辑转换为跨度级别。上述两个步骤应用于系统输出序列和黄金饮用，将它们转换为跨度级编辑集。 最后，我们可以通过比较它们来计算P/R/F值。 如果有多个黄金饮用，我们会选择 F-score 最高的那个。</p>
<ul>
<li>跨度级词序错误：<strong>在计算整体指标时</strong>，我们只考虑以上三类错误。 <strong>在分析时</strong>，我们区分了第四种错误类型——词序错误。 跨度级词序错误通常由冗余错误和缺失错误组成，其中删除的跨度与插入的跨度相同。 我们使用简单的启发式规则来识别此类错误</li>
</ul>
<h4 id="6-3-Results-and-Analysis"><a href="#6-3-Results-and-Analysis" class="headerlink" title="6.3 Results and Analysis"></a>6.3 Results and Analysis</h4><ul>
<li><p>性能变化的总体趋势与表 5 中原始 NLPCC18 数据集上的基本一致。首先，Seq2Seq 和 Seq2Edit 模型在 F0.5 上表现相当接近，但在精度和召回率上明显表现出不同的强度，其次，模型集成方法大大提高了性能。</p>
</li>
<li><p>一个有趣的观察是，在 MuCGEC 上，“3×Seq2Edit+3×Seq2Seq”在 All-test 和所有三个子集上明显优于“1×Seq2Edit+1×Seq2Seq”。 相比之下，原始 NLPCC18 测试数据中两者的改进并不大。 我们怀疑这可能表明多参考数据集可以更准确地评估模型性能。 然而，它可能需要进一步的人类调查以获得更多见解。</p>
</li>
<li><p>最后，模型和人类之间仍然存在巨大的性能差距，表明 CGEC 研究还有很长的路要走。</p>
</li>
<li><p><strong>四种错误类型的表现</strong>：表 7 显示了对四种错误类型的更细粒度的评估结果</p>
<p>1）Seq2Edit 模型更擅长处理冗余错误，而 Seq2Seq 模型更擅长处理替换和词序错误。 对于遗漏错误，两者表现相似。</p>
<p>2）可以在考虑底层模型架构后理解。 一方面，为了纠正冗余错误，Seq2Edit 模型只需要执行固定的删除操作，这对于 Seq2Seq 模型来说是一个隐含得多的选择（<strong>问题？？</strong>），因为它的目标是重写整个句子。 另一方面，Seq2Seq 由于其利用语言模型信息的天然能力，特别是在 BART 的增强下，适合替换或重新排序单词</p>
<p>3）同样，模型集成方法显着提高了所有错误类型的性能。 集成模型在冗余错误方面最接近人类，可能是因为它们最容易纠正。 最大的差距出现在词序错误中，这需要全局结构知识来纠正并且极具挑战性。</p>
<p>4）当最大引用数增加时，模型和人类的性能都会不断提高，尤其是人类。 由于只有少数句子有超过 3 个引用，因此当最大引用数从 3 增加到 All 时，改进非常小。 这一趋势表明，与单参考数据集相比，多参考数据集降低了低估表现的风险，因此对于模型评估更可靠。</p>
</li>
</ul>
<h3 id="8-Conclusions"><a href="#8-Conclusions" class="headerlink" title="8. Conclusions"></a>8. Conclusions</h3><p>本文介绍了 MuCGEC，这是一个新标注的 CGEC 评估数据集，由 CSL 学习者编写的 7,063 个句子组成。 与现有的 CGEC 数据集相比，由于三个重要特征，我们的数据集可以支持更可靠的评估：1）提供多个参考； 2）涵盖三个文本源； 3) 采用严格的质量控制（即标注指南和工作流程）。 在描述了数据构建过程之后，我们对我们的数据进行了详细的分析。 然后，我们采用两种主流且有竞争力的 CGEC 模型，即 Seq2Seq 和 Seq2Edit，并进行基准实验。 我们还建议采用基于字符的评估指标来取代以前使用的基于单词的评估指标。</p>
<a id="more"></a>

]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>GEC</tag>
      </tags>
  </entry>
  <entry>
    <title>论文解读：PipeDream</title>
    <url>/29937.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><a id="more"></a>

<h2 id="本文介绍了微软新提出的-PipeDream，旨在使深度学习网络训练并行化水平更高，进而提高训练效率。"><a href="#本文介绍了微软新提出的-PipeDream，旨在使深度学习网络训练并行化水平更高，进而提高训练效率。" class="headerlink" title="本文介绍了微软新提出的 PipeDream，旨在使深度学习网络训练并行化水平更高，进而提高训练效率。"></a>本文介绍了微软新提出的 PipeDream，旨在使深度学习网络训练并行化水平更高，进而提高训练效率。</h2><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20191117085931501.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pkYmM=,size_16,color_FFFFFF,t_70"></p>
<p>深度神经网络（DNNs：Deep Neural Networks）已经在大量应用中取得了巨大进展，这些应用包括图像分类、翻译、语言建模以及视频字幕等。但 DNN 训练极其耗时，需要多个加速器高效地并行化。</p>
<p>在论文 “ <a href="https://www.microsoft.com/en-us/research/publication/pipedream-generalized-pipeline-parallelism-for-dnn-training/">PipeDream: Greneralized Pipeline Parallelism for DNN Training</a> ”（该论文发表于第 27 届 ACM 操作系统原理研讨会： <a href="https://sosp19.rcs.uwaterloo.ca/">SOSP 2019</a> ）中，微软系统研究小组的研究员，与来自卡内基梅隆大学和斯坦福大学的同事和学生们一起提出了一种 DNN 并行化训练的新方法。正如论文里在大量模型上展示的那样，这套被称为 PipeDream 的系统比传统的训练方法快了最多 4.3 倍。</p>
<p>DNN 训练是在前向和后向通道计算中迭代进行的。在每轮迭代中，训练过程循环处理输入数据的一个 minibatch，并且更新模型参数。最常见的 DNN 并行化训练方法是一种被称为数据并行化的方法（见下图 1），这种方法把输入数据分散到各个 workers（加速器）中运行。</p>
<p>不幸的是，尽管在数据并行化加速方面有一些性能优化的进展，但若要放在云基础设施上训练就会产生很大的通信开销。而且，GPU 计算速度的飞快提升，更进一步地把所有模型训练的耗时瓶颈转向了通信环节。</p>
<p>不那么常见的并行化训练形式是模型的并行化（见下图 1），是把算子分散到各个 worker 上计算的，这在以往通常用于训练大型 DNN 模型。模型并行化也遇到了挑战：它不能高效地利用硬件资源，并且需要程序员决定怎样按照给定的硬件资源来分割特定的模型，这其实无形中加重了程序员的负担。</p>
<p>PipeDream 是微软研究院 <a href="https://www.microsoft.com/en-us/research/project/fiddle/">Fiddle 项目</a>开发的一个系统，它引入了流水线并行化，这是一种 DNN 并行化训练的新方法，结合了传统的 batch 内并行化（模型并行化和数据并行化）和 batch 间并行化（流水线）。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20191117090027132.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pkYmM=,size_16,color_FFFFFF,t_70"></p>
<p>图 1：传统的 batch 内并行化训练方式（如数据并行化和模型并行化）对硬件利用率太低。左图中，数据并行化中的单个 worker 在交换梯度数据时不得不进行通信等待。右图是模型并行化，worker 之间只能同时处理一个 minibatch，这大大限制了硬件利用率。</p>
<h2 id="使用流水线并行化训练来解决-batch-内的并行化限制"><a href="#使用流水线并行化训练来解决-batch-内的并行化限制" class="headerlink" title="使用流水线并行化训练来解决 batch 内的并行化限制"></a>使用流水线并行化训练来解决 batch 内的并行化限制</h2><p>PipeDream 重新审视了模型的并行化，希望以此来优化性能，这与以往的动机不同，以前模型并行化是因为训练大型模型时，训练过程受限于数据集的大小。PipeDream 使用多个输入数据的流水线作业，来克服模型并行训练时硬件效率限制的问题。典型的流水线并行化设置涉及到不同 stage 之间 layer 的分割，每个 stage 都会被复制并且并行运行数据。流水线会被注入多个 batch，以使流水线满负荷运行在稳定状态。在大部分情况下，流水线并行化训练比数据并行化训练需要通信的数据要少很多，因为它只需要在两个 stage 边界之间传输 activation 和梯度。在稳定状态下，所有的 workers 时刻都在运转，不像模型并行化训练中会有停下来等待的时候（如下图所示）。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20191117090111768.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pkYmM=,size_16,color_FFFFFF,t_70"></p>
<p>图 2：左图中，我们展示了一个流水线并行化的例子，8 块 GPU 被分配到 4 个 stage 中。通信只用于 stage 边界的 actiavtion 和梯度上。stage 1,2 和 3 通过对其各自的 stage 进行复制来保证流水线的负载平衡。右图展示了具有 4 个 worker 的一个流水线，展示了启动阶段和稳定阶段。在这个例子中，后向处理花费的时间是前向处理的两倍。</p>
<h2 id="在-PipeDream-中克服流水线并行化训练的挑战"><a href="#在-PipeDream-中克服流水线并行化训练的挑战" class="headerlink" title="在 PipeDream 中克服流水线并行化训练的挑战"></a>在 PipeDream 中克服流水线并行化训练的挑战</h2><p>为了获得流水线并行化训练的潜在收益，PipeDream 必须克服三个主要挑战：</p>
<ul>
<li>  首先，PipeDream 必须在不同的输入数据间，协调双向流水线的工作。</li>
<li>  然后，PipeDream 必须管理后向通道里的权重版本，从而在数值上能够正确计算梯度，并且在后向通道里使用的权重版本必须和前向通道里使用的相同。</li>
<li>  最后，PipeDream 需要流水线里的所有 stage 都花费大致相同的计算时间，这是为了使流水线得到最大的通量（因为最慢的 stage 会成为流水线通量的瓶颈）。</li>
</ul>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20191117090229924.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pkYmM=,size_16,color_FFFFFF,t_70"> <strong>图 3：PipeDream 工作流程概览</strong></p>
<p>图 3 展示了 PipeDream 工作流程的顶层概览。给定一个模型和硬件部署方式，PipeDream 在单个 GPU 上进行短暂的运行时性能分析后，可以自动决定怎样分割这些 DNN 算子，如何平衡不同 stage 之间的计算负载，而同时尽可能减少目标平台上的通信量。PipeDream 即使是在不同的模型（不同点体现在计算和通信方面）和不同的平台上（不同点体现在互联的网络拓扑和分层带宽上）也能够有效的均衡负载。</p>
<p>由于 DNN 并不总是在可用的 worker 间进行均等分割，所以 PipeDream 可能在某些 stage 上使用数据并行化——多个 worker 会被分配到给定的 stage 上，并行化地处理不同的 minibatch。PipeDream 使用称作 1F1B 的调度算法来使硬件保持满负荷运转，同时还能达到类似数据并行化的语义。</p>
<p>在 1F1B 的稳定状态下，每个 worker 为它所在的 stage 严格地切换前向和后向通道，保证资源的高利用率（可忽略的流水线暂停，没有流水线 flush），即使在常见情况下，后向通道花费的时间多于前向通道时也是如此。上面的图 2 已经通过例子展示了这一点。与数据并行化不同，1F1B 还使用不同版本的权重来维持统计有效性。最后，PipeDream 扩展了 1F1B，在数据并行 stage 上引入了循环调度策略，保证了后向通道计算的梯度被引流至前向通道对应的 worker 上。</p>
<p>PipeDream 是基于 PyTorch（PipeDream 的<a href="https://arxiv.org/pdf/1806.03377.pdf">早期版本</a>使用 Caffe）构建出来的。我们的评估围绕着 DNN 模型、数据集和硬件配置的不同组合进行，证实了 PipeDream 流水线并行化所带来的训练时间上的收益。相比于数据并行化训练，PipeDream 在多 GPU 机器上达到了很高的精确度，性能方面，在图像分类上有 5.3 倍提升，机器翻译上 3.1 倍提升，语言建模任务有 4.3 倍提升，视频字幕模型则有 3 倍提升。PipeDream 比模型并行化有 2.6 至 15 倍的性能提升，相比混合并行化有 1.9 倍提升。</p>
<p>如果你对 PipeDream 更多细节感兴趣，可以在 GitHub 上找到<a href="https://github.com/msr-fiddle/pipedream">源码</a>。</p>
<p><strong>原文链接：</strong></p>
<p>[<a href="https://www.microsoft.com/en-us/research/blog/pipedream-a-more-effective-way-to-train-deep-neural-networks-using-pipeline-parallelism/]">https://www.microsoft.com/en-us/research/blog/pipedream-a-more-effective-way-to-train-deep-neural-networks-using-pipeline-parallelism/]</a>(</p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>毕设</tag>
      </tags>
  </entry>
</search>
