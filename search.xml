<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/16107.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><a id="more"></a>

<h2 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World!"></a>Hello World!</h2><h3 id="一个收藏回忆与分享技术的地方！"><a href="#一个收藏回忆与分享技术的地方！" class="headerlink" title="一个收藏回忆与分享技术的地方！"></a>一个收藏回忆与分享技术的地方！</h3>]]></content>
      <categories>
        <category>默认</category>
      </categories>
      <tags>
        <tag>默认</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo 写一篇博客</title>
    <url>/37981.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>欢迎来到 <a href="https://hexo.io/">Hexo</a>！这是您的第一篇文章。查看<a href="(https://hexo.io/docs/)">文档</a>以获取更多信息。如果在使用Hexo时遇到任何问题，可以在<a href="%5D(https://hexo.io/docs/troubleshooting.html)">故障排除</a>中找到答案，或者可以在<a href="https://github.com/hexojs/hexo/issues">GitHub</a>上问我。 </p>
<h2 id="快速开始"><a href="#快速开始" class="headerlink" title="快速开始"></a>快速开始</h2><h3 id="创建一篇新文章"><a href="#创建一篇新文章" class="headerlink" title="创建一篇新文章"></a>创建一篇新文章</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>更多信息：<a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="预览"><a href="#预览" class="headerlink" title="预览"></a>预览</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo server</span><br></pre></td></tr></table></figure>
<p>更多信息： <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="生成静态文件"><a href="#生成静态文件" class="headerlink" title="生成静态文件"></a>生成静态文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo generate</span><br></pre></td></tr></table></figure>
<p>更多信息： <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="部署到远程站点"><a href="#部署到远程站点" class="headerlink" title="部署到远程站点"></a>部署到远程站点</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo deploy</span><br></pre></td></tr></table></figure>
<p>更多信息： <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Blog</tag>
      </tags>
  </entry>
  <entry>
    <title>anaconda3的使用</title>
    <url>/32873.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><a id="more"></a>

<h2 id="1、安装-anaconda3"><a href="#1、安装-anaconda3" class="headerlink" title="1、安装 anaconda3"></a><strong>1、安装 anaconda3</strong></h2><h2 id="安装条件"><a href="#安装条件" class="headerlink" title="    安装条件"></a>    安装条件</h2><ul>
<li>  系统要求：32 位或 64 位系统均可</li>
<li>  下载文件大小：约 500MB</li>
<li>  所需空间大小：3GB 空间大小（Miniconda 仅需 400MB 空间即可）</li>
</ul>
<p>（1）地址 <a href="https://www.anaconda.com/products/individual#linux">anaconda 下载</a>。最新版本为 Anaconda3-2020.02-Linux-x86_64.sh，启动终端，输入如下指令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bash ~&#x2F;Downloads&#x2F;Anaconda3-2020.02-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>
<p>（2）安装过程中，看到提示 “In order to continue the installation process, please review the license agreement.”（“请浏览许可证协议以便继续安装。”），点击“Enter” 查看 “许可证协议”，也可以点击空格翻页，这样快一些。在“许可证协议” 界面将屏幕滚动至底，输入 “yes” 表示同意许可证协议内容。然后进行下一步。</p>
<p>（3）安装过程中，提示 “Press Enter to accept the default install location, CTRL-C to cancel the installation or specify an alternate installation directory.”（“按回车键确认安装路径，按’CTRL-C’取消安装或者指定安装目录。”）可以指定目录，输入要安装的目录即可。</p>
<p>（4）安装器若提示 You currently have a PYTHONPATH environment variable set….. 等类似提示时：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200628222025241.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pJRUpJTlFVQU5JTA==,size_16,color_FFFFFF,t_70"></p>
<p>这里是将 anaconda3 添加到环境变量中，这里十分建议选 no，否则安装完成后在终端前面始终会有个 (base) 存在，无论用不用 anaconda3。总之选 no 就对了！</p>
<p>        看到 Thank you for installing Anaconda3! 这个提示证明安装成功了！</p>
<h2 id="2、anaconda3-的使用"><a href="#2、anaconda3-的使用" class="headerlink" title="2、anaconda3 的使用"></a><strong>2、anaconda3 的使用</strong></h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd &#x2F;home&#x2F;zydz&#x2F;anaconda3&#x2F;bin</span><br><span class="line">$ . .&#x2F;activate</span><br></pre></td></tr></table></figure>
<p>         这两条指令是进入 conda 环境，其中第 2 条指令第一个点相当于 source，第二个点代表当前路径。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda list</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>         显示已经安装的包名和版本号。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">anaconda-navigator</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>         anaconda 交互界面。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda --version</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>          验证 conda 已被安装，查看 conda 版本号。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda info --envs</span><br><span class="line">$ conda info -e</span><br><span class="line">$ conda env list</span><br></pre></td></tr></table></figure>
<p>         这 3 条指令含义相同，显示已创建环境。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda --help</span><br><span class="line">$ conda -h</span><br></pre></td></tr></table></figure>
<p>         查看 conda 帮助信息。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda create --name &lt;env_name&gt; &lt;package_names&gt;</span><br><span class="line">$ conda create -n &lt;env_name&gt; &lt;package_names&gt;</span><br></pre></td></tr></table></figure>
<p>         创建新环境，以下举例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda create -n conda_test python&#x3D;3.5 numpy</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>         表示创建一个名为 conda_test 的环境，环境中安装版本为 3.5 的 python，同时也安装了 numpy。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda create --name &lt;new_env_name&gt; --clone &lt;copied_env_name&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>         复制环境，以下举例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda create --name new_conda --clone conda_test</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>         表示克隆名为 conda_test 的环境，克隆后的新环境名为 new_conda。此时，环境中将同时存在 conda_test 和 new_conda 环境，且两个环境的配置相同。 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda remove --name &lt;env_name&gt; --all</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>         删除环境，<env_name> 为被删除环境的名称。</env_name></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda search --full python</span><br><span class="line">$ conda search --full-name python</span><br></pre></td></tr></table></figure>
<p>         表示查找全名为 python 的包有哪些版本可供安装。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda install --name &lt;env_name&gt; &lt;package_name&gt;</span><br><span class="line">$ conda install --name conda_test pandas</span><br></pre></td></tr></table></figure>
<p>          表示在名为 conda_test 的环境中安装 pandas 包。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ pip install &lt;package_name&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>         当使用 conda install 无法进行安装时，可以使用 pip 进行安装。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda remove --name &lt;env_name&gt; &lt;package_name&gt;</span><br><span class="line">$ conda remove --name conda_test pandas</span><br></pre></td></tr></table></figure>
<p>         表示卸载名为 conda_test 环境中的 pandas 包。 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda remove &lt;package_name&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>         表示卸载当前环境中的包。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda activate &lt;env_name&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>          切换环境。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ conda deactivate</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>         退出环境至 root。</p>
<p>         如果在其他电脑上使用，你可以将已经配置好的 xxx 环境，复制到其他电脑上，仍然能够使用。</p>
<h2 id="3、卸载-anaconda3"><a href="#3、卸载-anaconda3" class="headerlink" title="3、卸载 anaconda3"></a><strong>3、卸载 anaconda3</strong></h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ rm -rf ~&#x2F;anaconda3</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>        即删除 Anaconda 的安装目录。根据安装的 Anaconda 版本选择相应的卸载命令。</p>
<h2 id="参考网址"><a href="#参考网址" class="headerlink" title="参考网址"></a><strong>参考网址</strong></h2><p><a href="https://www.jianshu.com/p/62f155eb6ac5">Anaconda 介绍、安装及使用教程</a></p>
]]></content>
      <categories>
        <category>Anaconda</category>
      </categories>
      <tags>
        <tag>毕设</tag>
      </tags>
  </entry>
  <entry>
    <title>Numpy基础用法</title>
    <url>/64862.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><a id="more"></a>

<p> 一个由国外大神制作的 NumPy 可视化教程，直观地介绍 NumPy 的各种用法，很容易就能理解。</p>
<p>话不多说，一睹为快。</p>
<h2 id="数组"><a href="#数组" class="headerlink" title="数组"></a><strong>数组</strong></h2><p>先来介绍最基础的一维数组。</p>
<h3 id="创建数组"><a href="#创建数组" class="headerlink" title="创建数组"></a><strong>创建数组</strong></h3><p>np.array() 直接创建：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/80/v2-90de7812f9fc3169cffb7e39d4c3cfd8_720w.jpg" alt="img"></p>
<p>使用 np.ones()、np.zeros() 等方法：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/80/v2-6e73db4bcf9e406d110da4f2827200ab_720w.jpg" alt="img"></p>
<p>我们在写数组的时候是横着写的，而其实数组是列向量，这样很直观。</p>
<h3 id="数组运算"><a href="#数组运算" class="headerlink" title="数组运算"></a><strong>数组运算</strong></h3><p>加减乘除</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/80/v2-ae2106c929707e5613c6d8b8bb7aab16_720w.png" alt="img"></p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/80/v2-651679ca10deeda9810c5a59de52bac6_720w.jpg" alt="img"></p>
<p>（注意这里的data和ones不是上面的data，ones）</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/80/v2-c2b47f98a0c1ae65eedd798bb222758d_720w.png" alt="img"></p>
<p>数组乘以数值</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/80/v2-e461f0c4e4a66622b487b71f22e3eb7b_720w.png" alt="img"></p>
<h3 id="数组索引"><a href="#数组索引" class="headerlink" title="数组索引"></a><strong>数组索引</strong></h3><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/80/v2-6ab0797b53e61645a9564376ae9f3d0c_720w.jpg" alt="img"></p>
<h3 id="数组聚合"><a href="#数组聚合" class="headerlink" title="数组聚合"></a><strong>数组聚合</strong></h3><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/80/v2-6e8ef73d761be46fdf60aefcf3953821_720w.jpg" alt="img"></p>
<p>上面是一维数组，下面介绍二维维数组也就是矩阵的使用技巧。</p>
<h2 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a><strong>矩阵</strong></h2><h3 id="创建矩阵"><a href="#创建矩阵" class="headerlink" title="创建矩阵"></a><strong>创建矩阵</strong></h3><p>直接创建：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/80/v2-58fe9b45561eed23069a721ec516279b_720w.jpg" alt="img"></p>
<p>使用 np.ones()、np.zeros() 等方法：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/80/v2-63b85180f9ac32626669e0cd662f5d92_720w.jpg" alt="img"></p>
<p>这样就很容易理解括号里 (3,2) 的含义。</p>
<h3 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a><strong>矩阵运算</strong></h3><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/80/v2-651679ca10deeda9810c5a59de52bac6_720w.jpg" alt="img"></p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/80/v2-b1a31f23c0cab9570a3f7b0d2af42031_720w.jpg" alt="img"></p>
<h3 id="矩阵点积"><a href="#矩阵点积" class="headerlink" title="矩阵点积"></a><strong>矩阵点积</strong></h3><p>矩阵点积跟线性代数基本一样，有些抽象，借助示意图能很好理解：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/80/v2-c3f791e0c27505216111dfe9d3f51c74_720w.jpg" alt="img"></p>
<p>进一步拆分解释：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/80/v2-ffeacb71407ec517f64294c30a962159_720w.jpg" alt="img"></p>
<h3 id="矩阵索引"><a href="#矩阵索引" class="headerlink" title="矩阵索引"></a><strong>矩阵索引</strong></h3><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/80/v2-45f4ce6a4ffd5f8af7ab6c7ba717720e_720w.jpg" alt="img"></p>
<h3 id="矩阵聚合"><a href="#矩阵聚合" class="headerlink" title="矩阵聚合"></a><strong>矩阵聚合</strong></h3><p>求最值</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/80/v2-fa64dda84ef6ac60c277446e5dea8029_720w.jpg" alt="img"></p>
<p>按行 / 列聚合</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/80/v2-e5dbfb6c39c9065351628b8e9f22ba35_720w.jpg" alt="img"></p>
<h3 id="矩阵转置"><a href="#矩阵转置" class="headerlink" title="矩阵转置"></a><strong>矩阵转置</strong></h3><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/80/v2-689c92bca6be7d624a473b41fa0fec9b_720w.jpg" alt="img"></p>
<h3 id="矩阵重塑"><a href="#矩阵重塑" class="headerlink" title="矩阵重塑"></a><strong>矩阵重塑</strong></h3><p>reshape() 用法：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/80/v2-676cd66b7083e7507ebec3b69c0b8d3f_720w.jpg" alt="img"></p>
<p>mark</p>
<h2 id="高维数组"><a href="#高维数组" class="headerlink" title="高维数组"></a><strong>高维数组</strong></h2><p>Numpy 不仅可以处理上述的一维数组和二维矩阵，还可以处理任意 N 维的数组，方法也大同小异。</p>
<h3 id="创建多维数组"><a href="#创建多维数组" class="headerlink" title="创建多维数组"></a><strong>创建多维数组</strong></h3><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/80/v2-00e2ef23fefed05864e66318b6c5d107_720w.jpg" alt="img"></p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/80/v2-e5bb94c075e23e20e58cea9e2df47cf1_720w.jpg" alt="img"></p>
<p>掌握了以上基础后，我们可以做个小练习，计算均方误差 MSE：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/80/v2-70d84cd6b6975b2757e5e164249b0cdb_720w.jpg" alt="img"></p>
<p>可以看到有减法、平方、求和等运算：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/80/v2-e52fd0fc193ad26494ee9ce35559753c_720w.jpg" alt="img"></p>
<p>分别假设相应的预测值和真实值：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/80/v2-0462ab48e91ca9081cc652fce298de6c_720w.jpg" alt="img"></p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/80/v2-fb39528fcfd3447639f203df41949b00_720w.jpg" alt="img"></p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/80/v2-a2ed39331e8841397d4908fc2157a46c_720w.jpg" alt="img"></p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/80/v2-56048efa2b5f2a10832388b69c050ebc_720w.jpg" alt="img"></p>
]]></content>
      <categories>
        <category>Numpy</category>
      </categories>
      <tags>
        <tag>毕设</tag>
      </tags>
  </entry>
  <entry>
    <title>卷积核（kernels）与滤波器（filters）的关系</title>
    <url>/45549.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><a id="more"></a>

<h2 id="简单理解："><a href="#简单理解：" class="headerlink" title="简单理解："></a><strong>简单理解：</strong></h2><p>卷积核：二维的矩阵<br>滤波器：多个卷积核组成的三维矩阵，多出的一维是通道。</p>
<p>先介绍一些术语：layers（层）、channels（通道）、feature maps（特征图），filters（滤波器），kernels（卷积核）。<br>从层次结构的角度来看，层和滤波器的概念处于同一水平，而通道和卷积核在下一级结构中。通道和特征图是同一个事情。一层可以有多个通道（或者说特征图）。如果输入的是一个 RGB 图像，那么就会有 3 个通道。“channel”通常被用来描述 “layer” 的结构。相似的，“kernel”是被用来描述 “filter” 的结构。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20190915145942491.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxMzc1NjA5,size_16,color_FFFFFF,t_70"><br>filter 和 kernel 之间的不同很微妙。很多时候，它们可以互换，所以这可能造成我们的混淆。</p>
<h2 id="那它们之间的不同在于哪里呢？"><a href="#那它们之间的不同在于哪里呢？" class="headerlink" title="那它们之间的不同在于哪里呢？"></a><strong>那它们之间的不同在于哪里呢？</strong></h2><p>一个 “Kernel” 更倾向于是 2D 的权重矩阵。而 “filter” 则是指多个 Kernel 堆叠的 3D 结构。如果是一个 2D 的 filter，那么两者就是一样的。但是一个 3Dfilter，在大多数深度学习的卷积中，它是包含 kernel 的。每个卷积核都是独一无二的，主要在于强调输入通道的不同方面。</p>
<p>参考：[深度学习中的各种卷积](</p>
]]></content>
      <categories>
        <category>默认</category>
      </categories>
      <tags>
        <tag>默认</tag>
      </tags>
  </entry>
  <entry>
    <title>安装Go环境</title>
    <url>/59924.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><a id="more"></a>

<h1 id="安装-Go-语言及搭建-Go-语言开发环境"><a href="#安装-Go-语言及搭建-Go-语言开发环境" class="headerlink" title="安装 Go 语言及搭建 Go 语言开发环境"></a>安装 Go 语言及搭建 Go 语言开发环境</h1><hr>
<h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><h3 id="下载地址"><a href="#下载地址" class="headerlink" title="下载地址"></a>下载地址</h3><ul>
<li><p>Go 官网下载地址：<a href="https://link.zhihu.com/?target=https://golang.org/dl/">https://golang.org/dl/</a></p>
</li>
<li><p>Go 官方镜像站（推荐）：<a href="https://link.zhihu.com/?target=https://golang.google.cn/dl/">https://golang.google.cn/dl/</a></p>
</li>
</ul>
<h3 id="版本的选择"><a href="#版本的选择" class="headerlink" title="版本的选择"></a>版本的选择</h3><p>Windows 平台和 Mac 平台推荐下载可执行文件版，Linux 平台下载压缩文件版。</p>
<p><em>(目前的最新版本可能不是 1.11.5，但是安装过程类似哦。)</em></p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/v2-5b7945253d9729f6d6ceee5f5322072e_r.jpg"></p>
<h1 id="Windows-下安装Go环境"><a href="#Windows-下安装Go环境" class="headerlink" title="Windows 下安装Go环境"></a>Windows 下安装Go环境</h1><hr>
<p>此安装实例以 <code>64位Win10</code>系统安装 <code>Go1.11.5可执行文件版本</code>为例。</p>
<p>将上一步选好的安装包下载到本地。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/v2-86fc9e0126b5d7dd1b820abdda433745_r.jpg"></p>
<p>双击下载好的文件，然后再按照下图步骤安装即可。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/v2-7481d0f0ca16726c5ae9b72af4db7650_r.jpg"><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/v2-dbb182b02a84582bdc13a42f29257ecf_r.jpg"><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/v2-58d7a6598586f787446df4d1b6390216_r.jpg"></p>
<h1 id="Linux下安装Go环境"><a href="#Linux下安装Go环境" class="headerlink" title="Linux下安装Go环境"></a>Linux下安装Go环境</h1><hr>
<h2 id="登录-Linux"><a href="#登录-Linux" class="headerlink" title="登录 Linux"></a>登录 Linux</h2><p>Mac 或 Linux 的用户可以用命令<code>ssh root@xxx.xxx.xxx.xxx</code>登录主机<br>Window 的用户可以使用 SecureCRT 登录主机<br>虚拟机用户直接打开你的虚拟机</p>
<h2 id="安装-Go-环境"><a href="#安装-Go-环境" class="headerlink" title="安装 Go 环境"></a>安装 Go 环境</h2><blockquote>
<p>Golang 官网下载地址：<a href="https://golang.org/dl/">https://golang.org/dl/</a></p>
</blockquote>
<ol>
<li>打开官网下载地址选择对应的系统版本, 复制下载链接<br> 这里我选择的是<br> <code>go1.11.5.linux-amd64.tar.gz</code>：<a href="https://dl.google.com/go/go1.11.5.linux-amd64.tar.gz">https://dl.google.com/go/go1.11.5.linux-amd64.tar.gz</a></li>
</ol>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="http://upload-images.jianshu.io/upload_images/1277729-7d5e14c2c8c994bf.png"> image.png</p>
<ol start="2">
<li> <code>cd</code>进入你用来存放安装包的目录，我习惯在<code>~</code>下面创建个<code>go</code>文件夹。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在 ~ 下创建 go 文件夹，并进入 go 文件夹</span><br><span class="line">mkdir ~&#x2F;go &amp;&amp; cd ~&#x2F;go</span><br><span class="line">下载的 go 压缩包</span><br><span class="line">wget https:&#x2F;&#x2F;dl.google.com&#x2F;go&#x2F;go1.11.5.linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="3">
<li> 下载完成</li>
</ol>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="http://upload-images.jianshu.io/upload_images/1277729-60625ab3615c752e.png"></p>
<ol start="4">
<li> 执行<code>tar</code>解压到<code>/usr/loacl</code>目录下（官方推荐），得到<code>go</code>文件夹等</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar -C &#x2F;usr&#x2F;local -zxvf  go1.11.5.linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="5">
<li> 添加<code>/usr/loacl/go/bin</code>目录到 PATH 变量中。添加到<code>/etc/profile</code> 或<code>$HOME/.profile</code>都可以</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 习惯用vim，没有的话可以用命令&#96;sudo apt-get install vim&#96;安装一个</span><br><span class="line">vim &#x2F;etc&#x2F;profile</span><br><span class="line"># 在最后一行添加</span><br><span class="line">export GOROOT&#x3D;&#x2F;usr&#x2F;local&#x2F;go</span><br><span class="line">export PATH&#x3D;$PATH:$GOROOT&#x2F;bin</span><br><span class="line"># 保存退出后source一下（vim 的使用方法可以自己搜索一下）</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="6">
<li>执行<code>go version</code>，如果现实版本号，则 Go 环境安装成功。是不是很简单呢？<br> <img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="http://upload-images.jianshu.io/upload_images/1277729-bd36e8e538989c62.png"></li>
</ol>
<h2 id="运行第一个程序"><a href="#运行第一个程序" class="headerlink" title="运行第一个程序"></a>运行第一个程序</h2><ol>
<li> 先创建你的工作空间 (<a href="%5Bworkspace%5D(https://golang.org/doc/code.html#Workspaces)">Workspaces</a>)，官方建议目录<code>$HOME/go</code>。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir $HOME&#x2F;go</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="2">
<li> 将你的工作空间路径声明到环境变量中。和上一部分的第 5 步相似。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 编辑 ~&#x2F;.bash_profile 文件</span><br><span class="line">vim ~&#x2F;.bash_profile</span><br><span class="line"># 在最后一行添加下面这句。$HOME&#x2F;go 为你工作空间的路径，你也可以换成你喜欢的路径</span><br><span class="line">export GOPATH&#x3D;$HOME&#x2F;go</span><br><span class="line"># 保存退出后source一下（vim 的使用方法可以自己搜索一下）</span><br><span class="line">source ~&#x2F;.bash_profile</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="3">
<li> 在你的工作空间创建你的第一个工程目录</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 创建并进入你的第一个工程目录</span><br><span class="line">mkdir -p $GOPATH&#x2F;src&#x2F;hello &amp;&amp; cd $GOPATH&#x2F;src&#x2F;hello</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="4">
<li> 在你的工程目录下创建名为<code>hello.go</code>的文件</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim hello.go</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="5">
<li> 将下面内容粘贴到 hello.go 文件</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import &quot;fmt&quot;</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">    fmt.Printf(&quot;hello, world\n&quot;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="6">
<li> 好了，工程目录和工程文件都准备好了。现在我们到我们的工程目录 (<code>$GOPATH/src/hello</code>) 下构建我们的工程</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 如果你当前的目录不在 $GOPATH&#x2F;src&#x2F;hello， 需要先执行 &quot;cd $GOPATH&#x2F;src&#x2F;hello&quot; 进入该目录</span><br><span class="line"># 执行构建工程的命令</span><br><span class="line">go build</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol start="7">
<li> 等一会，命令执行完之后你可以看到目录下会多出一个 hello 的文件，这就是我们编译之后的文件啦。怎么执行我们的程序呢？只需要在当前目录下执行<code>./xxx</code>就可以啦！是不是敲鸡煎蛋呢！</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;hello</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="http://upload-images.jianshu.io/upload_images/1277729-94425080feff1bbd.png"></p>
<h1 id="Mac-下安装Go环境"><a href="#Mac-下安装Go环境" class="headerlink" title="Mac 下安装Go环境"></a>Mac 下安装Go环境</h1><hr>
<p>下载可执行文件版，直接点击<strong>下一步</strong>安装即可，默认会将 go 安装到<code>/usr/local/go</code>目录下。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/v2-45081f19819b3c3a69e4c78fed850d47_r.jpg"></p>
<h2 id="检查"><a href="#检查" class="headerlink" title="检查"></a>检查</h2><p>上一步安装过程执行完毕后，可以打开终端窗口，输入<code>go version</code>命令，查看安装的 Go 版本。</p>
<h1 id="关于-Go-的一些介绍"><a href="#关于-Go-的一些介绍" class="headerlink" title="关于 Go 的一些介绍"></a>关于 Go 的一些介绍</h1><hr>
<h2 id="环境变量："><a href="#环境变量：" class="headerlink" title="环境变量："></a>环境变量：</h2><ul>
<li>$GOROOT:<br>  表示 Go 的安装目录。也就是上面我们解压出来的文件夹里面的<code>go</code>文件夹。</li>
<li>$GOPATH:<br>  表示我们的工作空间。用来存放我们的工程目录的地方。</li>
</ul>
<h2 id="GOPATH-目录："><a href="#GOPATH-目录：" class="headerlink" title="GOPATH 目录："></a>GOPATH 目录：</h2><p>一般来说 GOPATH 下面会有三个文件夹：<code>bin</code>、<code>pkg</code>、<code>src</code>，没有的话自己创建。每个文件夹都有其的作用。</p>
<ul>
<li>  bin：编译后可的执行文件的存放路径</li>
<li>  pkg：编译包时，生成的. a 文件的存放路径</li>
<li>  src：源码路径，一般我们的工程就创建在<code>src</code>下面。</li>
</ul>
<p>注意：如果要用<code>Go Mod</code>(Go1.11 及以上支持) 进行包管理，则需要在 GOPATH 以外的目录创建工程。关于<code>Go Mod</code>的使用，可以自行 Google 一下，这里就不赘述了。</p>
<h2 id="配置-GOPATH"><a href="#配置-GOPATH" class="headerlink" title="配置 GOPATH"></a>配置 GOPATH</h2><p><code>GOPATH</code>是一个环境变量，用来表明你写的 go 项目的存放路径（工作目录），本文就选择使用 <code>D:\code\go</code> 这个目录作为 <code>GOPATH</code> 目录。</p>
<p><code>GOPATH</code>路径最好只设置一个，我们写的所有 Go 项目代码都放到<code>GOPATH</code>的<code>src</code>目录下。</p>
<p><em>补充说明：Go1.11 版本之后，开启<code>go mod</code>模式之后就不再强制需要配置<code>GOPATH</code>了。</em></p>
<p>Linux 和 Mac 平台就参照上面配置环境变量的方式将自己的工作目录添加到环境变量中即可。</p>
<p>Windows 平台按下面的步骤将<code>D:\code\go</code>添加到环境变量：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/v2-5e4fa7826f177c0d3e8aaaf645559fd6_r.jpg"><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/v2-07f79095623e958b07a518dcb35cef24_r.jpg"><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/v2-c4f08c411d25b622f3577dd5cf72196f_r.jpg"><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/v2-4a875a0e2dca47c4c813aae850a5ed2a_r.jpg"><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/v2-d563ad97c44aaf2adfa4b3231b0ee834_r.jpg"><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/v2-f04b1d2b9f17c73675c07b98b2b4f21d_r.jpg"><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/v2-00eef37d36fe47cd68c8cd0e320a7405_r.jpg"></p>
<p>在 Go 1.8 版本之前，<code>GOPATH</code>环境变量默认是空的。从 Go 1.8 版本开始，Go 开发包在安装完成后会为 <code>GOPATH</code>设置一个默认目录，参见下图。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/v2-b8e098f6ce03c4d2947f6b3dd33c03eb_r.jpg"></p>
<p>同时，我们将 <code>GOROOT</code>下的 bin 目录及<code>GOPATH</code>下的 bin 目录都添加到环境变量中。</p>
<p>配置环境变量之后需要重启已经打开的终端才能让新配置的环境变量生效。（例如 cmd、VS Code 里面的终端和其他编辑器的终端等）。</p>
<h2 id="Go-项目结构"><a href="#Go-项目结构" class="headerlink" title="Go 项目结构"></a>Go 项目结构</h2><p>在进行 Go 语言开发的时候，我们的代码总是会保存在<code>$GOPATH/src</code>目录下。在工程经过<code>go build</code>、<code>go install</code>或<code>go get</code>等指令后，会将下载的第三方包源代码文件放在<code>$GOPATH/src</code>目录下， 产生的二进制可执行文件放在 <code>$GOPATH/bin</code>目录下，生成的中间缓存文件会被保存在 <code>$GOPATH/pkg</code> 下。</p>
<p>如果我们使用版本管理工具（Version Control System，VCS。常用如 Git）来管理我们的项目代码时，我们只需要添加<code>$GOPATH/src</code>目录的源代码即可。<code>bin</code> 和 <code>pkg</code> 目录的内容无需版本控制。</p>
<h3 id="适合个人开发者"><a href="#适合个人开发者" class="headerlink" title="适合个人开发者"></a>适合个人开发者</h3><p>我们知道源代码都是存放在<code>GOPATH</code>的<code>src</code>目录下，那我们可以按照下图来组织我们的代码。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/v2-8d441d70a1caebb5f69197686a3b1e5f_r.jpg"></p>
<h3 id="目前流行的项目结构"><a href="#目前流行的项目结构" class="headerlink" title="目前流行的项目结构"></a>目前流行的项目结构</h3><p>Go 语言中也是通过包来组织代码文件，我们可以引用别人的包也可以发布自己的包，但是为了防止不同包的项目名冲突，我们通常使用<code>顶级域名</code>来作为包名的前缀，这样就不担心项目名冲突的问题了。</p>
<p>因为不是每个个人开发者都拥有自己的顶级域名，所以目前流行的方式是使用个人的 github 用户名来区分不同的包。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic3.zhimg.com/v2-01248b3f1900f995a437395cdec5e756_r.jpg"></p>
<p>举个例子：张三和李四都有一个名叫<code>studygo</code>的项目，那么这两个包的路径就会是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import &quot;github.com&#x2F;zhangsan&#x2F;studygo&quot;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>和</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import &quot;github.com&#x2F;lisi&#x2F;studygo&quot;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>以后我们从 github 上下载别人包的时候，如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">go get github.com&#x2F;jmoiron&#x2F;sqlx</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>那么，这个包会下载到我们本地<code>GOPATH</code>目录下的<code>src/github.com/jmoiron/sqlx</code>。</p>
<h3 id="适合企业开发者"><a href="#适合企业开发者" class="headerlink" title="适合企业开发者"></a>适合企业开发者</h3><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/v2-da6bd50401d8c030f1b79e9b3131ed04_r.jpg"></p>
<h1 id="Goland-官网下载"><a href="#Goland-官网下载" class="headerlink" title="Goland 官网下载"></a>Goland 官网下载</h1><hr>
<p>Goland 官网地址：<a href="https://www.jetbrains.com/go/specials/go/go.html?gclid=EAIaIQobChMI9Y2H-vDV4AIV2CCtBh0nrAPYEAAYASAAEgJ-cfD_BwE&gclsrc=aw.ds">https://www.jetbrains.com/go/specials/go/go.html?gclid=EAIaIQobChMI9Y2H-vDV4AIV2CCtBh0nrAPYEAAYASAAEgJ-cfD_BwE&amp;gclsrc=aw.ds</a></p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/2019022522171329.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuZ2VsaWE2MjA=,size_16,color_FFFFFF,t_70"></p>
<h2 id="配置-GoLand-的-GOPATH-和-GOROOT"><a href="#配置-GoLand-的-GOPATH-和-GOROOT" class="headerlink" title="配置 GoLand 的 GOPATH 和 GOROOT"></a>配置 GoLand 的 GOPATH 和 GOROOT</h2><p>打开 Goland 软件，点击右下角 Configure，然后点击 Setting，如下图所示：</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20190225222635750.png"></p>
<p>在打开的界面中，点击 GO 列表，然后找到 GOPATH 和 GOROOT</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20190225222831313.png"></p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20190225222847786.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuZ2VsaWE2MjA=,size_16,color_FFFFFF,t_70"></p>
<h2 id="测试软件是否安装成功"><a href="#测试软件是否安装成功" class="headerlink" title="测试软件是否安装成功"></a>测试软件是否安装成功</h2><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20190225223126694.png"></p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20190225223141314.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FuZ2VsaWE2MjA=,size_16,color_FFFFFF,t_70"></p>
]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习(1)——特征工程</title>
    <url>/32820.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h3 id="机器学习的流程"><a href="#机器学习的流程" class="headerlink" title="机器学习的流程"></a>机器学习的流程</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/32820/1.png" style="zoom:25%;">

<center>预处理  ——> 特征工程  ——> 机器学习  ——> 模型评估</center>

<p>如果未达到要求，重新循环</p>
<h3 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h3><p>将<strong>原始数据</strong>转换为更好地<strong>代表预测模型的潜在问题的特征</strong>的工程，从而提高了模型对位置数据预测的准确性（ <strong>属于数据预处理阶段的工作</strong>）</p>
<p>以scikit-learn为例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer, TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler, StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dictvec</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    字典数据抽取</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 实例化</span></span><br><span class="line">    <span class="comment"># sparse改为True,输出的是每个不为零位置的坐标，稀疏矩阵可以节省存储空间</span></span><br><span class="line">    <span class="built_in">dict</span> = DictVectorizer(sparse=<span class="literal">False</span>)  <span class="comment"># 把sparse改为True看看</span></span><br><span class="line">    <span class="comment">#矩阵中存在大量的0，sparse存储只记录非零位置，节省空间</span></span><br><span class="line">    <span class="comment"># 调用fit_transform</span></span><br><span class="line">    data = <span class="built_in">dict</span>.fit_transform([&#123;<span class="string">&#x27;city&#x27;</span>: <span class="string">&#x27;北京&#x27;</span>, <span class="string">&#x27;temperature&#x27;</span>: <span class="number">100</span>&#125;,</span><br><span class="line">                               &#123;<span class="string">&#x27;city&#x27;</span>: <span class="string">&#x27;上海&#x27;</span>, <span class="string">&#x27;temperature&#x27;</span>: <span class="number">60</span>&#125;,</span><br><span class="line">                               &#123;<span class="string">&#x27;city&#x27;</span>: <span class="string">&#x27;深圳&#x27;</span>, <span class="string">&#x27;temperature&#x27;</span>: <span class="number">30</span>&#125;])</span><br><span class="line">    print(data)</span><br><span class="line">    print(<span class="string">&#x27;-&#x27;</span> * <span class="number">50</span>)</span><br><span class="line">    print(<span class="built_in">dict</span>.get_feature_names_out())  <span class="comment"># 字典中的一些类别数据，分别进行转换成特征</span></span><br><span class="line">    print(<span class="string">&#x27;-&#x27;</span> * <span class="number">50</span>)</span><br><span class="line">    print(<span class="built_in">dict</span>.inverse_transform(data))  <span class="comment">#去看每个特征代表的含义，逆转回去</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>DictVectorizer.fit_transform：（在训练集上）将字典数据<strong>转换</strong>为<strong>特征值</strong>数组（默认是one-hot编码）</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/32820/2.png" style="zoom:50%;">

<p>DictVectorizer.inverse_transform：上述过程逆转换</p>
<p>sklearn.feature_extraction.text.CountVectorizer：对文本进行特征值化,单个汉字单个字母不统计，因为单个汉字字母没有意义</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countvec</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对文本进行特征值化,单个汉字单个字母不统计，因为单个汉字字母没有意义,(“我”没有统计)</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    cv = CountVectorizer()</span><br><span class="line">    data = cv.fit_transform([<span class="string">&quot;人生苦短，我 喜欢 python python&quot;</span>, <span class="string">&quot;人生漫长，不用 python&quot;</span>])</span><br><span class="line"></span><br><span class="line">    print(cv.get_feature_names())</span><br><span class="line">    print(data)</span><br><span class="line">    print(data.toarray())</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">countvec()</span><br></pre></td></tr></table></figure>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/32820/3.png" style="zoom:50%;">

<h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><p>如果某个词或短语在一篇文章中出现的概率高,并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。</p>
<p>​        <strong>sklearn.feature extraction.text.TfidfVectorizer</strong></p>
<p>Tf：term frequency:词的频率出现的次数<br>idf：逆文档频率inverse document frequency：log(总文档数量/该词出现的文档数量)<br>tf*idf 来代表重要性程度</p>
<h4 id="（1）TF是词频-Term-Frequency"><a href="#（1）TF是词频-Term-Frequency" class="headerlink" title="（1）TF是词频(Term Frequency)"></a><strong>（1）TF是词频(Term Frequency)</strong></h4><p>        <strong>词频（TF）**</strong>表示词条（关键字）在文本中出现的频率**。</p>
<p>        这个数字通常会被归一化(一般是词频除以文章总词数), 以防止它偏向长的文件。</p>
<p>        <strong>公式：<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/32820/4.png" style="zoom:50%;"></strong>           **即：<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/32820/5.png" style="zoom:50%;"></p>
<p>其中 <strong>ni,j</strong> 是该词在文件 <strong>dj</strong> 中出现的次数，分母则是文件 dj 中所有词汇出现的次数总和；</p>
<h4 id="（2）-IDF是逆向文件频率-Inverse-Document-Frequency"><a href="#（2）-IDF是逆向文件频率-Inverse-Document-Frequency" class="headerlink" title="（2） IDF是逆向文件频率(Inverse Document Frequency)"></a><strong>（2） IDF是逆向文件频率(Inverse Document Frequency)</strong></h4><p>        <strong>逆向文件频率 (IDF)</strong> ：某一特定词语的IDF，可以由<strong>总文件数目除以包含该词语的文件的数目</strong>，<strong>再将得到的商取对数得到</strong>。</p>
<p>如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。</p>
<p>      <strong>公式：<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/32820/6.png" style="zoom:50%;"></strong> </p>
<p>其中，**|D|** <strong>是语料库中的文件总数</strong>。 <strong>|{j:ti∈dj}| 表示包含词语 ti 的文件数目</strong>（即 ni,j≠0 的文件数目）。如果该词语不在语料库中，就会导致分母为零，因此<strong>一般情况下使用 1+|{j:ti∈dj}|</strong></p>
<p> <strong>即：<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/32820/7.png" style="zoom:50%;"></strong></p>
<h4 id="（3）TF-IDF实际上是：TF-IDF"><a href="#（3）TF-IDF实际上是：TF-IDF" class="headerlink" title="*（3）TF-IDF实际上是：TF * IDF*"></a>*<em>（3）TF-IDF实际上是：TF * IDF*</em></h4><p>       某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。</p>
<p>       <strong>公式：<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/32820/8.png" style="zoom:50%;"></strong></p>
<p><strong>注：</strong>  TF-IDF算法非常容易理解，并且很容易实现，但是其简单结构并没有考虑词语的语义信息，无法处理一词多义与一义多词的情况。</p>
<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-11-——逻辑回归</title>
    <url>/30190.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>逻辑回归（Logistic Regression）是机器学习中的一种分类模型，逻辑回归是一种分类算法，虽然名字中带有回归，但是它与回归之间有一定的联系。由于算法的简单和高效，在实际中应用非常广泛。<strong>是解决二分类问题的利器。</strong></p>
<h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/1.png" style="zoom:40%;">

<p>逻辑回归的输入就是一个线性回归的结果。</p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>sigmoid函数</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/2.png" style="zoom:50%;">

<p>逻辑回归公式：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/3.png" style="zoom:60%;">

<p>输出：[0,1]区间的概率值，默认0.5作为阀值<br>注：g(z)为sigmoid函数<br>e: 2.71<br>Z =回归的结果</p>
<p><strong>逻辑回归即使用线性回归的思想去做二分类问题，也即将分类问题转化为概率问题，用sigmoid的目的是为了保证每一处都是连续可导的。</strong></p>
<p>输出结果解释(重要)：假设有两个类别A，B，并且假设我们的概率值为属于A(1)这个类别的概率值。现在有一个样本的输入到逻辑回归输出结果0.6，那么这个概率值超过0.5，意味着我们训练或者预测的结果就是A(1)类别。那么反之，如果得出结果为0.3那么，训练或者预测结果就为B(0)类别。</p>
<p>所以接下来我们回忆之前的线性回归预测结果我们用均方误差衡量，那如果对于逻辑回归，我们预测的结果不对该怎么去衡量这个损失呢？我们来看这样一张图</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/4.png" style="zoom:50%;">

<p>那么如何去衡量逻辑回归的预测结果与真实结果的差异呢？</p>
<h3 id="损失以及优化"><a href="#损失以及优化" class="headerlink" title="损失以及优化"></a>损失以及优化</h3><p>逻辑回归的损失，称之为<strong>对数似然损失</strong>，与线性回归原理相同，但由于是分类问题，损失函数不一样，只能通过梯度下降求解。公式如下：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/5.png" style="zoom:60%;">

<p>完整的损失函数：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/6.png" style="zoom:60%;">

<ul>
<li>cost损失的值越小，那么预测的类别准确度更高</li>
<li>yi就是真实值</li>
</ul>
<p>怎么理解单个的式子呢？这个要根据log的函数图像来理解</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/7.png" style="zoom:60%;">

<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/8.png" style="zoom:50%;">

<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/9.png" style="zoom:47%;">

<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/10.png" style="zoom:49%;">

<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/11.png" style="zoom:49%;">

<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/30190/12.png" style="zoom:49%;">

<h3 id="逻辑回归api"><a href="#逻辑回归api" class="headerlink" title="逻辑回归api"></a>逻辑回归api</h3><ul>
<li>sklearn.linear_model.LogisticRegression(solver=’liblinear’, penalty=‘l2’, C = 1.0)<ul>
<li>solver：优化算法的选择，可选参数:{‘liblinear’, ‘sag’, ‘saga’,’newton-cg’, ‘lbfgs’}，<ul>
<li>默认: ‘liblinear’；用于优化问题的算法。</li>
<li>对于小数据集来说，“liblinear”是个不错的选择，而“sag”和’saga’对于大型数据集会更快。</li>
<li>对于多类问题，只有’newton-cg’， ‘sag’， ‘saga’和’lbfgs’可以处理多项损失;“liblinear”仅限于“one-versus-rest”分类。</li>
</ul>
</li>
<li>penalty：正则化项的类型，L1或L2正则化</li>
<li>C：正则化强度的倒数，C越小表示正则化强度越高。C越大，高阶项系数越小。</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>默认将类别数量少的当做正例</strong></p>
</blockquote>
<p>LogisticRegression方法相当于 SGDClassifier(loss=”log”, penalty=” “),SGDClassifier实现了一个普通的随机梯度下降学习。而使用LogisticRegression(实现了SAG)</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>应用: 广告点击率预测、电商购物搭配推荐，是否患病</li>
<li>优点: 适合需要得到一个分类概率的场景，简单，速度快</li>
<li>缺点: 当特征空间很大时，逻辑回归的性能不是很好(看硬件能力)，不好处理多分类问题</li>
</ul>
<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-10-——拟合与正则化</title>
    <url>/6552.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="欠拟合和过拟合"><a href="#欠拟合和过拟合" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><ul>
<li>过拟合：一个假设<strong>在训练数据上能够获得比其他假设更好的拟合， 但是在测试数据集上却不能很好地拟合数据</strong>，此时认为这个假设出现了过拟合的现象。(模型过于复杂)</li>
<li>欠拟合：一个假设<strong>在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好地拟合数据</strong>，此时认为这个假设出现了欠拟合的现象。(模型过于简单)</li>
</ul>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/1.png" style="zoom:40%;">

<h3 id="原因以及解决办法"><a href="#原因以及解决办法" class="headerlink" title="原因以及解决办法"></a>原因以及解决办法</h3><ul>
<li>欠拟合原因以及解决办法<ul>
<li>原因：学习到数据的特征过少</li>
<li>解决办法：<ul>
<li><strong>1）添加其他特征项，</strong>有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。例如，“组合”、“泛化”、“相关性”三类特征是特征添加的重要手段，无论在什么场景，都可以照葫芦画瓢，总会得到意想不到的效果。除上面的特征之外，“上下文特征”、“平台特征”等等，都可以作为特征添加的首选项。</li>
<li><strong>2）添加多项式特征</strong>，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。</li>
</ul>
</li>
</ul>
</li>
<li>过拟合原因以及解决办法<ul>
<li>原因：原始特征过多，存在一些嘈杂特征， 模型过于复杂是因为模型尝试去兼顾各个测试数据点</li>
<li>解决办法：<ul>
<li>1）重新清洗数据，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。</li>
<li>2）增大数据的训练量，还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。</li>
<li><strong>3）正则化</strong></li>
<li>4）减少特征维度，防止<strong>维灾难</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><h3 id="什么是正则化"><a href="#什么是正则化" class="headerlink" title="什么是正则化"></a>什么是正则化</h3><p>在解决回归过拟合中，我们选择正则化。但是对于其他机器学习算法如分类算法来说也会出现这样的问题，除了一些算法本身作用之外（决策树、神经网络），我们更多的也是去自己做特征选择，包括之前说的删除、合并一些特征</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/2.png" style="zoom:50%;">

<p><strong>在学习的时候，数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽量减少这个特征的影响（甚至删除某个特征的影响），这就是正则化</strong></p>
<p>注：调整时候，算法并不知道某个特征影响，而是去调整参数得出优化的结果</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/3.png" style="zoom:40%;">

<h3 id="正则化类别"><a href="#正则化类别" class="headerlink" title="正则化类别"></a>正则化类别</h3><ul>
<li><strong>L2正则化</strong><ul>
<li>作用：可以使得其中一些W的都很小，都接近于0，削弱某个特征的影响</li>
<li>优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象</li>
<li><strong>Ridge回归(岭回归)</strong></li>
</ul>
</li>
<li><strong>L1正则化</strong><ul>
<li>作用：可以使得其中一些W的值直接为0，删除这个特征的影响</li>
<li><strong>LASSO回归</strong></li>
</ul>
</li>
</ul>
<h3 id="L1-与-L2-的区别"><a href="#L1-与-L2-的区别" class="headerlink" title="L1 与 L2 的区别"></a>L1 与 L2 的区别</h3><p>L1正则化产生稀疏的权值,L2正则化产生平滑的权值<br><strong>L1正则化偏向于稀疏，它会自动进行特征选择，去掉一些没用的特征，也就是将这些特征对应的权重置为0</strong>.<br>L2主要功能是为了防止过拟合，<strong>当要求参数越小时，说明模型越简单，而模型越简单则，越趋向于平滑</strong>，从而防止过拟合。</p>
<p><strong>正则化力度:</strong><br><strong>大: 参数趋近于0</strong><br><strong>小: 参数变化小(高阶项权重没变)</strong></p>
<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>欠拟合与过拟合</tag>
        <tag>L1和L2正则化</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-12-——聚类</title>
    <url>/27590.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><h3 id="聚类算法的概念"><a href="#聚类算法的概念" class="headerlink" title="聚类算法的概念"></a>聚类算法的概念</h3><p>一种典型的<strong>无监督</strong>学习算法，主要用于将相似的样本自动归到一个类别中。</p>
<p>在聚类算法中根据样本之间的相似性，将样本划分到不同的类别中，对于不同的相似度计算方法，会得到不同的聚类结果，常用的相似度计算方法有欧式距离法。</p>
<h3 id="聚类算法实现流程"><a href="#聚类算法实现流程" class="headerlink" title="聚类算法实现流程"></a>聚类算法实现流程</h3><p><strong>k-means</strong>其实包含两层内容：</p>
<p> K : 初始中心点个数（计划聚类数）<br>means：求中心点到其他数据点距离的平均值</p>
<p>k-means聚类步骤：</p>
<ul>
<li>1、随机设置K个特征空间内的点作为初始的聚类中心</li>
<li>2、对于其他每个点计算到K个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别</li>
<li>3、接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值）</li>
<li>4、如果计算得出的新中心点与原中心点一样（质心不再移动），那么结束，否则重新进行第二步过程</li>
</ul>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/27590/1.png" style="zoom:40%;">

<h3 id="案例练习"><a href="#案例练习" class="headerlink" title="案例练习"></a>案例练习</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/27590/2.png" style="zoom:50%;">

<p>1、随机设置K个特征空间内的点作为初始的聚类中心（本案例中设置p1和p2）</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/27590/3.png" style="zoom:50%;">

<p>2、对于其他每个点计算到K个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/27590/4.png" style="zoom:50%;">

<p>3、接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值）</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/27590/5.png" style="zoom:50%;">

<p>4、如果计算得出的新中心点与原中心点一样（质心不再移动），那么结束，否则重新进行第二步过程【经过判断，需要重复上述步骤，开始新一轮迭代】</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/27590/6.png" style="zoom:50%;">

<p>5、当每次迭代结果不变时，认为算法收敛，聚类完成，<strong>K-Means一定会停下，不可能陷入一直选质心的过程。</strong></p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/27590/7.png" style="zoom:50%;">

<h3 id="api介绍"><a href="#api介绍" class="headerlink" title="api介绍"></a>api介绍</h3><ul>
<li>sklearn.cluster.KMeans(n_clusters=8)<ul>
<li>参数:<ul>
<li>n_clusters:开始的聚类中心数量<ul>
<li>整型，缺省值=8，生成的聚类数，即产生的质心（centroids）数。</li>
</ul>
</li>
</ul>
</li>
<li>方法:<ul>
<li>estimator.fit(x)</li>
<li>estimator.predict(x)</li>
<li>estimator.fit_predict(x)<ul>
<li>计算聚类中心并预测每个样本属于哪个类别,相当于先调用fit(x),然后再调用predict(x)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Kmeans性能评估指标"><a href="#Kmeans性能评估指标" class="headerlink" title="Kmeans性能评估指标"></a>Kmeans性能评估指标</h3><p>轮廓系数：</p>
<p>结合了聚类的凝聚度（Cohesion）和分离度（Separation），用于评估聚类的效果：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/27590/8.png" style="zoom:65%;">

<p>注：对于每个点i 为已聚类数据中的样本 ，<strong>bi 为i 到最近族群的所有样本的平均距离</strong>，<strong>ai 为i 到本身簇的距离平均值</strong>，<strong>max是bi或者ai的中较大的那个</strong>！最终计算出所有的样本点的轮廓系数平均值</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/27590/9.png" style="zoom:50%;">

<p>对于每一个样本<br>1、计算蓝1到自身类别的点距离的平均值a_i<br>2、计算蓝1分别到红色类别，绿色类别所有的点的距离，求出平均值b1, b2，取其中最小的值当做b_i<br><strong>极端：–这样容易理解</strong><br>b_ i&gt;&gt;a_ i: 1完美<br>a_ i&gt;&gt;b_ i:-1最差<br>蓝1:轮廓系数[-1, 1]<br>超过0.1，说明聚类效果很好—经验</p>
<ul>
<li>sklearn.metrics.silhouette_score(X, labels)<ul>
<li>计算所有样本的平均轮廓系数</li>
<li>X：特征值</li>
<li>labels：被聚类标记的目标值</li>
</ul>
</li>
</ul>
<p><strong>流程</strong>:</p>
<ul>
<li>事先<strong>确定常数K</strong>，常数K意味着最终的聚类类别数;</li>
<li>首先随机<strong>选定初始点为质心</strong>，并通过计算每一个样本与质心之间的相似度(这里为欧式距离)，将样本点归到最相似的类中，</li>
<li>接着，<strong>重新计算</strong>每个类的质心(即为类中心)，重复这样的过程，直到<strong>质心不再改变</strong>，</li>
<li>最终就确定了每个样本所属的类别以及每个类的质心。</li>
</ul>
<p><strong>注意</strong>:</p>
<ul>
<li>由于每次都要计算所有的样本与每一个质心之间的相似度，故在大规模的数据集上，K-Means算法的收敛速度比较慢。</li>
</ul>
<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>无监督学习</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习(3)——数据压缩</title>
    <url>/6552.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="数据压缩"><a href="#数据压缩" class="headerlink" title="数据压缩"></a>数据压缩</h2><h3 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h3><p>主成分分析（Principal Component Analysis, PCA） 也称为 卡尔胡宁-勒夫变换（Karhunen-Loeve Transform），是一种用于探索高维数据结构的技术。PCA 通常用于高维数据集的探索与可视化；还可以用于数据压缩，数据预处理等。PCA 可以把可能具有相关性的高维变量合成线性无关的低维变量，称为主成分（ principal components）。新的低维数据集会尽可能的保留原始数据的变量。</p>
<p>作用：可以<strong>削减回归分析</strong>或者<strong>聚类分析</strong>中<strong>特征的数量</strong>，特征数量达到<strong>上百</strong>的时候，开始使用PCA</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/1.png" style="zoom:50%;">

<p>如图所示，第四张二维的平面图，最能体现出其三维的特性，因此，我们可以使用PCA，将三维的数据，转换为二维的数据，虽然会损失一部分信息，但不影响我们对特征的提取与预测。</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/2.png" style="zoom:50%;">

<p>直观就是，所有点到线的垂直距离和最短的一条线。也确定了每个点在新的维度上的特征值</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/3.png" style="zoom:50%;">

<p>(业界常使用90%～95%)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pca</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    主成分分析进行特征降维</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># n_ components:小数 0~1 90% 业界选择 90~95%</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># 整数   减少到的特征数量</span></span><br><span class="line"></span><br><span class="line">    pca = PCA(n_components=<span class="number">0.9</span>) <span class="comment">#成分保留</span></span><br><span class="line"></span><br><span class="line">    data = pca.fit_transform([[<span class="number">2</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">8</span>], [<span class="number">5</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">    print(data)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">pca()</span><br></pre></td></tr></table></figure>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/4.png" style="zoom:50%;">

<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习(4)——算法</title>
    <url>/6552.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="机器学习算法分类"><a href="#机器学习算法分类" class="headerlink" title="机器学习算法分类"></a>机器学习算法分类</h2><ol>
<li><strong>监督学习</strong>：（英语：Supervised learning），可以由输入数据中学到或建立一个模型，并依此模式推测新的结果。输入数据是由输入特征值和目标值所组成。函数的输出可以是一个连续的值（称为回归），或是输出是有限个离散值（称作分类）分为：<ul>
<li><strong>分类：k-近邻算法、贝叶斯分类、决策树与随机森林、逻辑回归、神经网络</strong></li>
<li><strong>回归：线性回归、岭回归、神经网络</strong></li>
</ul>
</li>
<li><strong>无监督学习</strong>：（英语：Supervised learning），可以由输入数据中学到或建立一个模型，并依此模式推测新的结果。输入数据是由输入特征值所组成。<ul>
<li><strong>聚类：k-means（有几十种聚类）</strong></li>
</ul>
</li>
</ol>
<p>监督学习与非监督学习区别：</p>
<ul>
<li>监督学习:特征值+目标值</li>
<li>非监督学习:特征值1000个样本</li>
</ul>
<p>分类与回归的区别：</p>
<ul>
<li><p>分类:目标值离散型</p>
</li>
<li><p>回归:目标值连续型</p>
</li>
</ul>
<h2 id="分类模型评估指标"><a href="#分类模型评估指标" class="headerlink" title="分类模型评估指标"></a>分类模型评估指标</h2><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/1.png" style="zoom:50%;">

<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/2.png" style="zoom:50%;">

<h3 id="精确率-Precision-与召回率-Recall"><a href="#精确率-Precision-与召回率-Recall" class="headerlink" title="精确率(Precision)与召回率(Recall)"></a>精确率(Precision)与召回率(Recall)</h3><p><strong>精确率</strong>： 预测结果为正例样本中真实为正例的比例（查得准） TP/(TP+FP)</p>
<p><strong>召回率</strong>： 真实为正例的样本中预测结果为正例的比例（查的全， 对正样本的区分能力）TP/(TP+FN)， <strong>医院非常重视 召回率</strong></p>
<h3 id="F1-Score"><a href="#F1-Score" class="headerlink" title="F1-Score"></a>F1-Score</h3><p>既考虑了精确率， 也考虑了召回率的一个评估指标</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/3.png" style="zoom:50%;">

<h3 id="ROC-曲线与-AUC-指标"><a href="#ROC-曲线与-AUC-指标" class="headerlink" title="ROC 曲线与 AUC 指标"></a>ROC 曲线与 AUC 指标</h3><p><strong>TPR（True Positive Rate）</strong> 可以理解为所有正类中， 有多少被预测成正类（正类预测正确） ， 即召回率， 给出定义如下：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/4.png" style="zoom:50%;">

<p><strong>FPR（False Positive Rate）</strong> 可以理解为所有反类中， 有多少被预测成正类（正类预测错误） ， 给出定义如下： 这个值是越小越好</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/5.png" style="zoom:50%;">

<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/6.png" style="zoom:50%;">

<p><strong>AUC 则是 ROC 曲线下的面积</strong></p>
<p>ROC 曲线每个点对应着一个分类器的不同阈值。 通过改变阈值， 可以得到不同的真阳性率（TPR） 和假阳性率（FPR） ， 进而绘制出一条完整的 ROC 曲线。</p>
<p>分类器的阈值是指当分类器输出的概率（或得分） 大于该阈值时， 将样本划分为正类； 否则将其划分为负类。 不同阈值的设置会影响到分类器的性能和预测结果。</p>
<h4 id="在实际应用中，-选取不同阈值所代表的含义如下："><a href="#在实际应用中，-选取不同阈值所代表的含义如下：" class="headerlink" title="在实际应用中， 选取不同阈值所代表的含义如下："></a>在实际应用中， 选取不同阈值所代表的含义如下：</h4><ul>
<li>阈值=0： 分类器将所有样本都预测为正例。 此时， TPR=1， FPR=1， ROC 曲线的最右上角的点。</li>
<li>0 &lt; 阈值 &lt; 1： 分类器将样本分为正例和负例。 随着阈值逐渐增大， TPR 和 FPR 也会变化。</li>
<li>当阈值= 1 时， 分类器将所有样本都预测为负例。 此时， TPR=0， FPR=0， ROC 曲线的最左下角的点。</li>
</ul>
<p>需要注意的是， 通过调整分类器阈值可以改变模型的预测结果和分类性能， 但也可能引入一些新的问题， 例如过拟合、 欠拟合等。 因此， 在实际应用中， 需要综合考虑多种指标和方法，并进行适当的调参和优化。</p>
<h4 id="在实际应用中，-ROC-曲线和-AUC-常被用于以下几个方面："><a href="#在实际应用中，-ROC-曲线和-AUC-常被用于以下几个方面：" class="headerlink" title="在实际应用中， ROC 曲线和 AUC 常被用于以下几个方面："></a>在实际应用中， ROC 曲线和 AUC 常被用于以下几个方面：</h4><ol>
<li><strong>对比不同模型的性能</strong>： 在比较不同分类器的性能时， 可以通过绘制 ROC 曲线并计算 AUC 来确定哪个模型具有更好的分类性能。 AUC 值越高的模型通常被认为是更优秀的模型。</li>
<li><strong>选择最佳阈值</strong>： 在特定场景下， 可能需要根据业务需求选择特定的分类阈值，这时可以通过 ROC 曲线来选择最佳的分类阈值。 例如， 在医疗领域中， 可能更倾向于将假阳性率降到最低， 而在金融领域中， 则可能更关注漏报率。</li>
<li><strong>检测模型的稳定性</strong>： 在数据集或分类器发生变化或者存在很多噪声的情况下，ROC 曲线和 AUC 可以帮助评估模型的稳健性， 即模型对这些变化的敏感程度。ROC 曲线和 AUC 是机器学习中重要的评价指标， 可以帮助我们更全面地了解模型的性能， 并进行模型选择、 调参和优化等工作。</li>
</ol>
<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
        <tag>评估指标</tag>
        <tag>ROC</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习(2)——归一化与标准化</title>
    <url>/11448.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h2><h3 id="归一化Normalization"><a href="#归一化Normalization" class="headerlink" title="归一化Normalization"></a>归一化Normalization</h3><p>由于每一组特征数据的量纲是不同的，我们需要先将他们的量纲统一，之后再去训练学习，每一组特征数据的变化对预测结果的影响。</p>
<p><strong>特点:通过对原始数据进行变换把数据映射到(默认为[0,1])之间</strong></p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/11448/1.png" style="zoom:50%;">

<p>def MinMaxScaler(feature_range=(0, 1…))：归一化。每个特征缩放到给定范围(默认[0,1])</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler, StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mm</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    归一化处理</span></span><br><span class="line"><span class="string">    :return: NOne</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 归一化容易受极值的影响</span></span><br><span class="line">    mm = MinMaxScaler(feature_range=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">		</span><br><span class="line">    <span class="comment">#通过fit_transform进行转换</span></span><br><span class="line">    data = mm.fit_transform([[<span class="number">90</span>, <span class="number">2</span>, <span class="number">10</span>, <span class="number">40</span>], [<span class="number">60</span>, <span class="number">4</span>, <span class="number">15</span>, <span class="number">45</span>], [<span class="number">75</span>, <span class="number">3</span>, <span class="number">13</span>, <span class="number">46</span>]])</span><br><span class="line"></span><br><span class="line">    print(data)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#如果使用transform进行测试集的时候，量纲还是fit_transform的</span></span><br><span class="line">    out = mm.fit_transform([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">6</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">7</span>]])</span><br><span class="line"></span><br><span class="line">    print(out)</span><br><span class="line">    <span class="comment"># [[-1.96666667 0.		-1.4		-6		]</span></span><br><span class="line">    <span class="comment">#  [-1.8				1.5		-0.4		-5.5	]]</span></span><br><span class="line">		<span class="comment"># 相当于是用 (1-90/90-60) *1 + 1 = -1.966667</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">mm()</span><br></pre></td></tr></table></figure>


<ol>
<li><strong>归一化的好处</strong>：容易更快地通过梯度下降找到最优解</li>
<li>归一化的弊端：由于是使用max和min数据进行处理，会很容易受极值的影响，如果max很大，而其他数据很小，那归一化后的其他数据则很接近0</li>
</ol>
<p><strong>注意</strong>：在特定场景下最大值最小值是变化的，另外，最大值与最小值非常容易受<strong>异常点</strong>影响，所以这种方法鲁棒性较差，<strong>只适合传统精确小数据场景</strong>。</p>
<h3 id="标准化Standardization"><a href="#标准化Standardization" class="headerlink" title="标准化Standardization"></a>标准化Standardization</h3><p>z-score 标准化(zero-mean normalization)，最常见的标准化方法就是Z标准化，也叫<strong>标准差标准化</strong>。通过对原始数据进行变换<strong>把数据变换到均值为0,标准差为1范围内</strong>，不是标准正态分布，只是均值为0，标准差为1的均匀分布</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/11448/2.png" style="zoom:25%;">

<p>def  StandardScaler(…)：标准化</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler, StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stand</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    标准化缩放，不是标准正太分布，只均值为0，方差为1的分布</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    std = StandardScaler()</span><br><span class="line">    data = std.fit_transform([[<span class="number">1.</span>, -<span class="number">1.</span>, <span class="number">3.</span>], [<span class="number">2.</span>, <span class="number">4.</span>, <span class="number">2.</span>], [<span class="number">4.</span>, <span class="number">6.</span>, -<span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line">    print(data)</span><br><span class="line">    print(std.mean_)</span><br><span class="line">    print(std.var_)</span><br><span class="line">    print(std.n_samples_seen_)  <span class="comment"># 样本数</span></span><br><span class="line">    print(<span class="string">&#x27;-&#x27;</span> * <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">stand()</span><br></pre></td></tr></table></figure>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/11448/3.png" style="zoom:50%;">

<ul>
<li>对于归一化来说：如果出现异常点，影响了<strong>最大值和最小值</strong>，那么结果显然会发生改变</li>
<li>对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于<strong>平均值的影响</strong>并不大，从而方差改变<strong>较小</strong>。</li>
</ul>
<p><strong>注意</strong>：在已有<strong>样本足够多</strong>的情况下比较稳定，适合现代嘈杂大数据场景。</p>
<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-5-——转换器与估计器</title>
    <url>/6552.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="转换器（Transformer）"><a href="#转换器（Transformer）" class="headerlink" title="转换器（Transformer）"></a>转换器（Transformer）</h2><p>转换器是一种用于数据预处理的工具，它可以将原始数据转换为机器学习算法可以处理的格式。转换器通常用于特征工程，这是一种从原始数据中提取有意义特征的过程。</p>
<p>在sklearn中，转换器通常实现了一个名为<code>fit_transform</code>的方法。这个方法首先通过<code>fit</code>步骤学习数据的特性（如平均值、标准差等），然后通过<code>transform</code>步骤应用这些学习到的特性来转换数据。</p>
<p>转换器是一种实现特征工程操作的一组<a href="https://so.csdn.net/so/search?q=API&spm=1001.2101.3001.7020">API</a>，可以较方便的完成常用的特征工程操作，包括：</p>
<ul>
<li>fit：计算一些数据的平均数、方差等</li>
<li>transform：进行一些数据转换</li>
<li>fit_transform：相当于fit+transform，既实现了fit的功能，又实现了transform的功能。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">std1=StandardScaler()</span><br><span class="line">a=[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]</span><br><span class="line">std1.fit_transform(a)</span><br><span class="line"><span class="comment"># 结果如下</span></span><br><span class="line">array([[-<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>], [ <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line">print(std1.fit(a))<span class="comment">#计算每一列的平均值与标准差</span></span><br><span class="line">StandardScaler(copy=<span class="literal">True</span>, with_mean=<span class="literal">True</span>, with_std=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">std1.transform(a)<span class="comment">#进行最终的标准化</span></span><br><span class="line"><span class="comment"># 结果如下</span></span><br><span class="line">array([[-<span class="number">1.</span>, -<span class="number">1.</span>, -<span class="number">1.</span>],</span><br><span class="line">[ <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure>
<p>例如，<code>StandardScaler</code>是一个常用的转换器，它可以将特征数据标准化，即减去平均值并除以标准差。这样做的好处是，标准化后的数据具有零均值和单位方差，这对于许多机器学习算法来说是非常有益的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化转换器</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用fit_transform方法对数据进行转换</span></span><br><span class="line">X_scaled = scaler.fit_transform(X)</span><br></pre></td></tr></table></figure>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/3.png" style="zoom:45%;">



<h2 id="估计器（Estimator）"><a href="#估计器（Estimator）" class="headerlink" title="估计器（Estimator）"></a>估计器（Estimator）</h2><p>估计器是机器学习算法的主要实现者。在sklearn中，估计器是一个实现了特定机器学习算法的类。例如，<code>LogisticRegression</code>是一个实现了逻辑回归算法的估计器，<code>RandomForestClassifier</code>是一个实现了随机森林算法的估计器。包括：</p>
<p><strong>1、用于分类的估计器：</strong></p>
<ul>
<li>sklearn.neighbors k-近邻算法</li>
<li>sklearn.naive_bayes 贝叶斯</li>
<li>sklearn.linear_model.LogisticRegression 逻辑回归</li>
<li>sklearn.tree 决策树与随机森林</li>
</ul>
<p>2、用于回归的估计器：</p>
<ul>
<li>sklearn.linear_model.LinearRegression 线性回归</li>
<li>sklearn.linear_model.Ridge 岭回归</li>
</ul>
<p><strong>3、用于无监督学习的估计器：</strong></p>
<ul>
<li>sklearn.cluster.KMeans 聚类</li>
</ul>
<p>估计器通常提供了<code>fit</code>、<code>predict</code>等方法。<code>fit</code>方法用于训练模型，它接受训练数据作为输入并学习数据的特性。<code>predict</code>方法则用于对新数据进行预测。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化估计器</span></span><br><span class="line">clf = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用fit方法对模型进行训练</span></span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用predict方法对测试数据进行预测</span></span><br><span class="line">y_pred = clf.predict(X_test)</span><br></pre></td></tr></table></figure>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/2.jpg" style="zoom:45%;">

<h3 id="估计器工作流程"><a href="#估计器工作流程" class="headerlink" title="估计器工作流程"></a>估计器工作流程</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/1.jpg" style="zoom:25%;">



<h2 id="转换器与估计器的结合"><a href="#转换器与估计器的结合" class="headerlink" title="转换器与估计器的结合"></a>转换器与估计器的结合</h2><p>在sklearn中，转换器和估计器可以非常方便地结合使用。这是因为sklearn提供了一个名为<code>Pipeline</code>的工具，它可以将转换器和估计器组合成一个整体。</p>
<p>例如，我们可以创建一个包含标准化转换器和逻辑回归估计器的管道：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建管道</span></span><br><span class="line">pipe = Pipeline([</span><br><span class="line">    (<span class="string">&#x27;scaler&#x27;</span>, StandardScaler()),</span><br><span class="line">    (<span class="string">&#x27;clf&#x27;</span>, LogisticRegression())</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用fit方法对模型进行训练</span></span><br><span class="line">pipe.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用predict方法对测试数据进行预测</span></span><br><span class="line">y_pred = pipe.predict(X_test)</span><br></pre></td></tr></table></figure>
<p>在这个例子中，<code>Pipeline</code>会自动处理数据的转换和模型的训练。我们只需要调用<code>fit</code>和<code>predict</code>方法，<code>Pipeline</code>就会按照我们指定的顺序执行转换和预测。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>转换器和估计器是机器学习中的两个核心概念。转换器负责数据的预处理，而估计器则负责实现具体的机器学习算法。在sklearn中，这两个概念得到了很好的实现和整合，使得我们可以方便地进行机器学习任务。通过理解并熟练使用转换器和估计器，我们可以更好地掌握机器学习的精髓，并在实际应用中取得更好的效果。</p>
<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-7-——朴素贝叶斯算法</title>
    <url>/6552.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><p>贝叶斯公式（两个特征之间独立）：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/1.png" style="zoom:70%;">

<p>换个表达形式就会明朗很多，如下：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/2.png" style="zoom:40%;">

<p>我们最终求的p(类别|特征)即可！就相当于完成了我们的任务。</p>
<p>公式分为三个部分：</p>
<ul>
<li>P(C):每个文档类别的概率(某文档类别数/总文档数</li>
<li>P(WIC):<strong>给定某个类别</strong>下<strong>特征(被预测文档中出现的词)的概率</strong></li>
</ul>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/3.png" style="zoom:50%;">

<h3 id="拉普拉斯平滑"><a href="#拉普拉斯平滑" class="headerlink" title="拉普拉斯平滑"></a>拉普拉斯平滑</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/4.png" style="zoom:50%;">

<p>属于某个类别为 0， 合适吗？</p>
<p><strong>问题</strong>： 从上面的例子我们得到娱乐概率为 0， 这是不合理的， 如果词频列表里面有很多出现次数都为 0， 很可能计算结果都为零<br><strong>解决方法</strong>： 拉普拉斯平滑系数</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/5.png" style="zoom:40%;">

<p>α 为指定的系数一般为 1， m 为代表类别的数目（标签的数目） (我们分为两类,科技类与娱乐类)</p>
<h3 id="sklearn-朴素贝叶斯实现-API"><a href="#sklearn-朴素贝叶斯实现-API" class="headerlink" title="sklearn 朴素贝叶斯实现 API"></a>sklearn 朴素贝叶斯实现 API</h3><p>sklearn.naive_bayes.MultinomialNB</p>
<p>MultinomialNB</p>
<ul>
<li>sklearn.naive_bayes.MultinomialNB(alpha = 1.0)</li>
<li>朴素贝叶斯分类</li>
<li>alpha： 拉普拉斯平滑系数</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">朴素贝叶斯进行文本分类</span></span><br><span class="line"><span class="string">:return: None</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">news = fetch_20newsgroups(subset=<span class="string">&#x27;all&#x27;</span>, data_home=<span class="string">&#x27;data&#x27;</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="built_in">len</span>(news.data))  <span class="comment">#样本数，包含的特征</span></span><br><span class="line">print(<span class="string">&#x27;-&#x27;</span>*<span class="number">50</span>)</span><br><span class="line">print(news.data[<span class="number">0</span>]) <span class="comment">#第一个样本 特征</span></span><br><span class="line">print(<span class="string">&#x27;-&#x27;</span>*<span class="number">50</span>)</span><br><span class="line">print(news.target) <span class="comment">#标签</span></span><br><span class="line">print(np.unique(news.target)) <span class="comment">#标签的类别</span></span><br><span class="line">print(news.target_names) <span class="comment">#标签的名字</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;-&#x27;</span>*<span class="number">50</span>)</span><br><span class="line"><span class="comment"># 进行数据分割</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(news.data, news.target, test_size=<span class="number">0.25</span>, random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对数据集进行特征抽取</span></span><br><span class="line">tf = TfidfVectorizer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以训练集当中的词的列表进行每篇文章重要性统计[&#x27;a&#x27;,&#x27;b&#x27;,&#x27;c&#x27;,&#x27;d&#x27;]</span></span><br><span class="line">x_train = tf.fit_transform(x_train)</span><br><span class="line"><span class="comment">#针对特征内容，可以自行打印，下面的打印可以得到特征数目</span></span><br><span class="line">print(<span class="built_in">len</span>(tf.get_feature_names_out()))</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="comment"># 进行朴素贝叶斯算法的预测,alpha是拉普拉斯平滑系数，分子和分母加上一个系数，分母加alpha*特征词数目</span></span><br><span class="line">mlt = MultinomialNB(alpha=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">print(x_train.toarray())</span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">start=time.time()</span><br><span class="line">mlt.fit(x_train, y_train)</span><br><span class="line">end=time.time()</span><br><span class="line">end-start <span class="comment">#统计训练时间</span></span><br><span class="line"></span><br><span class="line">x_test = tf.transform(x_test)  <span class="comment">#特征数目不发生改变</span></span><br><span class="line">print(<span class="built_in">len</span>(tf.get_feature_names_out()))</span><br><span class="line"></span><br><span class="line">start=time.time()</span><br><span class="line">y_predict = mlt.predict(x_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;预测的前面10篇文章类别为：&quot;</span>, y_predict[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得出准确率,这个是很难提高准确率，为什么呢？</span></span><br><span class="line">print(<span class="string">&quot;准确率为：&quot;</span>, mlt.score(x_test, y_test))</span><br><span class="line">end=time.time()</span><br><span class="line">end-start</span><br><span class="line"></span><br><span class="line"><span class="comment"># 目前这个场景我们不需要召回率，support是真实的为那个类别的有多少个样本</span></span><br><span class="line">print(<span class="string">&quot;每个类别的精确率和召回率：&quot;</span>,</span><br><span class="line">      classification_report(y_test, y_predict,</span><br><span class="line">      target_names=news.target_names))</span><br></pre></td></tr></table></figure>


<h3 id="朴素贝叶斯分类优缺点"><a href="#朴素贝叶斯分类优缺点" class="headerlink" title="朴素贝叶斯分类优缺点"></a>朴素贝叶斯分类优缺点</h3><p><strong>优点：</strong></p>
<ul>
<li>朴素贝叶斯模型发源于古典数学理论， 有稳定的分类效率。</li>
<li>对缺失数据不太敏感， 算法也比较简单， 常用于文本分类。</li>
<li>分类准确度高， 速度快</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>需要知道先验概率 P(F1,F2,…|C)， 因此在某些时候会由于假设的先验模型的原因 导致预测效果不佳</li>
<li>假设了文章当中一些词语另外一些是独立没关系—-如果有关系， 会造成不太靠谱</li>
<li>训练集当中去进行统计词这些工作 文章收集的不好， 比如有作弊文章， 充斥某个词会对结果造成干扰</li>
</ul>
<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>分类算法</tag>
        <tag>朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-6-——K近邻算法</title>
    <url>/6552.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="k近邻（k-NN）算法"><a href="#k近邻（k-NN）算法" class="headerlink" title="k近邻（k-NN）算法"></a>k近邻（k-NN）算法</h2><p>k近邻算法是一种<strong>基本分类和回归方法</strong>。K近邻算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例<strong>最邻近</strong>的K个实例，<strong>这K个实例的多数属于某个类</strong>，就把该输入实例分类到这个类中。（<strong>这就类似于现实生活中少数服从多数的思想</strong>）根据这个说法，咱们来看下引自维基百科上的一幅图：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/1.png" style="zoom:50%;">

<p>如上图所示，有<strong>两类</strong>不同的<a href="https://zhida.zhihu.com/search?content_id=2584561&content_type=Article&match_order=1&q=%E6%A0%B7%E6%9C%AC%E6%95%B0%E6%8D%AE&zhida_source=entity">样本数据</a>，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的那个绿色的圆所标示的数据则是<strong>待分类的数据</strong>。这也就是我们的目的，来了一个新的数据点，我要得到它的类别是什么？好的，下面我们根据k近邻的思想来给绿色圆点进行分类。</p>
<ul>
<li>如果K=3，绿色圆点的最邻近的3个点是2个红色小三角形和1个蓝色小正方形，<strong>少数从属于多数，</strong>基于统计的方法，判定绿色的这个待分类点属于红色的三角形一类。</li>
<li>如果K=5，绿色圆点的最邻近的5个邻居是2个红色三角形和3个蓝色的正方形，<strong>还是少数从属于多数，</strong>基于统计的方法，判定绿色的这个待分类点属于蓝色的正方形一类。</li>
</ul>
<h3 id="sklearn-k-近邻算法-API"><a href="#sklearn-k-近邻算法-API" class="headerlink" title="sklearn k-近邻算法 API"></a>sklearn k-近邻算法 API</h3><ul>
<li>sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm=’auto’)</li>
<li>n_neighbors： int,可选（默认= 5） ， k_neighbors 查询默认使用的邻居数</li>
<li>algorithm： {‘auto’ ， ‘ball_tree’ ， ‘kd_tree’ ， ‘brute’ }， 可选用于计算最近邻居的算法： ‘ball_tree’ 将会使用 BallTree， ‘kd_tree’ 将使用 KDTree。 ‘auto’ 将尝试根据传递给 fit 方法的值来决定最合适的算法。 (不同实现方式影响效率)</li>
</ul>
<h3 id="一般来说，KNN-分类算法的计算过程-预测-："><a href="#一般来说，KNN-分类算法的计算过程-预测-：" class="headerlink" title="一般来说，KNN 分类算法的计算过程(预测)："></a>一般来说，KNN 分类算法的计算过程(预测)：</h3><ol>
<li>计算待分类点与已知类别的点之间的距离</li>
<li>按照距离递增次序排序</li>
<li>选取与待分类点距离最小的 K 个点</li>
<li>确定前 K 个点所在类别的出现次数</li>
<li>返回前 K 个点出现次数最高的类别作为待分类点的预测分类</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行数据的分割训练集合测试集</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.25</span>, random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征工程（标准化）,下面3行注释，一开始我们不进行标准化，看下效果，目标值要不要标准化？</span></span><br><span class="line">std = StandardScaler()</span><br><span class="line"><span class="comment"># #</span></span><br><span class="line"><span class="comment"># # # 对测试集和训练集的特征值进行标准化,服务于knn fit</span></span><br><span class="line">x_train = std.fit_transform(x_train)</span><br><span class="line"><span class="comment"># # transform返回的是copy，不在原有的输入对象中去修改</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 进行算法流程 # 超参数，可以通过设置n_neighbors=5，来调整结果好坏</span></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># # fit， predict,score，训练，knn的fit是不训练的，只是把训练集的特征值和目标值放入到内存中</span></span><br><span class="line">knn.fit(x_train, y_train)</span><br><span class="line"><span class="comment"># # #</span></span><br><span class="line"><span class="comment"># # # 得出预测结果</span></span><br><span class="line">y_predict = knn.predict(x_test)</span><br><span class="line"><span class="comment"># #</span></span><br><span class="line">print(<span class="string">&quot;预测的目标签到位置为：&quot;</span>, y_predict[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"><span class="comment"># # #</span></span><br><span class="line"><span class="comment"># # # # 得出准确率</span></span><br><span class="line">print(<span class="string">&quot;预测的准确率:&quot;</span>, knn.score(x_test, y_test))</span><br><span class="line"><span class="comment"># print(y_predict)</span></span><br><span class="line"><span class="comment"># y_test</span></span><br></pre></td></tr></table></figure>


<h3 id="特征归一化的必要性"><a href="#特征归一化的必要性" class="headerlink" title="特征归一化的必要性"></a>特征归一化的必要性</h3><p>首先举例如下，我用一个人身高(cm)与脚码（尺码）大小来作为<a href="https://zhida.zhihu.com/search?content_id=2584561&content_type=Article&match_order=1&q=%E7%89%B9%E5%BE%81%E5%80%BC&zhida_source=entity">特征值</a>，类别为男性或者女性。我们现在如果有5个训练样本，分布如下：</p>
<p>A [(179,42),男] B [(178,43),男] C [(165,36)女] D [(177,42),男] E [(160,35),女]</p>
<p>很容易看到第一维身高特征是第二维脚码特征的4倍左右，那么在进行距离度量的时候，<strong>我们就会偏向于第一维特征。</strong>这样造成俩个特征并不是等价重要的，最终可能会导致距离计算错误，从而导致预测错误。口说无凭，举例如下：</p>
<p>现在我来了一个测试样本 F(167,43)，让我们来预测他是男性还是女性，我们采取k=3来预测。</p>
<p>下面我们用欧式距离分别算出F离训练样本的欧式距离，然后选取最近的3个，多数类别就是我们最终的结果，计算如下：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/2.png" style="zoom:45%;">

<p>由计算可以得到，最近的前三个分别是C,D,E三个样本，那么由C,E为女性，D为男性，女性多于男性得到我们要预测的结果为<strong>女性</strong>。</p>
<p><strong>这样问题就来了，一个女性的脚43码的可能性，远远小于男性脚43码的可能性，那么为什么算法还是会预测F为女性呢？那是因为由于各个特征量纲的不同，在这里导致了身高的重要性已经远远大于脚码了，这是不客观的。</strong>所以我们应该让每个特征都是同等重要的！这也是我们要归一化的原因！</p>
<h3 id="k近邻算法中k的选取"><a href="#k近邻算法中k的选取" class="headerlink" title="k近邻算法中k的选取"></a>k近邻算法中k的选取</h3><ol>
<li><strong>选取k值以及它的影响</strong></li>
</ol>
<p>k近邻的k值我们应该怎么选取呢？</p>
<p><strong>如果我们选取较小的k值，那么就会意味着我们的整体模型会变得复杂，容易发生过拟合！</strong>恩<del>结论说完了，太抽象了吧你，不上图讲解号称通俗讲解的都是流氓**</del>好吧，那我就上图来讲解**</p>
<p><strong>假设我们选取k=1这个极端情况，怎么就使得模型变得复杂，又容易过拟合了呢？</strong></p>
<p><strong>假设我们有训练数据和待分类点如下图：</strong></p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/3.png" style="zoom:50%;">

<p>上图中有俩类，一个是<strong>黑色的圆点</strong>，一个是<strong>蓝色的长方形</strong>，现在我们的待分类点是<strong>红色的五边形。</strong></p>
<p>好，根据我们的<a href="https://zhida.zhihu.com/search?content_id=2584561&content_type=Article&match_order=6&q=k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95&zhida_source=entity">k近邻算法</a>步骤来决定待分类点应该归为哪一类。我们由图中可以得到，<strong>很容易我们能够看出来五边形离黑色的圆点最近，k又等于1，那太好了</strong>，我们最终判定待分类点是黑色的圆点。</p>
<p>由这个处理过程我们很容易能够感觉出问题了，如果k太小了，比如等于1，那么模型就太复杂了，<strong>我们很容易学习到噪声</strong>，也就非常容易判定为噪声类别，而在上图，如果，k大一点，k等于8，<strong>把长方形都包括进来</strong>，我们很容易得到我们正确的分类应该是蓝色的长方形！如下图：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/4.png" style="zoom:50%;">

<p>所谓的<a href="https://zhida.zhihu.com/search?content_id=2584561&content_type=Article&match_order=3&q=%E8%BF%87%E6%8B%9F%E5%90%88&zhida_source=entity">过拟合</a>就是在训练集上准确率非常高，而在测试集上准确率低，经过上例，我们可以得到k太小会导致<strong>过拟合</strong>，<strong>很容易将一些噪声（如上图离五边形很近的黑色圆点）学习到模型中，而忽略了数据真实的分布！</strong></p>
<p><strong>如果我们选取较大的k值，就相当于用较大邻域中的训练数据进行预测，这时与输入实例较远的（不相似）训练实例也会对预测起作用，使预测发生错误，k值的增大意味着整体模型变得简单。</strong></p>
<p>k值增大怎么就意味着模型变得简单了？</p>
<p><strong>我们想，如果k=N（N为<a href="https://zhida.zhihu.com/search?content_id=2584561&content_type=Article&match_order=1&q=%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC&zhida_source=entity">训练样本</a>的个数）,那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型是不是非常简单，这相当于你压根就没有训练模型呀！</strong>直接拿训练数据统计了一下各个数据的类别，找最大的而已！这好像下图所示：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/5.png" style="zoom:40%;">

<p>我们统计了黑色圆形是8个，长方形个数是7个，那么哈哈，如果k=N，我就得出结论了，红色五边形是属于黑色圆形的。<strong>这个时候，模型过于简单，完全忽略训练数据实例中的大量有用信息，是不可取的。</strong></p>
<h3 id="距离的度量"><a href="#距离的度量" class="headerlink" title="距离的度量"></a>距离的度量</h3><p>在上文中说到，k近邻算法是在训练数据集中找到与该实例<strong>最邻近</strong>的K个实例，这K个实例的多数属于某个类，我们就说预测点属于哪个类。</p>
<p>定义中所说的最邻近是如何度量呢？我们怎么知道谁跟测试点最邻近。这里就会引出我们几种度量俩个点之间距离的标准。</p>
<p>我们可以有以下几种度量方式：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/6.png" style="zoom:40%;">

<p>其中当p=2的时候，就是我们最常见的欧式距离，我们也一般都用欧式距离来衡量我们<a href="https://zhida.zhihu.com/search?content_id=2584561&content_type=Article&match_order=1&q=%E9%AB%98%E7%BB%B4%E7%A9%BA%E9%97%B4&zhida_source=entity">高维空间</a>中俩点的距离。在实际应用中，<a href="https://zhida.zhihu.com/search?content_id=2584561&content_type=Article&match_order=1&q=%E8%B7%9D%E7%A6%BB%E5%87%BD%E6%95%B0&zhida_source=entity">距离函数</a>的选择应该根据数据的特性和分析的需要而定，一般选取p=2欧式距离表示，这不是本文的重点。</p>
<h3 id="k-近邻算法优缺点"><a href="#k-近邻算法优缺点" class="headerlink" title="k-近邻算法优缺点"></a>k-近邻算法优缺点</h3><p><strong>优点：</strong></p>
<ol>
<li>算法简单，理论成熟，既可以用来做分类也可以用来做回归。</li>
<li>可用于非线性分类。(Y=kx 是线性）</li>
<li>没有明显的训练过程，而是在程序开始运行时，把数据集加载到内存后，不需要进行训练，直接进行预测，所以训练时间复杂度为 0。</li>
<li>由于 KNN 方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属的类别，因此对于类域的交叉或重叠较多的待分类样本集来说，KNN 方法较其他方法更为适合。</li>
<li>该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量比较小的类域采用这种算法比较容易产生误分类情况。</li>
</ol>
<p><strong>缺点：</strong></p>
<ol>
<li>需要算每个测试点与训练集的距离，当训练集较大时，计算量相当大，时间复杂度高，特别是特征数量比较大的时候。（预测的时间复杂度高）</li>
<li>需要大量的内存，空间复杂度高。</li>
<li>样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少），对稀有类别的预测准确度低。</li>
</ol>
<h3 id="KNN-算法适用于以下场景："><a href="#KNN-算法适用于以下场景：" class="headerlink" title="KNN 算法适用于以下场景："></a>KNN 算法适用于以下场景：</h3><ol>
<li>数据集具有明显的类别划分；</li>
<li>样本数量比较大， 样本特征维度较小；</li>
<li>类别边界比较清晰， 即不同类别的数据在特征空间中分离明显；</li>
<li>预测目标的类别或值受到周围邻居影响较大。</li>
</ol>
<p>需要注意的是， KNN 算法在处理高维稀疏数据时可能会出现“维数灾难” 问题， 同时也对噪<br>声敏感， 并且需要选择合适的距离度量方法和 K 值大小。</p>
<h2 id="网格搜索"><a href="#网格搜索" class="headerlink" title="网格搜索"></a>网格搜索</h2><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><p>交叉验证：将拿到的训练数据，分为训练和验证集。以下图为例：将数据分成5份，其中一份作为验证集。然后经过5次(组)的测试，每次都更换不同的验证集。即得到5组模型的结果，取平均值作为最终结果。又称5折交叉验证。</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/7.png" style="zoom:40%;">

<h3 id="网格搜索-1"><a href="#网格搜索-1" class="headerlink" title="网格搜索"></a>网格搜索</h3><p>通常情况下，有很多参数是需要手动指定的（如k-近邻算法中的K值），这种叫超参数。但是手动过程繁杂，所以需要对模型预设几种超参数组合。每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。</p>
<ul>
<li>sklearn.model_selection.GridSearchCV(estimator, param_grid=None,cv=None)<ul>
<li>对估计器的指定参数值进行详尽搜索</li>
<li>estimator：估计器对象</li>
<li>param_grid：估计器参数(dict){“n_neighbors”:[1,3,5]}</li>
<li>cv：指定几折交叉验证</li>
<li>fit：输入训练数据</li>
<li>score：准确率</li>
<li>结果分析：<ul>
<li>best_score_:在交叉验证中验证的最好结果*</li>
<li>best_estimator_：最好的参数模型</li>
<li>cv_results_:每次交叉验证后的验证集准确率结果和训练集准确率结果</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="comment">#网格搜索时讲解</span></span><br><span class="line"><span class="comment"># # 构造一些参数的值进行搜索</span></span><br><span class="line">param = &#123;<span class="string">&quot;n_neighbors&quot;</span>: [<span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">15</span>],<span class="string">&#x27;weights&#x27;</span>:[<span class="string">&#x27;uniform&#x27;</span>, <span class="string">&#x27;distance&#x27;</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行网格搜索，cv=3是3折交叉验证，用其中2折训练，1折验证</span></span><br><span class="line">gc = GridSearchCV(knn, param_grid=param, cv=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">gc.fit(x_train, y_train)  <span class="comment">#你给它的x_train，它又分为训练集，验证集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测准确率，为了给大家看看</span></span><br><span class="line">print(<span class="string">&quot;在测试集上准确率：&quot;</span>, gc.score(x_test, y_test))</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;在交叉验证当中最好的结果：&quot;</span>, gc.best_score_) <span class="comment">#最好的结果</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;选择最好的模型是：&quot;</span>, gc.best_estimator_) <span class="comment">#最好的模型,告诉你用了哪些参数</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;每个超参数每次交叉验证的结果：&quot;</span>, gc.cv_results_)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果如图所示：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/8.png" style="zoom:50%;">



<h2 id="参数与超参数"><a href="#参数与超参数" class="headerlink" title="参数与超参数"></a>参数与超参数</h2><p><strong>参数</strong>：就是模型可以根据数据可以自动学习出的变量，应该就是参数。比如，深度学习的权重，偏差等</p>
<p><strong>超参数</strong>：就是用来确定模型的一些参数，超参数不同，模型是不同的(这个模型不同的意思就是有微小的区别，比如假设都是CNN模型，如果层数不同，模型不一样，虽然都是CNN模型哈。)，超参数一般就是根据经验确定的变量。在深度学习中，超参数有：学习速率，迭代次数，层数，每层神经元的个数等等。</p>
<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
        <tag>K近邻</tag>
        <tag>网格搜索</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-8-——决策树算法</title>
    <url>/6552.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a>决策树算法</h2><p><strong>决策树：是一种树形结构，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，最后每个叶节点代表一种分类结果，本质是一颗由多个判断节点组成的树</strong>。</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/1.png" style="zoom:50%;">

<p>树的节点如何划分？很明显，从图中是按照重要程度进行，逐个划分，越往上重要程度更高，但如何定义重要程度呢？这就需要引入<strong>信息熵</strong>的概念</p>
<h3 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h3><p>物理学上，<strong>熵 Entropy</strong> 是“混乱”程度的量度。</p>
<p><strong>系统越有序，熵值越低；系统越混乱或者分散，熵值越高</strong>。</p>
<ul>
<li><strong>信息理论</strong>：</li>
</ul>
<p>1、<strong>从信息的完整性上进行的描述:</strong></p>
<p>当<strong>系统的有序状态一致时</strong>，**数据越集中的地方熵值越小，数据越分散的地方熵值越大。</p>
<p>2、<strong>从信息的有序性上进行的描述:</strong></p>
<p>当<strong>数据量一致时</strong>，<strong>系统越有序，熵值越低；系统越混乱或者分散，熵值越高</strong>。</p>
<p>1948年香农提出了<strong>信息熵</strong>（Entropy）的概念。</p>
<p>假如事件A的分类划分是（A1,A2,…,An），每部分发生的概率是(p1,p2,…,pn)，那信息熵定义为公式如下：（log是以2为底，lg是以10为底）</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/2.png" style="zoom:50%;">

<h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">课堂案例1：</span><br><span class="line">如果一颗骰子的六个面都是1 ，投掷它不会给你带来任何新信息，因为你知道它的结果肯定是1，它的信息熵为？？</span><br><span class="line"></span><br><span class="line">答案：</span><br><span class="line"> - log(1) = 0 。</span><br></pre></td></tr></table></figure>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">课堂案例2：</span><br><span class="line">假设我们没有看世界杯的比赛，但是想知道哪支球队会是冠军，</span><br><span class="line">我们只能猜测某支球队是或不是冠军，然后观众用对或不对来回答，</span><br><span class="line">我们想要猜测次数尽可能少，你会用什么方法？</span><br><span class="line"></span><br><span class="line">答案：</span><br><span class="line">二分法：</span><br><span class="line">假如有 16 支球队，分别编号，先问是否在 1-8 之间，如果是就继续问是否在 1-4 之间，</span><br><span class="line">以此类推，直到最后判断出冠军球队是哪只。</span><br><span class="line">如果球队数量是 16，我们需要问 4 次来得到最后的答案。那么世界冠军这条消息的信息熵就是 4。</span><br><span class="line"></span><br><span class="line">如果有32个球队，准确的信息量应该是： </span><br><span class="line">H = -（p1 * logp1 + p2 * logp2 + ... + p32 * logp32），</span><br><span class="line">其中 p1, ..., p32 分别是这 32 支球队夺冠的概率。</span><br><span class="line">当每支球队夺冠概率相等都是 1/32 的时：H = -（32 * 1/32 * log1/32） = 5</span><br><span class="line">每个事件概率相同时，熵最大，这件事越不确定。</span><br></pre></td></tr></table></figure>
<h3 id="决策树的划分依据一——信息增益——ID3"><a href="#决策树的划分依据一——信息增益——ID3" class="headerlink" title="决策树的划分依据一——信息增益——ID3"></a>决策树的划分依据一——信息增益——ID3</h3><p><strong>信息增益：</strong>以某特征划分数据集前后的熵的差值。熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。因此可以<strong>使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏</strong>。</p>
<p><strong>信息增益 = entroy(前) - entroy(后)</strong></p>
<ul>
<li>定义与公式</li>
</ul>
<p>特征A对训练数据集D的信息增益g(D,A),定义为集合D的信息熵H(D)与特征A给定条件下D的信息条件熵H(D|A)之差，即公式为：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/3.png" style="zoom:35%;">

<p>公式的详细解释：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/4.png" style="zoom:40%;">

<h4 id="案例-1"><a href="#案例-1" class="headerlink" title="案例"></a>案例</h4><p>如下左图，第一列为论坛号码，第二列为性别，第三列为活跃度，最后一列用户是否流失。</p>
<p>我们要解决一个问题：<strong>性别和活跃度两个特征，哪个对用户流失影响更大</strong>？</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/5.png" style="zoom:50%;">

<p>通过计算信息增益可以解决这个问题，统计上右表信息</p>
<p>其中Positive为正样本（已流失），Negative为负样本（未流失），下面的数值为不同划分下对应的人数。可得到三个熵：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/6.png" style="zoom:45%;">

<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/7.png" style="zoom:45%;">

<p>活跃度的信息增益比性别的信息增益大，也就是说，活跃度对用户流失的影响比性别大。</p>
<p>在做<strong>特征选择</strong>或者<strong>数据分析</strong>的时候，我们应该<strong>重点考察活跃度这个指标</strong>。</p>
<p>从极限角度来理解： 加入某个特征只有一个类别， 那么信息增益为零， 我们会删除这个特征</p>
<p><strong>缺点</strong>：一个特征中的两个特征值， 某个特征值的熵很大， 乘以的是 1/2， 而三个特征值， 某个特征值的熵很大， 乘 1/3， 所以增益就大。比如上述的E(a2)在只有两个的时候，只是1/2，但三个特征的时候1/3更大了，这是不好的，<strong>导致偏向特征值较多的特征</strong></p>
<h3 id="决策树的划分依据二—-信息增益率——C4-5"><a href="#决策树的划分依据二—-信息增益率——C4-5" class="headerlink" title="决策树的划分依据二—-信息增益率——C4.5"></a>决策树的划分依据二—-信息增益率——C4.5</h3><p><strong>增益率：</strong>增益比率度量是用前面的增益度量Gain(S，A)和所分离信息度量SplitInformation(如上例的性别，活跃度等)的比值来共同定义的。</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/8.png" style="zoom:45%;">

<p>这里需要注意，信息增益率对可取值较少的特征有所偏好**(分母越小，整体越大)**</p>
<p><strong>缺点</strong>： <strong>偏向于特征值较小的特征</strong>， HA(D)特征的特征值越多， 它就越大， 特征值越少， 就越小，与上面的刚好相反。</p>
<h3 id="决策树的划分依据三——基尼值和基尼指数"><a href="#决策树的划分依据三——基尼值和基尼指数" class="headerlink" title="决策树的划分依据三——基尼值和基尼指数"></a>决策树的划分依据三——基尼值和基尼指数</h3><p><strong>基尼值Gini（D）：</strong>从数据集D中随机抽取两个样本，其类别标记不一致的概率。故，Gini（D）值越小，数据集D的纯度越高。</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/9.png" style="zoom:45%;">

<h4 id="案例-2"><a href="#案例-2" class="headerlink" title="案例"></a>案例</h4><p>请根据下图列表，按照基尼指数的划分依据，做出决策树。</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/10.png" style="zoom:45%;">

<ol>
<li><p>对数据集非类标号属性{是否有房，婚姻状况，年收入}分别计算它们的Gini系数增益，<strong>取Gini系数增益值最大的属性作为决策树的根节点属性。</strong></p>
</li>
<li><p>根节点的Gini系数为(基尼整体熵)：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/11.png" style="zoom:45%;">
</li>
<li><p>当根据是否有房来进行划分时，Gini系数增益计算过程为(基尼条件熵)：</p>
<p>左子节点：有房的，是否有拖欠贷款</p>
<p>右子节点：没房的，是否有拖欠贷款</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/12.png" style="zoom:45%;">
</li>
<li><p>同理可得年收入Gini：</p>
<p>对于年收入属性为<strong>数值型属性</strong>，首先需要对数据按升序排序，然后从小到大依次用相邻值的中间值作为分隔将样本划分为两组。例如当面对年收入为60和70这两个值时，我们算得其中间值为65。以中间值65作为分割点求出Gini系数增益。</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/13.png" style="zoom:45%;">

</li>
</ol>
<p><strong>最大化增益等价于最小化子女结点的不纯性度量（Gini系数）的加权平均值</strong>，现在我们希望最大化Gini系数的增益。根据计算知道，三个属性划分根节点的增益最大的有两个：年收入属性和婚姻状况，他们的增益都为0.12。此时，选取首先出现的属性作为第一次划分。</p>
<p><strong>一，决策树构建的基本步骤如下</strong>：</p>
<ol>
<li>开始将所有记录看作一个节点</li>
<li>遍历每个变量的每一种分割方式，找到最好的分割点</li>
<li>分割成两个节点N1和N2</li>
<li>对N1和N2分别继续执行2-3步，直到每个节点足够“纯”为止。</li>
</ol>
<p><strong>二，决策树的变量可以有两种</strong>：</p>
<ol>
<li>数字型（Numeric）：变量类型是整数或浮点数，如前面例子中的“年收入”。用“&gt;=”，“&gt;”,“&lt;”或“&lt;=”作为分割条件（排序后，利用已有的分割情况，可以优化分割算法的时间复杂度）。</li>
<li>名称型（Nominal）：类似编程语言中的枚举类型，变量只能从有限的选项中选取，比如前面例子中的“婚姻情况”，只能是“单身”，“已婚”或“离婚”，使用“=”来分割。</li>
</ol>
<p><strong>三，如何评估分割点的好坏？</strong></p>
<p> 如果一个分割点可以将当前的所有节点分为两类，使得每一类都很“纯”，也就是同一类的记录较多，那么就是一个好分割点。</p>
<p> 比如上面的例子，“拥有房产”，可以将记录分成了两类，“是”的节点全部都可以偿还债务，非常“纯”；“否”的节点，可以偿还贷款和无法偿还贷款的人都有，不是很“纯”，但是两个节点加起来的纯度之和与原始节点的纯度之差最大，所以按照这种方法分割。构建决策树采用贪心算法，只考虑当前纯度差最大的情况作为分割点。</p>
<h3 id="代码公式："><a href="#代码公式：" class="headerlink" title="代码公式："></a>代码公式：</h3><h4 id="sklearn-决策树-API-DecisionTreeClassifier"><a href="#sklearn-决策树-API-DecisionTreeClassifier" class="headerlink" title="sklearn 决策树 API DecisionTreeClassifier"></a>sklearn 决策树 API DecisionTreeClassifier</h4><ul>
<li>class sklearn.tree.DecisionTreeClassifier(criterion=’gini’, max_depth=None,random_state=None)<ul>
<li>criterion<ul>
<li>特征选择标准</li>
<li>“gini”或者”entropy”，前者代表基尼系数，后者代表信息增益。一默认”gini”，即CART算法。</li>
</ul>
</li>
<li>min_samples_split<ul>
<li>内部节点再划分所需最小样本数</li>
<li>这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。我之前的一个项目例子，有大概10万样本，建立决策树时，我选择了min_samples_split=10。可以作为参考。</li>
</ul>
</li>
<li>min_samples_leaf<ul>
<li>叶子节点最少样本数</li>
<li>这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。之前的10万样本项目使用min_samples_leaf的值为5，仅供参考。</li>
</ul>
</li>
<li>max_depth<ul>
<li>决策树最大深度</li>
<li>决策树的最大深度，默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间</li>
</ul>
</li>
<li>random_state<ul>
<li>随机数种子</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier, export_graphviz</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用决策树进行预测，修改max_depth试试</span></span><br><span class="line">dec = DecisionTreeClassifier()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">dec.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;预测的准确率： &quot;</span>, dec.score(x_test,y_test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导出决策树的结构</span></span><br><span class="line">export_graphviz(dec, out_file=<span class="string">&quot;tree.dot&quot;</span>,</span><br><span class="line">                feature_names=[<span class="string">&#x27;年龄&#x27;</span>, <span class="string">&#x27;pclass=1st&#x27;</span>, <span class="string">&#x27;pclass=2nd&#x27;</span>, <span class="string">&#x27;pclass=3rd&#x27;</span>, <span class="string">&#x27;女性&#x27;</span>, <span class="string">&#x27;男性&#x27;</span>])</span><br></pre></td></tr></table></figure>


<h3 id="决策树调参"><a href="#决策树调参" class="headerlink" title="决策树调参"></a>决策树调参</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/14.png" style="zoom:30%;">

<p>横轴表示在决策树创建过程中树的结点总数，纵轴表示决策树的预测精度。<br>实线显示的是决策树在训练集上的精度，虚线显示的则是在一个独立的测试集上测量出来的精度。</p>
<p>随着树的增长，在训练样集上的精度是单调上升的， 然而在独立的测试样例上测出的精度先上升后下降。也即是<strong>当决策树很深的时候，就可能出现过拟合的情况</strong></p>
<p>出现这种情况的原因：</p>
<ul>
<li>原因1：噪声、样本冲突，即错误的样本数据。</li>
<li>原因2：特征即属性不能完全作为分类标准。</li>
<li>原因3：巧合的规律性，数据量不够大。</li>
</ul>
<h4 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h4><p>（1） 每一个结点所包含的最小样本数目， 例如 10， 则该结点总样本数小于 10时， 则不再分；<strong>（min_samples_split）</strong><br>（2） 指定树的高度或者深度， 例如树的最大深度为 4；<strong>（max_depth）</strong><br>（3） 指定结点的熵小于某个值， 不再划分。 随着树的增长， 在训练样集上的精度是单调上升的， 然而在独立的测试样例上测出的精度先上升后下降。<strong>对应超参数是 min_impurity_decrease</strong></p>
<h4 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h4><p>后剪枝， 在已生成过拟合决策树上进行剪枝， 可以得到简化版的剪枝决策树。</p>
<h3 id="决策树的优缺点以及改进"><a href="#决策树的优缺点以及改进" class="headerlink" title="决策树的优缺点以及改进"></a>决策树的优缺点以及改进</h3><h4 id="优点："><a href="#优点：" class="headerlink" title="优点："></a><strong>优点：</strong></h4><ul>
<li>简单的理解和解释， <strong>树木可视化</strong>。</li>
<li>需要很少的数据准备， 其他技术通常需要数据归一化， 标准化（<strong>决策树不需要进行归一化和标准化</strong>）</li>
</ul>
<h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a><strong>缺点：</strong></h4><ul>
<li>决策树学习者可以创建不能很好地推广数据的过于复杂的树，这被称为<strong>过拟合</strong>。</li>
<li>决策树<strong>可能不稳定</strong>， 因为数据的小变化可能会导致完全不同的树被生成<strong>（弱分类器）</strong></li>
</ul>
<h4 id="改进："><a href="#改进：" class="headerlink" title="改进："></a><strong>改进：</strong></h4><ul>
<li>减枝 cart(Classification and regression tree)算法—这里我们来看下源码实现还有 png 图</li>
<li><strong>随机森林—解决过拟合</strong></li>
</ul>
<p>过拟合： 我们在训练集可以达到 100%的效果， 但是在测试集只能达到 70%， 80%的效果，<br>称为过拟合</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#调整决策树的参数</span></span><br><span class="line"><span class="comment"># 分割数据集到训练集合测试集</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.25</span>, random_state=<span class="number">4</span>)</span><br><span class="line"><span class="comment"># 进行处理（特征工程）特征-》类别-》one_hot编码</span></span><br><span class="line"><span class="built_in">dict</span> = DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这一步是对字典进行特征抽取</span></span><br><span class="line">x_train = <span class="built_in">dict</span>.fit_transform(x_train.to_dict(orient=<span class="string">&quot;records&quot;</span>))</span><br><span class="line">x_test = <span class="built_in">dict</span>.transform(x_test.to_dict(orient=<span class="string">&quot;records&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(x_train)</span></span><br><span class="line"><span class="comment"># # 用决策树进行预测，修改max_depth为10，发现提升了,min_impurity_decrease带来的增益要大于0.01才会进行划分</span></span><br><span class="line">dec = DecisionTreeClassifier(max_depth=<span class="number">7</span>,min_impurity_decrease=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">dec.fit(x_train, y_train)</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># # 预测准确率</span></span><br><span class="line">print(<span class="string">&quot;预测的准确率：&quot;</span>, dec.score(x_test, y_test))</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># # 导出决策树的结构</span></span><br><span class="line">export_graphviz(dec, out_file=<span class="string">&quot;tree1.dot&quot;</span>,</span><br><span class="line">                feature_names=<span class="built_in">dict</span>.get_feature_names_out())</span><br></pre></td></tr></table></figure>


<h3 id="集成学习方法-随机森林"><a href="#集成学习方法-随机森林" class="headerlink" title="集成学习方法-随机森林"></a>集成学习方法-随机森林</h3><p>集成学习通过建立几个模型来解决单一预测问题。它的工作原理是<strong>生成多个分类器/模型</strong>，各自独立地学习和作出预测。<strong>这些预测最后结合成组合预测，因此优于任何一个单分类的做出预测。</strong></p>
<h4 id="Bagging集成原理"><a href="#Bagging集成原理" class="headerlink" title="Bagging集成原理"></a>Bagging集成原理</h4><p>目标：把下面的圈和方块进行分类</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/15.png" style="zoom:40%;">

<p>实现过程：</p>
<p>1.采样不同数据集</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/16.png" style="zoom:40%;">

<p>2.训练分类器</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/17.png" style="zoom:40%;">

<p>3.平权投票，获取最终结果</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/18.png" style="zoom:40%;">

<h4 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h4><p>定义： 在机器学习中， 随机森林是一个包含多个决策树的分类器， 并且其输出的类别是由个别树输出的类别的众数而定。</p>
<p><strong>随机森林</strong> <strong>= Bagging +</strong> <strong>决策树</strong></p>
<p>例如, 如果你训练了5个树, 其中有4个树的结果是True, 1个树的结果是False, 那么最终投票结果就是True</p>
<p><strong>随机森林够造过程中的关键步骤</strong>（用N来表示训练用例（样本）的个数，M表示特征数目）：</p>
<p> <strong>1）一次随机选出一个样本，有放回的抽样，重复N次（有可能出现重复的样本）</strong></p>
<p> <strong>2） 随机去选出m个特征, m &lt;&lt;M，建立决策树</strong></p>
<ul>
<li><p>1.为什么要随机抽样训练集？　　</p>
<p>如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的</p>
</li>
<li><p>2.为什么要有放回地抽样？</p>
<p>如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是“有偏的”，都是绝对“片面的”（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决。</p>
</li>
</ul>
<h4 id="随机森林api"><a href="#随机森林api" class="headerlink" title="随机森林api"></a>随机森林api</h4><p>sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, bootstrap=True, random_state=None, min_samples_split=2)</p>
<ul>
<li><p>n_estimators：integer，optional（default = 10）森林里的树木数量120,200,300,500,800,1200</p>
</li>
<li><p>Criterion：string，可选（default =“gini”）分割特征的测量方法</p>
</li>
<li><p>max_depth：integer或None，可选（默认=无）树的最大深度 5,8,15,25,30</p>
</li>
<li><p>max_features=”auto”,每个决策树的最大特征数量</p>
<ul>
<li>If “auto”, then <code>max_features=sqrt(n_features)</code>.</li>
<li>If “sqrt”, then <code>max_features=sqrt(n_features)</code>(same as “auto”).</li>
<li>If “log2”, then <code>max_features=log2(n_features)</code>.</li>
<li>If None, then <code>max_features=n_features</code>.</li>
</ul>
</li>
<li><p>bootstrap：boolean，optional（default = True）是否在构建树时使用放回抽样</p>
</li>
<li><p>min_samples_split:节点划分最少样本数</p>
</li>
<li><p>min_samples_leaf:叶子节点的最小样本数</p>
</li>
<li><p>超参数：n_estimator, max_depth, min_samples_split,min_samples_leaf</p>
</li>
</ul>
<h4 id="bagging集成优点"><a href="#bagging集成优点" class="headerlink" title="bagging集成优点"></a>bagging集成优点</h4><p> <strong>Bagging + 决策树/线性回归/逻辑回归/深度学习… = bagging集成学习方法</strong></p>
<p>经过上面方式组成的集成学习方法:</p>
<ol>
<li><strong>均可在原有算法上提高约2%左右的泛化正确率</strong></li>
<li><strong>简单, 方便, 通用</strong></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 随机森林进行预测 （超参数调优），n_jobs充分利用多核的一个参数</span></span><br><span class="line">rf = RandomForestClassifier(n_jobs=-<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 120, 200, 300, 500, 800, 1200,n_estimators森林中决策树的数目，也就是分类器的数目</span></span><br><span class="line"><span class="comment"># max_samples  是最大样本数</span></span><br><span class="line"><span class="comment">#bagging类型</span></span><br><span class="line">param = &#123;<span class="string">&quot;n_estimators&quot;</span>: [<span class="number">1500</span>, <span class="number">2000</span>, <span class="number">5000</span>], <span class="string">&quot;max_depth&quot;</span>: [<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">8</span>,<span class="number">15</span>,<span class="number">25</span>]&#125;</span><br><span class="line"></span><br><span class="line">gc= GridSearchCV(rf, param_grid=param, cv=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">gc.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;准确率：&quot;</span>, gc.score(x_test, y_test))</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;查看选择的参数模型：&quot;</span>, gc.best_params_)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;选择最好的模型是：&quot;</span>, gc.best_estimator_)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;每个超参数每次交叉验证的结果：&quot;</span>, gc.cv_results_)</span><br></pre></td></tr></table></figure>


<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>分类算法</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title>论文解读：GPipe</title>
    <url>/58735.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><a id="more"></a>

<h2 id="背景："><a href="#背景：" class="headerlink" title="背景："></a><strong>背景：</strong></h2><p>增大模型规模通常是提高模型效果的有效方案，但是更多的模型参数，更复杂的模型计算逻辑，导致模型的训练过程需要更多的内存。同时计算设备 (GPU) 的内存资源往往非常非常有限，单台设备不足以支持超大规模模型的训练，而将模型分布在多台设备上则是一种可能的解决方案。</p>
<h2 id="简介："><a href="#简介：" class="headerlink" title="简介："></a><strong>简介：</strong></h2><p>GPipe 使用模型并行方案，将模型切分成一连串 stage，每个 stage 放在独立的设备（GPU/TPU）上，实现对超大规模模型的支持，同时利用 Pipeline 的方案，提高了模型并行模式下的设备利用率。最终 GPipe 通过更大规模的模型和更大的 batch_size，在图像和 NLP 的模型上都得到更好的模型效果。</p>
<h2 id="方案设计"><a href="#方案设计" class="headerlink" title="方案设计"></a><strong>方案设计</strong></h2><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/v2-dea46cace07195b41da36a71cfdf6169_r.jpg"></p>
<h3 id="1-划分-stage"><a href="#1-划分-stage" class="headerlink" title="1. 划分 stage"></a><strong>1. 划分 stage</strong></h3><p>如图 1.(a) 左图所示，将模型划分为连续的几个 stage，每个 stage 各自对应一个设备。这样就使得模型的大小可以突破单个设备内存的大小，因为一台设备只需要能够容纳部分模型的参数和计算。</p>
<h3 id="2-划分-Micro-Batch-和-Pipeline"><a href="#2-划分-Micro-Batch-和-Pipeline" class="headerlink" title="2. 划分 Micro-Batch 和 Pipeline"></a><strong>2. 划分 Micro-Batch 和 Pipeline</strong></h3><p>图 1.(b) 为一般的模型并行的运算模式，在每个时间点只有一台设备在处理计算逻辑，完成计算后将结果发送给下一台设备，其中设备空闲的时间称为 Bubble。GPipe 将 mini-batch 进一步划分成更小的 micro-batch，同时利用 pipipline 方案，每次处理一个 micro-batch 的数据，得到结果后，将该 micro-batch 的结果发送给下游设备，同时开始处理后一个 micro-batch 的数据，通过这套方案减小设备中的 Bubble。</p>
<p>GPipe 实验发现，当增加 micro_batch 数目的时候，设备上的 bubble 可忽略 (一定条件下)，一方面是由于如图 1.(c) 所示的的运行模式导致的，另一方面是由于 Re-Materializaition 方案中，计算梯度过程中的前向计算部分可以不依赖前一部分数据的计算结果 (即在图 1.(c) 中的 Bubble 阶段执行。)</p>
<h3 id="3-Re-Materialization"><a href="#3-Re-Materialization" class="headerlink" title="3. Re-Materialization"></a><strong>3. Re-Materialization</strong></h3><p>基于现有的训练框架，在前向计算时，训练框架会记录每一个算子的计算结果，用于 backward 时的梯度计算。在很多模型中，这部分的内存消耗可能大于模型参数，是限制模型大小（以及 batch-size）的一个重要原因。</p>
<p>Re-Materialization 具体是指在前向计算过程中，GPipe 只记录 stage 划分处的输出，在计算梯度时，GPipe 会重新执行前向计算逻辑，从而得到各个算子的前向结果，然后再计算梯度结果。</p>
<p>Re-Materialization 的好处是大大减少了内存需求，可以有效增大训练的 batch_size，比如正常的训练模式下 (忽略输出所占的内存，这部分一般占比较小)，内存能够支持 mini-batch=16，在 Gpipe 的模式下，可以将 micro-batch 设置为 16, 这样实际的 batch_size 就扩大了 min-batch/micor_batch 倍。</p>
<h2 id="方案效果"><a href="#方案效果" class="headerlink" title="方案效果"></a><strong>方案效果</strong></h2><h3 id="1-更大的模型"><a href="#1-更大的模型" class="headerlink" title="1. 更大的模型"></a><strong>1. 更大的模型</strong></h3><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/v2-a250b74bdb6cefb01ad66396d7d3b89c_r.jpg"></p>
<p>Gpipe 给出的结果中，AmoebaNet-D 模型最大的是~ 25GB，使用了 8 块 GPU,</p>
<p>对于 Transformer-L 模型，在使用 128 块 GPU 的情况下，模型最大达到 937.9GB，使用了 128 块 GPU。</p>
<h3 id="2-更大的-batch-size"><a href="#2-更大的-batch-size" class="headerlink" title="2. 更大的 batch_size"></a><strong>2. 更大的 batch_size</strong></h3><p>Transformer Big 的训练 batch size 从之前的 400k 提高到 4M。</p>
<h3 id="3-更好的模型效果"><a href="#3-更好的模型效果" class="headerlink" title="3. 更好的模型效果"></a><strong>3. 更好的模型效果</strong></h3><p>结合大模型和大 batch 的优势，大部分图像和 NLP 模型效果都得到了比之前更好的结果。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/v2-9b67b5ad569d20766eb810469912c311_r.jpg"><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/v2-572fc0b4f52ac7b0a8a7149bab83e6cc_b.jpg"></p>
<h3 id="4-训练效率"><a href="#4-训练效率" class="headerlink" title="4. 训练效率"></a><strong>4. 训练效率</strong></h3><p>按照 paper 所说，在增加设备个数的情况下，最优能达到接近线性的加速效果。不过没有看到与相同设备下的数据并行方案的对比。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic2.zhimg.com/v2-f2da3e9dd8fdf1cadc88bf5bd8d86d41_b.jpg"><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/v2-f655032c4a5a7cfe663b756c73405c5c_b.jpg"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>这是谷歌出品的一个可扩展的模型并行库，用于训练巨型神经网络。通过批量分隔流水线并行方法，取得了几乎线性加速，并且按照论文表述，可支持各种深度网络，有很高的可靠性（同步梯度下降模式下，无论分区数量多少，都可以保证一致的训练）。</p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>毕设</tag>
      </tags>
  </entry>
  <entry>
    <title>毕业设计-日志</title>
    <url>/45623.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><a id="more"></a>

<h2 id="2021-3-3"><a href="#2021-3-3" class="headerlink" title="2021/3/3"></a>2021/3/3</h2><ul>
<li><p>阅读论文： <strong><em>PipeDream: Fast and Effiffifficient Pipeline Parallel DNN Training</em></strong> ，</p>
<p>​                   <strong><em>GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism</em></strong></p>
</li>
<li><p>学习PyTorch分布式架构的使用</p>
</li>
</ul>
<h2 id="2021-3-2"><a href="#2021-3-2" class="headerlink" title="2021/3/2"></a>2021/3/2</h2><ul>
<li>配置服务器环境，安装anaconda, pytorch, 导入torchgpipe。学习其使用</li>
<li>翻译论文：**<em>torchgpipe: On-the-ﬂy Pipeline Parallelism for Training Giant Models**</em></li>
</ul>
<h2 id="2021-2-2021-2-7"><a href="#2021-2-2021-2-7" class="headerlink" title="2021/2/-2021/2/7"></a>2021/2/-2021/2/7</h2><ul>
<li><p><strong>1.查阅分布式深度学习的相关知识</strong></p>
<p>（在对分布式深度学习有一定了解后，再研读论文，有一定的思路。）</p>
<p>‘’它使用同步随机梯度下降和流水线并行的方式进行训练，适用于任何由多个有序的层组成的深度神经网络(Deep Neural Networks, DNN)。 Gpipe通过跨不同的加速器来分割模型，并自动将一小批训练示例划分成更小的批量。 该模型允许GPipe的加速器进行并行操作，最大限度地提高了训练过程中的可扩展性。‘’层序</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/45623/Users\HP\AppData\Roaming\Typora\typora-user-images\image-20210115040032282.png" alt="image-20210115040032282"></p>
<p><strong>2.阅读论文–torchgpipe: On-the-flfly Pipeline Parallelism for Training Giant Models</strong></p>
<p>通过设置参数将一个计算划分为相同的几个小批次，实现并行。通过超参数调优，有效的将训练时间减少到一定大小的批次。</p>
<p>文中解决的问题：</p>
<p>每个分布式机器做完自己的任务，需要更新参数，用于同步网络参数。大量的参数需要同步时，可能会导致大量的通信负载</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/45623/Users\HP\AppData\Roaming\Typora\typora-user-images\image-20210115041557350.png" alt="image-20210115041557350"></p>
<p>1：</p>
<p>​    不同任务所占据的CPU时间不定，可长可短，解决方法：引入确定性时间周期，给出任务总排序</p>
<p>2：</p>
<p>​    由于pytorch的特点是，需要时调用，用完即弃，所以不保存前向图逻辑，只保留计算梯度等参数。所以反馈时不一定会顺序正确，所以需要引入fork，join原语标注顺序</p>
<p>3：</p>
<p>​    出现双向同步的问题，导致使用不足，出现双方相互等待。所以需要使用非默认C UDA流来避免这个问题，这样副本就不会阻止计算，除非计算必须等待数据。</p>
<p>4：</p>
<p>​    考虑有些任务会跳过一些批次，所以放宽顺序执行，增加设计一个接口来跳过中间张量。</p>
</li>
</ul>
<p>  <strong>提出的问题</strong></p>
<p>  ​    不同批次的大小该如何划分以达到最优？</p>
<p>  <strong>近期计划</strong></p>
<p>  1.开始在pytorch上实践torchpipe，尽早发现问题，解决问题。前期工作主要是实现torchpipe的思想，后期再进一步优化。</p>
<p>  2.继续研读相关论文，正确理解其含义，误解的开始</p>
<p>  介于假期不能线下开会，计划每两周开一次小会，将有关问题汇总和研究生学长讨论。</p>
]]></content>
      <categories>
        <category>日志</category>
      </categories>
      <tags>
        <tag>毕设</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-9-——线性回归</title>
    <url>/6552.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><h3 id="定义与公式"><a href="#定义与公式" class="headerlink" title="定义与公式"></a>定义与公式</h3><p><strong>线性回归(Linear regression)**是利用</strong>回归方程(函数)<strong>对</strong>一个或多个自变量(特征值)和因变量(目标值)之间**关系进行建模的一种分析方式。</p>
<ul>
<li>特点：只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归</li>
</ul>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/1.png" style="zoom:40%;">

<ul>
<li>线性回归用矩阵表示举例</li>
</ul>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/2.png" style="zoom:40%;">

<p>那么怎么理解呢？我们来看几个例子</p>
<ul>
<li>期末成绩：0.7×考试成绩+0.3×平时成绩</li>
<li>房子价格 = 0.02×中心区域的距离 + 0.04×城市一氧化氮浓度 + (-0.12×自住房平均房价) + 0.254×城镇犯罪率</li>
</ul>
<p>上面两个例子，<strong>我们看到特征值与目标值之间建立了一个关系，这个关系可以理解为线性模型</strong>。</p>
<h3 id="线性回归的损失和优化"><a href="#线性回归的损失和优化" class="headerlink" title="线性回归的损失和优化"></a>线性回归的损失和优化</h3><p><strong>假设刚才的房子例子，真实的数据之间存在这样的关系</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">真实关系：真实房子价格 = <span class="number">0.02</span>×中心区域的距离 + <span class="number">0.04</span>×城市一氧化氮浓度 + (-<span class="number">0.12</span>×自住房平均房价) + <span class="number">0.254</span>×城镇犯罪率</span><br></pre></td></tr></table></figure>
<p>那么现在呢，我们随意指定一个关系（猜测）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">随机指定关系：预测房子价格 = <span class="number">0.25</span>×中心区域的距离 + <span class="number">0.14</span>×城市一氧化氮浓度 + <span class="number">0.42</span>×自住房平均房价 + <span class="number">0.34</span>×城镇犯罪率</span><br></pre></td></tr></table></figure>
<p>请问这样的话，会发生什么？真实结果与我们预测的结果之间是不是存在一定的误差呢？类似这样样子</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/3.png" style="zoom:40%;">

<p>既然存在这个误差，那我们就将这个误差给衡量出来</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>总损失定义为：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/4.png" style="zoom:50%;">

<ul>
<li>yi为第i个训练样本的真实值</li>
<li>h(xi)为第i个训练样本特征值组合预测函数</li>
<li>又称最小二乘法</li>
</ul>
<p>如何去减少这个损失，使我们预测的更加准确些？既然存在了这个损失，我们一直说机器学习有自动学习的功能，在线性回归这里更是能够体现。这里可以通过一些优化方法去优化（其实是数学当中的求导功能）回归的总损失！！！<strong>目的是找到最小损失对应的W值，这是重点</strong></p>
<h3 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h3><p><strong>如何去求模型当中的W，使得损失最小？（目的是找到最小损失对应的W值）</strong></p>
<p>线性回归经常使用的两种优化算法：即<strong>正规方程</strong>和<strong>梯度下降</strong></p>
<h3 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/6.png" style="zoom:50%;">

<p><strong>推导过程：</strong></p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/5.png" style="zoom:50%;">

<ul>
<li><strong>推导方式二【拓展】：</strong></li>
</ul>
<p><a href="https://www.jianshu.com/p/2b6633bd4d47">https://www.jianshu.com/p/2b6633bd4d47</a></p>
<h4 id="正规方程接口api："><a href="#正规方程接口api：" class="headerlink" title="正规方程接口api："></a>正规方程接口api：</h4><ul>
<li>sklearn.linear_model.LinearRegression(fit_intercept=True)<ul>
<li>通过正规方程优化</li>
<li>fit_intercept：是否计算偏置</li>
<li>LinearRegression.coef_：回归系数</li>
<li>LinearRegression.intercept_：偏置</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error,classification_report,roc_auc_score</span><br><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">线性回归直接预测房子价格</span></span><br><span class="line"><span class="string">:return: None</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 获取数据</span></span><br><span class="line">lb = fetch_california_housing(data_home=<span class="string">&#x27;data&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分割数据集到训练集和测试集</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(lb.data, lb.target, test_size=<span class="number">0.25</span>, random_state=<span class="number">1</span>)</span><br><span class="line"><span class="comment">#</span></span><br><span class="line">print(x_train.shape)</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># # 进行标准化处理(?) 目标值处理？</span></span><br><span class="line"><span class="comment"># # 特征值和目标值是都必须进行标准化处理, 实例化两个标准化API</span></span><br><span class="line">std_x = StandardScaler()</span><br><span class="line"></span><br><span class="line">x_train = std_x.fit_transform(x_train)</span><br><span class="line">x_test = std_x.transform(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 目标值进行了标准化</span></span><br><span class="line">std_y = StandardScaler()</span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># temp = y_train.reshape(-1, 1) #-1代表把剩余的元素都堆到哪一维</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># #标签进行标准化</span></span><br><span class="line"><span class="comment"># # 目标值是一维的，这里需要传进去2维的</span></span><br><span class="line">y_train = std_y.fit_transform(y_train.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># print(y_train.shape)</span></span><br><span class="line">y_test = std_y.transform(y_test.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># print(y_test.shape)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # estimator预测</span></span><br><span class="line"><span class="comment"># # # 正规方程求解方式预测结果，正规方程进行线性回归</span></span><br><span class="line">lr = LinearRegression()</span><br><span class="line"><span class="comment"># fit是耗时的</span></span><br><span class="line">lr.fit(x_train, y_train)</span><br><span class="line"><span class="comment">#回归系数可以看特征与目标之间的相关性</span></span><br><span class="line">print(<span class="string">&#x27;回归系数&#x27;</span>, lr.coef_)</span><br><span class="line"></span><br><span class="line">y_predict = lr.predict(x_test)</span><br><span class="line"><span class="comment"># 预测测试集的房子价格，通过inverse得到真正的房子价格</span></span><br><span class="line">y_lr_predict = std_y.inverse_transform(y_predict)</span><br><span class="line"><span class="comment"># 保存训练好的模型，模型中保存的是w的值，也保存了模型结构</span></span><br><span class="line"><span class="comment">#保存模型放在fit之后即可</span></span><br><span class="line"></span><br><span class="line">joblib.dump(lr, <span class="string">&quot;./tmp/test.pkl&quot;</span>)</span><br><span class="line">print(<span class="string">&quot;正规方程测试集里面每个房子的预测价格：&quot;</span>, y_predict[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"><span class="comment">#下面是求测试集的损失</span></span><br><span class="line">print(<span class="string">&quot;正规方程的均方误差：&quot;</span>, mean_squared_error(y_test, y_predict))</span><br></pre></td></tr></table></figure>


<h3 id="梯度下降-Gradient-Descent"><a href="#梯度下降-Gradient-Descent" class="headerlink" title="梯度下降(Gradient Descent)"></a>梯度下降(Gradient Descent)</h3><p>梯度下降法的基本思想可以类比为一个下山的过程。</p>
<p>假设这样一个场景：一个人被困在山上，需要从山上下来(i.e. 找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用梯度下降算法来帮助自己下山。具体来说就是，以他当前的所处的位置为基准，<strong>寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走</strong>，（同理，如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝着最陡峭的方向往上走）。然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷。</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/7.png" style="zoom:30%;">

<p>梯度下降的基本过程就和下山的场景很类似。</p>
<p>首先，我们有一个<strong>可微分的函数</strong>。这个函数就代表着一座山。</p>
<p>我们的目标就是找到<strong>这个函数的最小值</strong>，也就是山底。</p>
<p>根据之前的场景假设，最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到函数中，就是<strong>找到给定点的梯度</strong> ，然后朝着梯度相反的方向，就能让函数值下降的最快！因为梯度的方向就是函数之变化最快的方向。 所以，我们重复利用这个方法，反复求取梯度，最后就能到达局部的最小值，这就类似于我们下山的过程。而求取梯度就确定了最陡峭的方向，也就是场景中测量方向的手段。</p>
<h4 id="梯度下降（Gradient-Descent）公式"><a href="#梯度下降（Gradient-Descent）公式" class="headerlink" title="梯度下降（Gradient Descent）公式"></a>梯度下降<strong>（</strong>Gradient Descent）公式</h4><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/8.png" style="zoom:50%;">

<ul>
<li>α在梯度下降算法中被称作为<strong>学习率</strong>或者<strong>步长</strong>，意味着我们可以通过α来控制每一步走的距离，以保证不要步子跨的太大扯着蛋，哈哈，其实就是不要走太快，错过了最低点。同时也要保证不要走的太慢，导致太阳下山了，还没有走到山下。所以α的选择在梯度下降法中往往是很重要的！α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点！</li>
<li>梯度前加一个负号，就意味着朝着梯度相反的方向前进！我们在前文提到，梯度的方向实际就是函数在此点上升最快的方向！而我们需要朝着下降最快的方向走，自然就是负的梯度的方向，所以此处需要加上负号</li>
</ul>
<h4 id="梯度下降接口api："><a href="#梯度下降接口api：" class="headerlink" title="梯度下降接口api："></a>梯度下降接口api：</h4><ul>
<li>sklearn.linear_model.SGDRegressor(loss=”squared_loss”, fit_intercept=True, learning_rate =’invscaling’, eta0=0.01)<ul>
<li>SGDRegressor类实现了随机梯度下降学习，它支持不同的<strong>loss函数和正则化惩罚项</strong>来拟合线性回归模型。</li>
<li>loss:损失类型<ul>
<li><strong>loss=”squared_loss”: 普通最小二乘法</strong></li>
</ul>
</li>
<li>fit_intercept：是否计算偏置</li>
<li>learning_rate : string, optional<ul>
<li>学习率填充</li>
<li><strong>‘constant’: eta = eta0</strong></li>
<li><strong>‘optimal’: eta = 1.0 / (alpha * (t + t0)) [default]</strong></li>
<li>‘invscaling’: eta = eta0 / pow(t, power_t)<ul>
<li><strong>power_t=0.25:存在父类当中</strong></li>
</ul>
</li>
<li><strong>对于一个常数值的学习率来说，可以使用learning_rate=’constant’ ，并使用eta0来指定学习率。</strong></li>
</ul>
</li>
<li>SGDRegressor.coef_：回归系数</li>
<li>SGDRegressor.intercept_：偏置</li>
</ul>
</li>
</ul>
<h3 id="梯度下降和正规方程的对比"><a href="#梯度下降和正规方程的对比" class="headerlink" title="梯度下降和正规方程的对比"></a>梯度下降和正规方程的对比</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/9.png" style="zoom:50%;">

<p>选择：</p>
<ul>
<li>小规模数据：<ul>
<li><strong>LinearRegression(不能解决拟合问题)</strong></li>
<li>岭回归</li>
</ul>
</li>
<li>大规模数据：SGDRegressor</li>
</ul>
<h3 id="回归性能评估"><a href="#回归性能评估" class="headerlink" title="回归性能评估"></a>回归性能评估</h3><p>均方误差(Mean Squared Error)MSE)评价机制：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/10.png" style="zoom:40%;">

<p><strong>注：yi为预测值，¯y为真实值</strong></p>
<ul>
<li>sklearn.metrics.mean_squared_error(y_true, y_pred)<ul>
<li>均方误差回归损失</li>
<li>y_true:真实值</li>
<li>y_pred:预测值</li>
<li>return:浮点数结果</li>
</ul>
</li>
</ul>
<h2 id="正则化线性模型"><a href="#正则化线性模型" class="headerlink" title="正则化线性模型"></a>正则化线性模型</h2><h2 id="Ridge-Regression-岭回归，又名-Tikhonov-regularization"><a href="#Ridge-Regression-岭回归，又名-Tikhonov-regularization" class="headerlink" title="Ridge Regression (岭回归，又名 Tikhonov regularization)"></a>Ridge Regression (岭回归，又名 Tikhonov regularization)</h2><p>岭回归是线性回归的正则化版本，即在原来的线性回归的 cost function 中添加正则项（regularization term）:</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/11.png" style="zoom:50%;">

<h3 id="岭回归接口api："><a href="#岭回归接口api：" class="headerlink" title="岭回归接口api："></a>岭回归接口api：</h3><ul>
<li>sklearn.linear_model.Ridge(alpha=1.0, fit_intercept=True,solver=”auto”, normalize=False)<ul>
<li>具有l2正则化的线性回归</li>
<li>alpha:正则化力度，也叫 λ<ul>
<li><strong>λ取值：0<del>1 1</del>10</strong></li>
</ul>
</li>
<li>solver:会根据数据自动选择优化方法<ul>
<li><strong>sag:如果数据集、特征都比较大，选择该随机梯度下降优化</strong></li>
</ul>
</li>
<li>normalize:数据是否进行标准化<ul>
<li>normalize=False:可以在fit之前调用preprocessing.StandardScaler标准化数据</li>
</ul>
</li>
<li>Ridge.coef_:回归权重</li>
<li>Ridge.intercept_:回归偏置</li>
</ul>
</li>
</ul>
<p><strong>Ridge方法相当于SGDRegressor(penalty=’l2’, loss=”squared_loss”),只不过SGDRegressor实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG)</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># # # 岭回归去进行房价预测</span></span><br><span class="line"><span class="comment">#岭回归是对线性回归加入L2正则化，L2正则化是对系数的平方和进行惩罚</span></span><br><span class="line"><span class="comment">#alpha就是补偿的系数</span></span><br><span class="line"><span class="comment">#正规方程求解，加补偿就可以让正规方程可逆</span></span><br><span class="line">rd = Ridge(alpha=<span class="number">0.02</span>)</span><br><span class="line"></span><br><span class="line">rd.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">print(rd.coef_)</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># # 预测测试集的房子价格</span></span><br><span class="line">print(rd.predict(x_test).shape)</span><br><span class="line"><span class="comment"># y_rd_predict = std_y.inverse_transform(rd.predict(x_test))</span></span><br><span class="line">y_predict = rd.predict(x_test)</span><br><span class="line"><span class="comment"># print(&quot;岭回归里面每个房子的预测价格：&quot;, y_rd_predict)</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;岭回归的均方误差：&quot;</span>, mean_squared_error(y_test, y_predict))</span><br><span class="line"><span class="comment"># print(&quot;岭回归的均方误差：&quot;, mean_squared_error(std_y.inverse_transform(y_test), y_rd_predict))</span></span><br></pre></td></tr></table></figure>
<h3 id="观察正则化程度的变化，对结果的影响？"><a href="#观察正则化程度的变化，对结果的影响？" class="headerlink" title="观察正则化程度的变化，对结果的影响？"></a>观察正则化程度的变化，对结果的影响？</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/14.png" style="zoom:45%;">

<ul>
<li><p>正则化力度越大，权重系数会越小</p>
</li>
<li><p>正则化力度越小，权重系数会越大</p>
</li>
</ul>
<h2 id="Lasso-Regression-Lasso-回归"><a href="#Lasso-Regression-Lasso-回归" class="headerlink" title="Lasso Regression(Lasso 回归)"></a>Lasso Regression(Lasso 回归)</h2><p>Lasso 回归是线性回归的另一种正则化版本，正则项为权值向量的 L1 范数。</p>
<p>Lasso回归的代价函数 ：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/12.png" style="zoom:60%;">

<p>【注意 】</p>
<ul>
<li>Lasso Regression 的代价函数在 θi=0处是不可导的.</li>
<li>解决方法：在θi=0处用一个次梯度向量(subgradient vector)代替梯度，如下式</li>
<li>Lasso Regression 的次梯度向量</li>
</ul>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/6552/13.png" style="zoom:45%;">

<p>Lasso Regression 有一个很重要的性质是：倾向于完全消除不重要的权重。</p>
<p>例如：当α 取值相对较大时，高阶多项式退化为二次甚至是线性：高阶多项式特征的权重被置为0。</p>
<p>也就是说，Lasso Regression 能够自动进行特征选择，并输出一个稀疏模型（只有少数特征的权重是非零的）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># # # Lasso回归去进行房价预测</span></span><br><span class="line"><span class="comment">#alpha就是补偿的系数</span></span><br><span class="line">print(x_train.shape)</span><br><span class="line">print(y_train.shape)</span><br><span class="line">ls = Lasso(alpha=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">ls.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line">print(ls.coef_)</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># # 预测测试集的房子价格</span></span><br><span class="line">print(ls.predict(x_test).shape)</span><br><span class="line">print(<span class="string">&#x27;-&#x27;</span>*<span class="number">50</span>)</span><br><span class="line"><span class="comment"># y_ls_predict = std_y.inverse_transform(ls.predict(x_test).reshape(-1,1))</span></span><br><span class="line">y_predict = ls.predict(x_test)</span><br><span class="line"><span class="comment"># print(&quot;Lasso回归里面每个房子的预测价格：&quot;, y_rd_predict)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">print(<span class="string">&quot;Lasso回归的均方误差：&quot;</span>, mean_squared_error(y_test, y_predict))</span><br><span class="line"><span class="comment"># print(&quot;Lasso回归的均方误差：&quot;, mean_squared_error(std_y.inverse_transform(y_test), y_ls_predict))</span></span><br></pre></td></tr></table></figure>


<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>回归算法</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title>论文解读：MuCGEC</title>
    <url>/42505.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="MuCGEC-a-Multi-Reference-Multi-Source-Evaluation-Dataset-for-Chinese-Grammatical-Error-Correction"><a href="#MuCGEC-a-Multi-Reference-Multi-Source-Evaluation-Dataset-for-Chinese-Grammatical-Error-Correction" class="headerlink" title="MuCGEC- a Multi-Reference Multi-Source Evaluation Dataset for Chinese Grammatical Error Correction"></a>MuCGEC- a Multi-Reference Multi-Source Evaluation Dataset for Chinese Grammatical Error Correction</h2><hr>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><ul>
<li>用于中文语法纠错（CGEC）的多参考多源评估数据集,包含收录自CSL中的7063个句子。其中每个句子由三名annotator更正，他们的更正由senior annotator仔细审查，每个句子产生 2.3 条参考。</li>
<li>使用两个主流CGEC模型进行实验，并用一个大规模预训练模型进行提升，在数据集上达到了有竞争力的基准表现</li>
<li>使用基于字符的指标。</li>
</ul>
<h3 id="1-introduction"><a href="#1-introduction" class="headerlink" title="1. introduction"></a>1. introduction</h3><ul>
<li>GEC：表示对有可能有语法错误的输入文字句子（生句子）进行检测以及纠正所有的错误并且生成正确干净的句子。</li>
<li>英文数据集(EGEC)有着丰富的高质量人工标注评估数据，但中文数据集(CGEC)相对欠缺。主流的CGEC是NLPCC18和CGED。</li>
<li>现存的CGEC缺少多参考的数据，大部分来自上述两个数据集，这可能是由于采用了不同的标注工作流。</li>
</ul>
<p>MuCGEC：解决下述的两个问题：单参考问题和单文本源问题</p>
<ul>
<li><p><strong>多参考的标注</strong>对整个<strong>GEC模型以及GEC数据标注</strong>都很重，因为对于一个不正确的句子，通常存在不止一个具有相似含义的可接受引用。</p>
</li>
<li><ol>
<li>如果 GEC 模型输出正确的参考，但与评估数据中给出的参考不同，那么模型性能将被不公平地低估 –&gt;增加引用数量</li>
<li>施加单引用约束会使数据标注出现问题，比如annotator提交不同的但同样是可以接受的修正，senior不知道如何决定哪个更好</li>
</ol>
<p>（认为是一句话的歧义问题以及更改在单参考的环境下会出现错误评估以及评估失效）</p>
</li>
<li><p>现有的 CGEC 数据集还从单一文本源收集句子，这可能不足以进行稳健的模型评估</p>
</li>
</ul>
<p>总结：基于基于浏览器的在线标注工具，每个句子分配给三名标注员进行独立批改，并分配给一名高级标注员进行终审。 标注者可以提交多个引用，高级标注者除了拒绝不正确的提交外，还可以补充新的引用。 通过这种方式，我们的目标是产生尽可能多的参考资料。 总之，这项工作做出了以下贡献。</p>
<p>1）我们新构建的 MuCGEC 由 7,063 个句子组成，这些句子来自CSL文本的三个代表性来源。 每个句子平均获得 2.3 个参考。 我们对新数据集进行详细分析以获得更多见解。</p>
<p>2）我们使用Seq2Edit和Seq2Seq模型进行基准实验，这两种模型都通过预训练语言模型 (PLM) 进行了增强，我们还尝试了一种非常有效的集成策略。使用比基于单词的评估指标更简单的基于字符的评估指标。</p>
<h3 id="2-Data-Annotation"><a href="#2-Data-Annotation" class="headerlink" title="2. Data Annotation"></a>2. Data Annotation</h3><h4 id="2-1-Multi-Source-Data-Selection"><a href="#2-1-Multi-Source-Data-Selection" class="headerlink" title="2.1 Multi-Source Data Selection"></a>2.1 Multi-Source Data Selection</h4><p>在CSL学习者文本上调查各种各样的中文语法错误。</p>
<ul>
<li>数据源：<ol>
<li>重新标注NLPCC18(包含北大语料库)</li>
<li>重新标注CGED-2018 和 CGED-2020 测试数据集(来自HSK的写作章节)，从总共 5,006 个句子中删除标记为正确的句子后，我们获得了 3,137 个可能错误的句子用于重新标注。</li>
<li>NLPCC2018 共享任务组织者从 Lang8 收集了约 717,000 个中文句子及其更正。我们随机选择 2,000 个 30 到 60 个字符的句子进行重新标注。</li>
</ol>
</li>
</ul>
<p>放弃所有原始更正，并按照我们新的标注指南和工作流程直接从头开始重新标注。</p>
<h4 id="2-2-Annotation-Paradigm-Direct-Rewriting"><a href="#2-2-Annotation-Paradigm-Direct-Rewriting" class="headerlink" title="2.2 Annotation Paradigm: Direct Rewriting"></a>2.2 Annotation Paradigm: Direct Rewriting</h4><p>构建 GEC 数据的标注范式主要有两种类型，即<strong>错误编码</strong>和<strong>直接重写</strong>。</p>
<ol>
<li>错误编码范式：要求标注者明确标记原始句子中的错误跨度，然后选择其错误类型，最后进行更正。(单词纠正)</li>
</ol>
<p>存在的问题：</p>
<ul>
<li>不同的标注者很难就错误跨度的边界及其错误类型达成一致，尤其是当有许多类别需要考虑时-&gt;导致标注工作量的增加和 标注质量下降</li>
<li>在复杂的标注范式下，标注者忽视结果引用的流畅性，有时甚至会导致表达不自然。</li>
</ul>
<ol start="2">
<li>直接重写范式：要求标注者直接重写输入句子并生成相应的语法正确的句子，而不改变原始含义。这种标注范式已被证明是高效且具有成本效益的（全句重写）</li>
</ol>
<p>我们采用直接重写范式。 除了上述优点外，我们相信这种范式可以帮助提高引用的多样性，因为标注者可以更自由地纠正错误。</p>
<h4 id="2-3-Annotation-Guidelines"><a href="#2-3-Annotation-Guidelines" class="headerlink" title="2.3 Annotation Guidelines"></a>2.3 Annotation Guidelines</h4><p>编写了 30 页的 CGEC 标注综合指南，并根据反馈逐步完善。指南采用了两层层次的错误分类法，包括5种主要错误类型和14种次要错误类型</p>
<ul>
<li>标点：丢失; 冗余; 滥用</li>
<li>拼写：拼音混乱； 字形混乱； 语无伦次</li>
<li>单词：丢失; 冗余; 滥用</li>
<li>句法：词序；混合语法模式</li>
<li>语用学：逻辑不一致； 歧义； 常识性错误</li>
</ul>
<p><strong>处理word-missing errors遇到的问题</strong>：以往的CGEC 数据集会直接插入特定词，但漏词本身是与上下文有关的。导致语义错误，能会给 GEC 模型评估带来麻烦，因为还有很多其他可接受的候选词。-&gt; 使用missing components (MC)标签标注，因为“[MC]”不包含在现有的训练数据和词汇表中。 我们将这个问题留作未来的工作。</p>
<h4 id="2-4-Annotation-Workflow-and-Tool"><a href="#2-4-Annotation-Workflow-and-Tool" class="headerlink" title="2.4 Annotation Workflow and Tool"></a>2.4 Annotation Workflow and Tool</h4><ul>
<li><p>将每个句子分配给三个随机标注者进行独立标注。 然后将他们提交的内容汇总并发送给随机的高级标注者（审阅者）进行审阅。</p>
</li>
<li><p>高级标注者的工作包括： 1）将不正确的引用修改为正确的引用（有时只是拒绝它们）； 2）根据指南添加新的正确引用。</p>
</li>
<li><p>评审后，被接受的引用被定义为<strong>最终黄金饮用</strong></p>
</li>
<li><p>采用一种自学机制，允许标注者在提交不正确的引用时从错误中吸取教训。：简而言之，就是标注者提交的引用不是黄金引用，则可以对其进行修改成一个正确的引用。另加上投诉机制，对自学机制进行优化。我们发现自学和投诉机制可以引发非常有益的讨论。</p>
</li>
</ul>
<h4 id="2-5-Annotation-Process"><a href="#2-5-Annotation-Process" class="headerlink" title="2.5 Annotation Process"></a>2.5 Annotation Process</h4><p>聘请了21名母语为汉语、熟悉汉语语法的本科生作为兼职标注员。所有参与者都被要求每天至少标注 1 小时。 整个标注过程历时约3个月。</p>
<h4 id="2-6-Ethical-Issues"><a href="#2-6-Ethical-Issues" class="headerlink" title="2.6 Ethical Issues"></a>2.6 Ethical Issues</h4><p>这三个来源的所有数据都是公开的。 同时，我们已获得 NLPCC-2018 和 CGED 共享任务的组织者的许可</p>
<h3 id="3-Analysis-of-Our-Annotated-Data"><a href="#3-Analysis-of-Our-Annotated-Data" class="headerlink" title="3 Analysis of Our Annotated Data"></a>3 Analysis of Our Annotated Data</h3><ol>
<li>与原标注相比，新标注中的错句比例有一定减少，体现了我们对<strong>过度修正现象</strong>的严格控制。</li>
<li>关于句子长度，NLPCC18 的句子最短，而 CGED 的句子要长得多。因为考生喜欢写长句子…</li>
<li>重新标注的 NLPCC18 中的每个句子平均收到 2.5 个引用，这是原始 NLPCC18 数据中的两倍多。可以使我们的数据集在评估时更可靠</li>
<li>比较不同数据集中每个引用的基于字符的编辑数量。我们可以看到编辑次数与句子长度密切相关。平均句子长度和编辑次数的差异表明三个数据源在质量和难度上可能存在系统性差异，我们认为这有助于评估模型的泛化能力。</li>
<li>标注者更倾向于提供单一引用，因为更经济，他们觉得应该让更多标注着提供他们的最佳饮用。</li>
<li>我们通过根据最终的黄金参考评估所有标注提交来计算基于字符的 F0.5 分数。平均 F0.5 为 72.12。</li>
<li>最佳标注者获得 82.34 F0.5 分数，而完成最多任务的标注者仅获得 68.32 分。 这表明我们在计算工资时应该更加关注标注质量，避免标注者过于关注标注速度。</li>
<li>标注者常犯的错误：最常见的错误是由于更正不完整造成的，占无效引用的 56.7%。很难无遗漏地纠正所有错误，第二类错误是错误更正，即旧错误的更正导致新错误，此外，有 11.0% 的投稿因意思改变而被拒绝。</li>
</ol>
<h3 id="4-Benchmark-Models"><a href="#4-Benchmark-Models" class="headerlink" title="4 Benchmark Models"></a>4 Benchmark Models</h3><p>为了了解前沿的 GEC 模型在我们的数据上的表现如何，我们采用了两种主流的 GEC 方法，即 Seq2Edit 和 Seq2Seq。</p>
<ul>
<li><p><strong>Seq2Edit 模型</strong>将 GEC 视为序列标记任务，并通过一系列令牌级别的编辑（包括插入、删除和替换）执行错误更正（Malmi 等人，2019）。 一个token在英文中对应一个词或子词，在中文中对应一个字。 我们采用 GECToR（Omelianchuk 等人，2020 年）进行微小修改以适应中文，从而在 EGEC 数据集上实现了最好的表现(SOTA)。</p>
</li>
<li><p>使用StructBERT （流行的PLM，因为它在微调后具有优越的性能）作为encoder以提升GECToR</p>
</li>
<li><p><strong>Seq2Seq 模型</strong>直接将 GEC 视为单语翻译任务。 最近的工作建议使用 T5 或 BART 等 PLM 来增强基于 Transformer 的 Seq2Seq EGEC 模型。 与 BERT不同，T5 和 BART 是专门为文本生成而设计的。 因此，继续使用 GEC 数据对它们进行培训很简单。 我们遵循这些工作并利用 Shao 等人最近提出的中文 BART。初始化我们的 Seq2Seq 模型。</p>
</li>
<li><p><strong>集成模型</strong>。 之前的几项工作已经证明了模型集成对 CGEC 的有效性。 在这项工作中，我们清楚地观察到上述两个模型在修复不同错误类型方面的互补能力，因此尝试将它们结合起来。<strong>Seq2Seq 模型 善于处理 词序 错误 ，而Seq2Edit 模型 在 冗余错误上表现较好</strong></p>
</li>
<li><p>我们用两种集成设定进行实验：</p>
<p>1）一个 Seq2Edit 和一个 Seq2Seq，用“1×Seq2Edit+1×Seq2Seq”表示</p>
<p>2）三个 Seq2Edit 和三个 Seq2Seq，用“3×Seq2Edit+3×Seq2Seq”表示。</p>
<p>这三个 Seq2Edit模型是使用不同的随机种子进行初始化得到的，Seq2Seq也是如此。</p>
</li>
</ul>
<ul>
<li><strong>其他设置</strong>。 为了获得两种模型的单模型性能，我们使用不同的随机种子分别运行它们三次进行初始化并计算平均指标。 对于“1×Seq2Edit+1×Seq2Seq”，我们随机选择一对单一模型。 对于“3×Seq2Edit+3×Seq2Seq”，我们汇总了所有六个单一模型的结果。</li>
</ul>
<h3 id="5-Experiments-on-NLPCC18-Orig-Data"><a href="#5-Experiments-on-NLPCC18-Orig-Data" class="headerlink" title="5 Experiments on NLPCC18-Orig Data"></a>5 Experiments on NLPCC18-Orig Data</h3><p>训练数据：将训练数据严格限制为公共资源，即 Lang8 数据 和 HSK 数据，<strong>按照 Junczys-Dowmunt 等人 (2018) 的重新加权程序，我们将 HSK 数据复制五次，并将它们与 Lang8 数据合并。</strong> 问题？？</p>
<ul>
<li>当仅使用 Lang8 进行训练时，我们的单个 Seq2Seq 模型已经具有相当的竞争力。 它的性能在 F0.5 中仅比 MaskGEC (Zhao and Wang, 2020) 低 1 个百分点。</li>
<li>添加 HSK 训练数据可将我们所有模型的性能提高约 4 个百分点。 我们的两个基准模型已经在单模型设置下实现了 SOTA 性能。</li>
<li>与单个模型相比，模型集成技术带来了明显的性能提升（超过 5 分）。 然而，增加组件模型数量的收益似乎相当小。</li>
</ul>
<h3 id="6-Experiments-on-MuCGEC"><a href="#6-Experiments-on-MuCGEC" class="headerlink" title="6 Experiments on MuCGEC"></a>6 Experiments on MuCGEC</h3><h4 id="6-1-Data-Splits"><a href="#6-1-Data-Splits" class="headerlink" title="6.1 Data Splits"></a>6.1 Data Splits</h4><p>在这项工作中，我们建议通过从 CGED 源中随机选择 1,125 个句子，为我们新标注的数据集提供一个固定的开发集，表示为 CGED-dev。 剩下的5938个句子作为测试集，其中每个数据源的句子数量大致相等，即NLPCC18-test 1996个句子，CGED-test 2000个句子，Lang8-test 1942个句子。（以往是随机选择）</p>
<h4 id="6-2-Evaluation-Metrics"><a href="#6-2-Evaluation-Metrics" class="headerlink" title="6.2 Evaluation Metrics"></a>6.2 Evaluation Metrics</h4><ul>
<li><strong>基于单词的指标存在问题：</strong>以前的 CGEC 数据集是根据单词序列进行标注的，并采用基于单词的度量标准进行评估。 在标注和评估之前，需要使用中文分词 (CWS) 模型将句子分割成单词。因为1）CWS 模型不可避免地会产生分词错误。 2）存在多种异构的CWS标准。3）我们发现正确的编辑可能会因为词边界不匹配而被判断为错误的。</li>
</ul>
<p>这项工作改为采用基于字符的跨度级别评估指标。 首先，给定一个输入句子和一个修正，我们获得了具有最小编辑距离的最佳字符级编辑序列。 我们考虑三种类型的字符级编辑，对应三种错误类型：<br>• 为冗余错误，删除一个字符；<br>• 为缺失的错误，插入一个字符；<br>• 为替换错误，将一个字符替换为另一个；</p>
<p>然后，我们按照之前在 EGEC 和 CGEC 中的做法，通过合并相同类型的连续编辑，将所有字符级别的编辑转换为跨度级别。上述两个步骤应用于系统输出序列和黄金饮用，将它们转换为跨度级编辑集。 最后，我们可以通过比较它们来计算P/R/F值。 如果有多个黄金饮用，我们会选择 F-score 最高的那个。</p>
<ul>
<li>跨度级词序错误：<strong>在计算整体指标时</strong>，我们只考虑以上三类错误。 <strong>在分析时</strong>，我们区分了第四种错误类型——词序错误。 跨度级词序错误通常由冗余错误和缺失错误组成，其中删除的跨度与插入的跨度相同。 我们使用简单的启发式规则来识别此类错误</li>
</ul>
<h4 id="6-3-Results-and-Analysis"><a href="#6-3-Results-and-Analysis" class="headerlink" title="6.3 Results and Analysis"></a>6.3 Results and Analysis</h4><ul>
<li><p>性能变化的总体趋势与表 5 中原始 NLPCC18 数据集上的基本一致。首先，Seq2Seq 和 Seq2Edit 模型在 F0.5 上表现相当接近，但在精度和召回率上明显表现出不同的强度，其次，模型集成方法大大提高了性能。</p>
</li>
<li><p>一个有趣的观察是，在 MuCGEC 上，“3×Seq2Edit+3×Seq2Seq”在 All-test 和所有三个子集上明显优于“1×Seq2Edit+1×Seq2Seq”。 相比之下，原始 NLPCC18 测试数据中两者的改进并不大。 我们怀疑这可能表明多参考数据集可以更准确地评估模型性能。 然而，它可能需要进一步的人类调查以获得更多见解。</p>
</li>
<li><p>最后，模型和人类之间仍然存在巨大的性能差距，表明 CGEC 研究还有很长的路要走。</p>
</li>
<li><p><strong>四种错误类型的表现</strong>：表 7 显示了对四种错误类型的更细粒度的评估结果</p>
<p>1）Seq2Edit 模型更擅长处理冗余错误，而 Seq2Seq 模型更擅长处理替换和词序错误。 对于遗漏错误，两者表现相似。</p>
<p>2）可以在考虑底层模型架构后理解。 一方面，为了纠正冗余错误，Seq2Edit 模型只需要执行固定的删除操作，这对于 Seq2Seq 模型来说是一个隐含得多的选择（<strong>问题？？</strong>），因为它的目标是重写整个句子。 另一方面，Seq2Seq 由于其利用语言模型信息的天然能力，特别是在 BART 的增强下，适合替换或重新排序单词</p>
<p>3）同样，模型集成方法显着提高了所有错误类型的性能。 集成模型在冗余错误方面最接近人类，可能是因为它们最容易纠正。 最大的差距出现在词序错误中，这需要全局结构知识来纠正并且极具挑战性。</p>
<p>4）当最大引用数增加时，模型和人类的性能都会不断提高，尤其是人类。 由于只有少数句子有超过 3 个引用，因此当最大引用数从 3 增加到 All 时，改进非常小。 这一趋势表明，与单参考数据集相比，多参考数据集降低了低估表现的风险，因此对于模型评估更可靠。</p>
</li>
</ul>
<h3 id="8-Conclusions"><a href="#8-Conclusions" class="headerlink" title="8. Conclusions"></a>8. Conclusions</h3><p>本文介绍了 MuCGEC，这是一个新标注的 CGEC 评估数据集，由 CSL 学习者编写的 7,063 个句子组成。 与现有的 CGEC 数据集相比，由于三个重要特征，我们的数据集可以支持更可靠的评估：1）提供多个参考； 2）涵盖三个文本源； 3) 采用严格的质量控制（即标注指南和工作流程）。 在描述了数据构建过程之后，我们对我们的数据进行了详细的分析。 然后，我们采用两种主流且有竞争力的 CGEC 模型，即 Seq2Seq 和 Seq2Edit，并进行基准实验。 我们还建议采用基于字符的评估指标来取代以前使用的基于单词的评估指标。</p>
<a id="more"></a>

]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>GEC</tag>
      </tags>
  </entry>
  <entry>
    <title>论文解读：PipeDream</title>
    <url>/29937.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><a id="more"></a>

<h2 id="本文介绍了微软新提出的-PipeDream，旨在使深度学习网络训练并行化水平更高，进而提高训练效率。"><a href="#本文介绍了微软新提出的-PipeDream，旨在使深度学习网络训练并行化水平更高，进而提高训练效率。" class="headerlink" title="本文介绍了微软新提出的 PipeDream，旨在使深度学习网络训练并行化水平更高，进而提高训练效率。"></a>本文介绍了微软新提出的 PipeDream，旨在使深度学习网络训练并行化水平更高，进而提高训练效率。</h2><p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20191117085931501.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pkYmM=,size_16,color_FFFFFF,t_70"></p>
<p>深度神经网络（DNNs：Deep Neural Networks）已经在大量应用中取得了巨大进展，这些应用包括图像分类、翻译、语言建模以及视频字幕等。但 DNN 训练极其耗时，需要多个加速器高效地并行化。</p>
<p>在论文 “ <a href="https://www.microsoft.com/en-us/research/publication/pipedream-generalized-pipeline-parallelism-for-dnn-training/">PipeDream: Greneralized Pipeline Parallelism for DNN Training</a> ”（该论文发表于第 27 届 ACM 操作系统原理研讨会： <a href="https://sosp19.rcs.uwaterloo.ca/">SOSP 2019</a> ）中，微软系统研究小组的研究员，与来自卡内基梅隆大学和斯坦福大学的同事和学生们一起提出了一种 DNN 并行化训练的新方法。正如论文里在大量模型上展示的那样，这套被称为 PipeDream 的系统比传统的训练方法快了最多 4.3 倍。</p>
<p>DNN 训练是在前向和后向通道计算中迭代进行的。在每轮迭代中，训练过程循环处理输入数据的一个 minibatch，并且更新模型参数。最常见的 DNN 并行化训练方法是一种被称为数据并行化的方法（见下图 1），这种方法把输入数据分散到各个 workers（加速器）中运行。</p>
<p>不幸的是，尽管在数据并行化加速方面有一些性能优化的进展，但若要放在云基础设施上训练就会产生很大的通信开销。而且，GPU 计算速度的飞快提升，更进一步地把所有模型训练的耗时瓶颈转向了通信环节。</p>
<p>不那么常见的并行化训练形式是模型的并行化（见下图 1），是把算子分散到各个 worker 上计算的，这在以往通常用于训练大型 DNN 模型。模型并行化也遇到了挑战：它不能高效地利用硬件资源，并且需要程序员决定怎样按照给定的硬件资源来分割特定的模型，这其实无形中加重了程序员的负担。</p>
<p>PipeDream 是微软研究院 <a href="https://www.microsoft.com/en-us/research/project/fiddle/">Fiddle 项目</a>开发的一个系统，它引入了流水线并行化，这是一种 DNN 并行化训练的新方法，结合了传统的 batch 内并行化（模型并行化和数据并行化）和 batch 间并行化（流水线）。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20191117090027132.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pkYmM=,size_16,color_FFFFFF,t_70"></p>
<p>图 1：传统的 batch 内并行化训练方式（如数据并行化和模型并行化）对硬件利用率太低。左图中，数据并行化中的单个 worker 在交换梯度数据时不得不进行通信等待。右图是模型并行化，worker 之间只能同时处理一个 minibatch，这大大限制了硬件利用率。</p>
<h2 id="使用流水线并行化训练来解决-batch-内的并行化限制"><a href="#使用流水线并行化训练来解决-batch-内的并行化限制" class="headerlink" title="使用流水线并行化训练来解决 batch 内的并行化限制"></a>使用流水线并行化训练来解决 batch 内的并行化限制</h2><p>PipeDream 重新审视了模型的并行化，希望以此来优化性能，这与以往的动机不同，以前模型并行化是因为训练大型模型时，训练过程受限于数据集的大小。PipeDream 使用多个输入数据的流水线作业，来克服模型并行训练时硬件效率限制的问题。典型的流水线并行化设置涉及到不同 stage 之间 layer 的分割，每个 stage 都会被复制并且并行运行数据。流水线会被注入多个 batch，以使流水线满负荷运行在稳定状态。在大部分情况下，流水线并行化训练比数据并行化训练需要通信的数据要少很多，因为它只需要在两个 stage 边界之间传输 activation 和梯度。在稳定状态下，所有的 workers 时刻都在运转，不像模型并行化训练中会有停下来等待的时候（如下图所示）。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20191117090111768.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pkYmM=,size_16,color_FFFFFF,t_70"></p>
<p>图 2：左图中，我们展示了一个流水线并行化的例子，8 块 GPU 被分配到 4 个 stage 中。通信只用于 stage 边界的 actiavtion 和梯度上。stage 1,2 和 3 通过对其各自的 stage 进行复制来保证流水线的负载平衡。右图展示了具有 4 个 worker 的一个流水线，展示了启动阶段和稳定阶段。在这个例子中，后向处理花费的时间是前向处理的两倍。</p>
<h2 id="在-PipeDream-中克服流水线并行化训练的挑战"><a href="#在-PipeDream-中克服流水线并行化训练的挑战" class="headerlink" title="在 PipeDream 中克服流水线并行化训练的挑战"></a>在 PipeDream 中克服流水线并行化训练的挑战</h2><p>为了获得流水线并行化训练的潜在收益，PipeDream 必须克服三个主要挑战：</p>
<ul>
<li>  首先，PipeDream 必须在不同的输入数据间，协调双向流水线的工作。</li>
<li>  然后，PipeDream 必须管理后向通道里的权重版本，从而在数值上能够正确计算梯度，并且在后向通道里使用的权重版本必须和前向通道里使用的相同。</li>
<li>  最后，PipeDream 需要流水线里的所有 stage 都花费大致相同的计算时间，这是为了使流水线得到最大的通量（因为最慢的 stage 会成为流水线通量的瓶颈）。</li>
</ul>
<p><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20191117090229924.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pkYmM=,size_16,color_FFFFFF,t_70"> <strong>图 3：PipeDream 工作流程概览</strong></p>
<p>图 3 展示了 PipeDream 工作流程的顶层概览。给定一个模型和硬件部署方式，PipeDream 在单个 GPU 上进行短暂的运行时性能分析后，可以自动决定怎样分割这些 DNN 算子，如何平衡不同 stage 之间的计算负载，而同时尽可能减少目标平台上的通信量。PipeDream 即使是在不同的模型（不同点体现在计算和通信方面）和不同的平台上（不同点体现在互联的网络拓扑和分层带宽上）也能够有效的均衡负载。</p>
<p>由于 DNN 并不总是在可用的 worker 间进行均等分割，所以 PipeDream 可能在某些 stage 上使用数据并行化——多个 worker 会被分配到给定的 stage 上，并行化地处理不同的 minibatch。PipeDream 使用称作 1F1B 的调度算法来使硬件保持满负荷运转，同时还能达到类似数据并行化的语义。</p>
<p>在 1F1B 的稳定状态下，每个 worker 为它所在的 stage 严格地切换前向和后向通道，保证资源的高利用率（可忽略的流水线暂停，没有流水线 flush），即使在常见情况下，后向通道花费的时间多于前向通道时也是如此。上面的图 2 已经通过例子展示了这一点。与数据并行化不同，1F1B 还使用不同版本的权重来维持统计有效性。最后，PipeDream 扩展了 1F1B，在数据并行 stage 上引入了循环调度策略，保证了后向通道计算的梯度被引流至前向通道对应的 worker 上。</p>
<p>PipeDream 是基于 PyTorch（PipeDream 的<a href="https://arxiv.org/pdf/1806.03377.pdf">早期版本</a>使用 Caffe）构建出来的。我们的评估围绕着 DNN 模型、数据集和硬件配置的不同组合进行，证实了 PipeDream 流水线并行化所带来的训练时间上的收益。相比于数据并行化训练，PipeDream 在多 GPU 机器上达到了很高的精确度，性能方面，在图像分类上有 5.3 倍提升，机器翻译上 3.1 倍提升，语言建模任务有 4.3 倍提升，视频字幕模型则有 3 倍提升。PipeDream 比模型并行化有 2.6 至 15 倍的性能提升，相比混合并行化有 1.9 倍提升。</p>
<p>如果你对 PipeDream 更多细节感兴趣，可以在 GitHub 上找到<a href="https://github.com/msr-fiddle/pipedream">源码</a>。</p>
<p><strong>原文链接：</strong></p>
<p>[<a href="https://www.microsoft.com/en-us/research/blog/pipedream-a-more-effective-way-to-train-deep-neural-networks-using-pipeline-parallelism/]">https://www.microsoft.com/en-us/research/blog/pipedream-a-more-effective-way-to-train-deep-neural-networks-using-pipeline-parallelism/]</a>(</p>
]]></content>
      <categories>
        <category>论文</category>
      </categories>
      <tags>
        <tag>毕设</tag>
      </tags>
  </entry>
  <entry>
    <title>GPU并行训练</title>
    <url>/53759.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="GPU并行训练"><a href="#GPU并行训练" class="headerlink" title="GPU并行训练"></a>GPU并行训练</h2><h3 id="为什么要使用多GPU并行训练"><a href="#为什么要使用多GPU并行训练" class="headerlink" title="为什么要使用多GPU并行训练"></a>为什么要使用<a href="https://zhida.zhihu.com/search?content_id=177507443&content_type=Article&match_order=1&q=%E5%A4%9AGPU%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83&zhida_source=entity">多GPU并行训练</a></h3><p>简单来说，有两种原因：第一种是模型在一块GPU上放不下，两块或多块GPU上就能运行完整的模型（如早期的AlexNet）。第二种是多块GPU并行计算可以达到加速训练的效果。想要成为“炼丹大师“，多GPU并行训练是不可或缺的技能。</p>
<h3 id="常见的多GPU训练方法："><a href="#常见的多GPU训练方法：" class="headerlink" title="常见的多GPU训练方法："></a>常见的多GPU训练方法：</h3><p>1.<a href="https://zhida.zhihu.com/search?content_id=177507443&content_type=Article&match_order=1&q=%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C&zhida_source=entity">模型并行</a>方式：如果模型特别大，GPU显存不够，无法将一个显存放在GPU上，需要把网络的不同模块放在不同GPU上，这样可以训练比较大的网络。（下图左半部分）</p>
<p>2.<a href="https://zhida.zhihu.com/search?content_id=177507443&content_type=Article&match_order=1&q=%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C&zhida_source=entity">数据并行</a>方式：将整个模型放在一块GPU里，再复制到每一块GPU上，同时进行正向传播和反向误差传播。相当于加大了batch_size。（下图右半部分）</p>
<h3 id="GPU-设置"><a href="#GPU-设置" class="headerlink" title="GPU 设置"></a>GPU 设置</h3><p>nn.DataParallel<code>和</code>torch.nn.parallel.DistributedDataParallel`都是 PyTorch 用来在多个 GPU 上并行训练模型的工具， 但它们在实现上有一些重要的区别。</p>
<ol>
<li><code>nn.DataParallel</code>：</li>
</ol>
<ul>
<li><strong>单机多卡并行</strong>： <code>nn.DataParallel</code>适用于在单台机器的多个 GPU 上进行模型的并行训练。</li>
<li>模型复制： 它会在多个 GPU 上<strong>复制同一个模型</strong>， 每个模型处理<strong>部分数据</strong>（mini-batch） ，然后将<strong>梯度汇总</strong>以<strong>更新模型的参数</strong>。</li>
<li>简单易用： 使用简单， 只需在**模型外层套用<code>nn.DataParallel(model)</code>**即可。 PyTorch 会自动处理数据切分和梯度汇总的过程。</li>
</ul>
<p><strong>建议使用 DistributedDataParallel</strong>， 而不是这个类， 进行多 GPU 训练， 即使只有一个 节点。 请参阅： 使用 nn.parallel.DistributedDataParallel 而不是多处理或 nn.DataParallel 和 分布式数据并行 .</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/53759/1.png" style="zoom:40%;">

<p>如果要增加 GPU 占用率， 提高 <strong>batch_size</strong></p>
<ol start="2">
<li><p><code>torch.nn.parallel.DistributedDataParallel</code>：<br>PyTorch 的分布式训练主要通过<code>torch.distributed</code>包来实现， 该包提供了多进程多GPU 训练模型的能力。 以下是分布式训练的关键概念和组件：</p>
</li>
<li><p>进程组（Process Group） :<br>进程组是一组可以相互通信的进程。 在分布式训练中， 每个进程都是一个独立的 Python进程， 通常运行在不同的机器或机器的不同 GPU 上。</p>
</li>
<li><p>初始化进程组:<br>在 开 始 分 布 式 训 练 之 前 ， 需 要 初 始 化 进 程 组 。 这 通 常 通 过 调 用<code>torch.distributed.init_process_group()</code>函数完成， 该函数需要指定后端（如”nccl”、 “gloo”）和初始化方法。</p>
</li>
<li><p>分布式数据并行（Distributed Data Parallel, DDP） :<br>DDP 是一种在多个进程和多个 GPU 上并行训练模型的方法。 每个进程拥有模型的一个副本， 并且只处理数据的一个子集。 通过减少每个进程所需的数据量， DDP 可以有效地扩展到大量的 GPU。</p>
</li>
<li><p><code>DistributedDataParallel</code>类:</p>
<p>使用<code>torch.nn.parallel.DistributedDataParallel</code>类可以很容易地将现有的<code>nn.Module</code>包装起来， 使其能够在 DDP 中使用。 该类负责在不同的进程间同步模型的梯度。</p>
</li>
<li><p>环境变量:<br>使用环境变量如<code>MASTER_ADDR</code>和<code>MASTER_PORT</code>可以指定用于初始化进程组的主机<br>地址和端口。</p>
</li>
<li><p>分布式采样器:<br>为了确保每个进程只处理数据的一个独立子集， 通常需要使用分布式采样器， 如<code>torch.utils.data.distributed.DistributedSampler</code>。</p>
</li>
<li><p>梯度累积:<br>在资源有限的情况下， 可以通过梯度累积技术来模拟更大批量的训练， 即使不能在单个迭代中放入足够多的样本。</p>
</li>
<li><p>通信后端:<br>PyTorch 支持多种通信后端， 如”nccl”、 “gloo”和”mpi”， 它们在不同的硬件和网络配置下提供了不同的性能和特性。</p>
</li>
<li><p>调试和优化:<br>分布式训练可能会引入额外的复杂性和潜在的错误源。 PyTorch 提供了工具和最佳实践来帮助调试和优化分布式训练的性能。</p>
<p><a href="https://pytorch.org/docs/stable/distributed.html#">https://pytorch.org/docs/stable/distributed.html#</a></p>
</li>
</ol>
<p>针对分布式训练<br>字节跳动（抖音）<br><a href="https://github.com/bytedance/byteps?tab=readme-ov-file">https://github.com/bytedance/byteps?tab=readme-ov-file</a><br>阿里巴巴<br><a href="https://github.com/alibaba/EasyParallelLibrary?tab=readme-ov-file">https://github.com/alibaba/EasyParallelLibrary?tab=readme-ov-file</a></p>
<h2 id="Llama-Factory-多机多卡并行训练"><a href="#Llama-Factory-多机多卡并行训练" class="headerlink" title="Llama- Factory 多机多卡并行训练"></a>Llama- Factory 多机多卡并行训练</h2><p><a href="https://llamafactory.readthedocs.io/zh-cn/latest/advanced/distributed.html">https://llamafactory.readthedocs.io/zh-cn/latest/advanced/distributed.html</a></p>
<a id="more"></a>

]]></content>
      <categories>
        <category>GPU</category>
        <category>并行训练</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>并行训练</tag>
        <tag>大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-13-——集成学习</title>
    <url>/22975.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><p>集成学习通过建立几个模型来解决单一预测问题。它的工作原理是<strong>生成多个分类器/模型</strong>，各自独立地学习和作出预测。<strong>这些预测最后结合成组合预测，因此优于任何一个单分类的做出预测。</strong></p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/22975/1.png" style="zoom:60%;">

<h3 id="集成学习中boosting和Bagging"><a href="#集成学习中boosting和Bagging" class="headerlink" title="集成学习中boosting和Bagging"></a>集成学习中boosting和Bagging</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/22975/2.png" style="zoom:60%;">

<p>面对一个机器学习问题， 通常有两种策略。 </p>
<ul>
<li>一种是研发人员尝试各种模型， 选择其中表现最好的模型做重点调参优化。 这种策略类似于奥运会比赛， 通过强强竞争来选拔最优的运动员， 并逐步提高成绩。 </li>
<li>另一种重要的策略是集各家之长， 如同贤明的君主广泛地听取众多谋臣的建议， 然后综合考虑， 得到最终决策。 </li>
</ul>
<p>后一种策略的核心， <strong>是将多个分类器的结果统一成一个最终的决策。</strong> 使用这类策略的机器学习方法统称为<strong>集成学习</strong>。 其中的<strong>每个单独的分类器称为基分类器</strong>。</p>
<ul>
<li><p>Boosting 方法是通过逐步聚焦于基分类器分错的样本， <strong>减小集成分类器的偏差</strong>。 </p>
</li>
<li><p>Bagging 方法则是采取分而治之的策略， 通过对训练样本多次采样， 并分别训练出多个不同模型， 然后做综合， <strong>来减小集成分类器的方差</strong>。</p>
</li>
</ul>
<h3 id="Boosting（串行）"><a href="#Boosting（串行）" class="headerlink" title="Boosting（串行）"></a>Boosting（串行）</h3><p>Boosting 方法训练基分类器时采用串行的方式， 各个基分类器之间有依赖。<br>它的基本思路是将基分类器层层叠加， 每一层在训练的时候， <strong>对前一层基分类器分错的样本， 给予更高的权重。</strong> 测试时， 根据各层分类器的结果的<strong>加权</strong>得到最终结果。</p>
<p>代表算法：Adaboost，GBDT，XGBoost</p>
<h4 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h4><p>实现过程：</p>
<h5 id="1-训练第一个学习器"><a href="#1-训练第一个学习器" class="headerlink" title="1.训练第一个学习器"></a>1.训练第一个学习器</h5><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/22975/5.png" style="zoom:60%;">

<h5 id="2-调整数据分布"><a href="#2-调整数据分布" class="headerlink" title="2.调整数据分布"></a>2.调整数据分布</h5><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/22975/6.png" style="zoom:60%;">

<h5 id="3-训练第二个学习器"><a href="#3-训练第二个学习器" class="headerlink" title="3.训练第二个学习器"></a>3.训练第二个学习器</h5><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/22975/7.png" style="zoom:60%;">

<h5 id="4-再次调整数据分布"><a href="#4-再次调整数据分布" class="headerlink" title="4.再次调整数据分布"></a>4.再次调整数据分布</h5><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/22975/8.png" style="zoom:60%;">

<h5 id="5-依次训练学习器，调整数据分布"><a href="#5-依次训练学习器，调整数据分布" class="headerlink" title="5.依次训练学习器，调整数据分布"></a>5.依次训练学习器，调整数据分布</h5><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/22975/9.png" style="zoom:60%;">

<h5 id="6-整体过程实现"><a href="#6-整体过程实现" class="headerlink" title="6.整体过程实现"></a><strong>6.整体过程实现</strong></h5><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/22975/10.png" style="zoom:60%;">

<p><strong>关键点：</strong></p>
<p><strong>如何确认投票权重？</strong></p>
<p><strong>如何调整数据分布？</strong></p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/22975/11.png" style="zoom:60%;">

<p><strong>ababoost 的α是否可以为负值？</strong></p>
<p>在 AdaBoost 算法中， 每个弱分类器的系数α需要满足一定条件才能保证模型的正确性。 具体而言， α应该是大于等于 0 且小于等于正无穷大的实数。</p>
<p>因为在 AdaBoost 算法中， 每个弱分类器的权重是根据上一轮分类器的错误率计算得到的。如果某个弱分类器的错误率大于 1/2， 则其权重会是负数， 但是这并不意味着α可以为负值，因为这违反了α的定义。</p>
<p>在实际应用中， <strong>如果出现了α为负值的情况， 通常表示算法存在问题， 例如数据集过小或者弱分类器的表现太差</strong>等。 此时可以尝试增加数据量、 改变弱分类器的参数或者换用其他算法来解决问题。</p>
<p><strong>AdaBoost的构造过程小结</strong></p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/22975/12.png" style="zoom:60%;">

<h4 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h4><p>梯度提升决策树(GBDT Gradient Boosting Decision Tree) <strong>是一种迭代的决策树算法</strong>，<strong>该算法由多棵决策树组成，所有树的结论累加起来做最终答案。</strong>它在被提出之初就被认为是泛化能力（generalization)较强的算法。近些年更因为被用于搜索排序的机器学习模型而引起大家关注。</p>
<p><strong>GBDT = 梯度下降 + Boosting + 决策树</strong></p>
<p><strong>其核心思想是， 每一棵树学的是之前所有树结论和的残差， 这个残差就是一个加预测值后能得真实值的累加量。</strong></p>
<p>例如：用户 A 的真实年龄是 25 岁， 但第一棵决策树的预测年龄是 22 岁， 差了 3岁， 即残差为 3。 那么在第二棵树里我们把 A 的年龄设为 3 岁去学习， 如果第二棵树能把 A 分到 3 岁的叶子节点， 那两棵树的结果相加就可以得到 A 的真实年龄； 如果第二棵树的结论是 5 岁， 则 A 仍然存在−2 岁的残差， 第三棵树里 A 的年龄就变成−2 岁，继续学。 这里使用残差继续学习， 就是 GBDT 中 Gradient Boosted 所表达的意思。</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/22975/13.png" style="zoom:60%;">

<p>训练一个模型 m1, 产生错误 e1<br>针对 e1 训练第二个模型 m2， 产生错误 e2<br>针对 e2 训练第三个模型 m3， 产生错误 e3…<br>最终预测结果是:m1 + m2 + m3 + …</p>
<h4 id="GBDT-的优点和局限性有哪些？"><a href="#GBDT-的优点和局限性有哪些？" class="headerlink" title="GBDT 的优点和局限性有哪些？"></a>GBDT 的优点和局限性有哪些？</h4><p><strong>■ 优点</strong><br>（1） 预测阶段的计算速度快， 树与树之间可并行化计算。 （是前一个有结果可以给下一个）<br>（2） 在分布稠密的数据集上， 泛化能力和表达能力都很好， 这使得 GBDT 在Kaggle 的众多竞赛中， 经常名列榜首。<br>（3） 采用决策树作为弱分类器使得 GBDT 模型具有较好的解释性和鲁棒性，能够自动发现特征间的高阶关系， 并且也不需要对数据进行特殊的预处理如归一化等。<br><strong>■ 局限性</strong><br>（1） GBDT 在高维稀疏的数据集上， 表现不如支持向量机(<a href="https://www.bilibili.com/video/BV1Hs411w7ci">https://www.bilibili.com/video/BV1Hs411w7ci</a> 向量机可以看这个B站博主的分享)或者神经网络。<br>（2） GBDT 在处理文本分类特征问题上， 相对其他模型的优势不如它在处理数值特征时明显。<br>（3） 训练过程需要串行训练， 只能在决策树内部采用一些局部并行的手段提高训练速度。</p>
<h3 id="Bagging（并行）"><a href="#Bagging（并行）" class="headerlink" title="Bagging（并行）"></a>Bagging（并行）</h3><p>Bagging 与 Boosting 的串行训练方式不同， Bagging 方法在训练过程中， <strong>各基分类器之间无强依赖</strong>， 可以进行<strong>并行训练</strong>。 其中很著名的算法之一是<strong>基于决策树基分类器的随机森林（Random Forest）</strong> 。 </p>
<p>为了让基分类器之间互相独立， 将训练集分为若干子集（ 当训练样本数量较少时， 子集之间可能有交叠） 。</p>
<p> Bagging 方法更像是一个集体决策的过程， 每个个体都进行单独学习， 学习的内容可以相同， 也可以不同， 也可以部分重叠。 但由于个体之间存在差异性， 最终做出的判断不会完全一致。 在最终做决策时， 每个个体单独作出判断， 再通过<strong>投票的方式</strong>做出<strong>最后的集体决策</strong></p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/22975/3.png" style="zoom:60%;">

<p>Model 1、 Model 2、 Model 3 都是用训练集的一个子集训练出来的， 单独来看， 它们的决策边界都很曲折， 有过拟合的倾向。 集成之后的模型（红线所示） 的决策边界就比各个独立的模型平滑了， 这是由于集成的加权投票方法， 减小了方差。</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/22975/4.png" style="zoom:60%;">

<h3 id="如何选择基分类器"><a href="#如何选择基分类器" class="headerlink" title="如何选择基分类器"></a>如何选择基分类器</h3><p><strong>最常用的基分类器是决策树</strong>， 主要有以下 3 个方面的原因。<br>（1） 决策树可以较为方便地将样本的权重整合到训练过程中， 而不需要使用过采样的方法来调整样本权重。<br>（2） 决策树的表达能力和泛化能力， 可以通过调节树的层数来做折中。<br>（3） 数据样本的扰动对于决策树的影响较大， 因此不同子样本集合生成的决策树基分类器<strong>随机性较大</strong>（弱分类器）**， 这样的“不稳定学习器”更适合作为基分类器。 此外，在决策树节点分裂的时候， 随机地选择一个特征子集， 从中找出最优分裂属性，很好地引入了随机性。</p>
<p>除了决策树外， <strong>神经网络模型</strong>也适合作为基分类器， 主要由于神经网络模型也比较“不稳定”， <strong>而且还可以通过调整神经元数量、 连接方式、 网络层数、 初始权值等方式引入随机性。</strong></p>
<h4 id="可否将随机森林中的基分类器，-由决策树替换为线性分类器或-K-近邻？-请解释为什么？"><a href="#可否将随机森林中的基分类器，-由决策树替换为线性分类器或-K-近邻？-请解释为什么？" class="headerlink" title="可否将随机森林中的基分类器， 由决策树替换为线性分类器或 K-近邻？ 请解释为什么？"></a>可否将随机森林中的基分类器， 由决策树替换为线性分类器或 K-近邻？ 请解释为什么？</h4><p>随机森林属于 Bagging 类的集成学习。 <strong>Bagging 的主要好处是集成后的分类器的方差， 比基分类器的方差小。</strong> Bagging 所采用的基分类器， 最好是<strong>本身对样本分布较为敏感的</strong>（即所谓<strong>不稳定的分类器</strong>） ， 这样 Bagging 才能有用武之地。 <strong>线性分类器或者 K-近邻都是较为稳定的分类器</strong>， 本身<strong>方差就不大</strong>， 所以以它们为基分类器使用 Bagging 并不能在原有基分类器的基础上获得更好的表现， 甚至可能因为Bagging 的采样， 而导致他们在训练中更难收敛， 从而增大了集成分类器的偏差。</p>
<h3 id="偏差与方差"><a href="#偏差与方差" class="headerlink" title="偏差与方差"></a>偏差与方差</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/22975/14.png" style="zoom:60%;">

<p>在有监督学习中， 模型的泛化误差来源于两个方面——偏差和方差， 具体来讲偏差和方差的定义如下：</p>
<p><strong>偏差</strong>：指的是由所有采样得到的大小为 m 的训练数据集训练出的所有模型的输出的平均值和真实模型输出之间的偏差。 偏差通常是由于我们对学习算法做了错误的假设所导致的， 比如真实模型是某个二次函数， 但我们假设模型是一次函数。由偏差带来的误差通常在训练误差上就能体现出来。</p>
<p><strong>方差</strong>：指的是由所有采样得到的大小为 m 的训练数据集训练出的所有模型的输出的方差。 方差通常是由于模型的复杂度相对于训练样本数 m 过高导致的， 比如一共有 100 个训练样本， 而我们假设模型是阶数不大于 200 的多项式函数。 由方差带来的误差通常体现在测试误差相对于训练误差的增量上。</p>
<p><strong>Bagging(并行)能够提高弱分类器性能的原因是降低了方差， Boosting(串行)能够提升弱分类器性能的原因是降低了偏差。</strong> </p>
<a id="more"></a>

]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Word2Vec与Embedding</title>
    <url>/63226.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><h3 id="什么是-Word2vec"><a href="#什么是-Word2vec" class="headerlink" title="什么是 Word2vec?"></a>什么是 Word2vec?</h3><p>NLP 里面，最细粒度的是 词语，词语组成句子，句子再组成段落、篇章、文档。所以处理 NLP 的问题，首先就要拿词语开刀。</p>
<p> NLP 里的词语，是人类的抽象总结，是符号形式的（比如中文、英文、拉丁文等等），所以需要把他们转换成数值形式，或者说——嵌入到一个数学空间里，这种嵌入方式，就叫词嵌入（word embedding)，而 Word2vec，就是词嵌入（ word embedding) 的一种</p>
<p>在 NLP 中，把 x 看做一个句子里的一个词语，y 是这个词语的上下文词语，那么这里的 f，便是 NLP 中经常出现的『语言模型』（language model），这个模型的目的，就是判断 (x,y) 这个样本，是否符合自然语言的法则，更通俗点说就是：词语x和词语y放在一起，是不是人话。</p>
<p>Word2vec 正是来源于这个思想，但它的最终目的，不是要把 f 训练得多么完美，而是只关心模型训练完后的副产物——模型参数（这里特指神经网络的权重），并将这些参数，作为输入 x 的某种向量化的表示，这个向量便叫做——词向量</p>
<p>我们来看个例子，如何用 Word2vec 寻找相似词：</p>
<ul>
<li>对于一句话：『她们 夸 吴彦祖 帅 到 没朋友』，如果输入 x 是『吴彦祖』，那么 y 可以是『她们』、『夸』、『帅』、『没朋友』这些词</li>
<li>现有另一句话：『她们 夸 我 帅 到 没朋友』，如果输入 x 是『我』，那么不难发现，这里的上下文 y 跟上面一句话一样</li>
<li>从而 f(吴彦祖) = f(我) = y，所以大数据告诉我们：我 = 吴彦祖（完美的结论</li>
</ul>
<h3 id="Skip-gram-和-CBOW-模型"><a href="#Skip-gram-和-CBOW-模型" class="headerlink" title="Skip-gram 和 CBOW 模型"></a><a href="https://zhida.zhihu.com/search?content_id=2709857&content_type=Article&match_order=1&q=Skip-gram&zhida_source=entity">Skip-gram</a> 和 <a href="https://zhida.zhihu.com/search?content_id=2709857&content_type=Article&match_order=1&q=CBOW&zhida_source=entity">CBOW</a> 模型</h3><p>上面我们提到了语言模型</p>
<ul>
<li>如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做『Skip-gram 模型』</li>
<li>而如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 『CBOW 模型』</li>
</ul>
<p>我们先来看个最简单的例子。上面说到， y 是 x 的上下文，所以 y 只取上下文里一个词语的时候，语言模型就变成：</p>
<blockquote>
<p>用当前词 x 预测它的下一个词 y</p>
</blockquote>
<p>但如上面所说，一般的数学模型只接受数值型输入，这里的 x 该怎么表示呢？ 显然不能用 Word2vec，因为这是我们训练完模型的产物，现在我们想要的是 x 的一个原始输入形式。</p>
<p>答案是：<strong>one-hot encoder</strong></p>
<p>当模型训练完后，最后得到的其实是<strong>神经网络的权重</strong>，比如现在输入一个 x 的 one-hot encoder: [1,0,0,…,0]，对应刚说的那个词语『吴彦祖』，则在输入层到隐含层的权重里，只有对应 1 这个位置的权重被激活，这些权重的个数，跟隐含层节点数是一致的，从而这些权重组成一个向量 vx 来表示x，而因为每个词语的 one-hot encoder 里面 1 的位置是不同的，所以，这个向量 vx 就可以用来唯一表示 x。</p>
<h3 id="Skip-gram-更一般的情形"><a href="#Skip-gram-更一般的情形" class="headerlink" title="Skip-gram 更一般的情形"></a>Skip-gram 更一般的情形</h3><p>一对多</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/63226/1.png" style="zoom:25%;">

<h3 id="CBOW-更一般的情形"><a href="#CBOW-更一般的情形" class="headerlink" title="CBOW 更一般的情形"></a>CBOW 更一般的情形</h3><p><strong>更为常用，</strong>跟 Skip-gram 相似，只不过:</p>
<blockquote>
<p>Skip-gram 是预测一个词的上下文，而 CBOW 是用上下文预测这个词</p>
<p>多对一</p>
</blockquote>
<p>网络结构如下:</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/63226/2.png" style="zoom:33%;">

<p>更 Skip-gram 的模型并联不同，这里是输入变成了多个单词，所以要对输入处理下（一般是求和然后平均），输出的 cost function 不变</p>
<p><strong>最终训练好的N-dim的向量（是一个密集向量）就是表示当前输出词语的词向量。</strong>相当于多维特征去表征一个词的词义，可以发现，相近的词向量之间的距离是对应相等的。比如：国王与皇后的距离与男人与女人的距离是相同的。</p>
<h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><p>Embedding(词嵌入)：word2vec是Embedding的一种，</p>
<h3 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras  <span class="comment">#这里的波浪线不用管</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#用karas有的数据集imdb，电影分类,分电影是积极的，还是消极的</span></span><br><span class="line">imdb = keras.datasets.imdb</span><br><span class="line"><span class="comment">#载入数据使用下面两个参数</span></span><br><span class="line">vocab_size = <span class="number">10000</span>  <span class="comment">#词典大小，仅保留训练数据中前10000个最经常出现的单词，低频单词被舍弃</span></span><br><span class="line">index_from = <span class="number">3</span>  <span class="comment">#0,1,2,3空出来做别的事</span></span><br><span class="line"><span class="comment">#前一万个词出现词频最高的会保留下来进行处理，后面的作为特殊字符处理，</span></span><br><span class="line"><span class="comment"># 小于3的id都是特殊字符，下面代码有写</span></span><br><span class="line"><span class="comment"># 需要注意的一点是取出来的词表还是从1开始的，需要做处理</span></span><br><span class="line">(train_data, train_labels), (test_data, test_labels) = imdb.load_data(</span><br><span class="line">    num_words=vocab_size, index_from=index_from)</span><br><span class="line"></span><br><span class="line"><span class="comment">#载入词表，看下词表长度，词表就像英语字典</span></span><br><span class="line">word_index = imdb.get_word_index()</span><br><span class="line">print(<span class="built_in">len</span>(word_index))</span><br><span class="line">print(<span class="built_in">type</span>(word_index))</span><br><span class="line"><span class="comment">#词表虽然有8万多，但是我们只载入了最高频的1万词！！！！</span></span><br></pre></td></tr></table></figure>
<h4 id="构造-word2idx-和-idx2word"><a href="#构造-word2idx-和-idx2word" class="headerlink" title="构造 word2idx 和 idx2word"></a>构造 word2idx 和 idx2word</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word2idx = &#123;word: idx + <span class="number">3</span> <span class="keyword">for</span> word, idx <span class="keyword">in</span> word_index.items()&#125;  <span class="comment"># 0,1,2,3空出来做别的事,这里的idx是从1开始的,所以加3</span></span><br><span class="line">word2idx.update(&#123;</span><br><span class="line">    <span class="string">&quot;[PAD]&quot;</span>: <span class="number">0</span>,  <span class="comment"># 填充 token</span></span><br><span class="line">    <span class="string">&quot;[BOS]&quot;</span>: <span class="number">1</span>,  <span class="comment"># begin of sentence</span></span><br><span class="line">    <span class="string">&quot;[UNK]&quot;</span>: <span class="number">2</span>,  <span class="comment"># 未知 token</span></span><br><span class="line">    <span class="string">&quot;[EOS]&quot;</span>: <span class="number">3</span>,  <span class="comment"># end of sentence</span></span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">idx2word = &#123;idx: word <span class="keyword">for</span> word, idx <span class="keyword">in</span> word2idx.items()&#125;  <span class="comment"># 反向词典,id变为单词(键值反转)</span></span><br></pre></td></tr></table></figure>
<h4 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a>Tokenizer</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tokenizer</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, word2idx, idx2word, max_length=<span class="number">500</span>, pad_idx=<span class="number">0</span>, bos_idx=<span class="number">1</span>, eos_idx=<span class="number">3</span>, unk_idx=<span class="number">2</span></span>):</span></span><br><span class="line">        self.word2idx = word2idx  <span class="comment">#词表,单词到id</span></span><br><span class="line">        self.idx2word = idx2word  <span class="comment">#词表，id到单词</span></span><br><span class="line">        self.max_length = max_length</span><br><span class="line">        self.pad_idx = pad_idx  <span class="comment">#填充</span></span><br><span class="line">        self.bos_idx = bos_idx  <span class="comment">#开始</span></span><br><span class="line">        self.eos_idx = eos_idx  <span class="comment">#结束</span></span><br><span class="line">        self.unk_idx = unk_idx  <span class="comment">#未知，未出现在最高频词表中的词</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, text_list</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将文本列表转化为索引列表</span></span><br><span class="line"><span class="string">        :param text_list:当前批次的文本列表</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        max_length = <span class="built_in">min</span>(self.max_length, <span class="number">2</span> + <span class="built_in">max</span>(</span><br><span class="line">            [<span class="built_in">len</span>(text) <span class="keyword">for</span> text <span class="keyword">in</span> text_list]))  <span class="comment">#最大长度，最大长度是500，但是如果句子长度小于500，就取句子长度（句子长度是本组句子中最长的），2是为了留出开始和结束的位置</span></span><br><span class="line">        indices = []</span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> text_list:</span><br><span class="line">            index = [self.word2idx.get(word, self.unk_idx) <span class="keyword">for</span> word <span class="keyword">in</span> text]  <span class="comment">#单词转化为id，未知的词用unk_idx代替</span></span><br><span class="line">            index = [self.bos_idx] + index + [self.eos_idx]  <span class="comment">#添加开始和结束</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(index) &lt; max_length:</span><br><span class="line">                index = index + [self.pad_idx] * (max_length - <span class="built_in">len</span>(index))  <span class="comment">#填充0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                index = index[:max_length]  <span class="comment">#如果句子长度大于500，就截断</span></span><br><span class="line">            indices.append(index)</span><br><span class="line">        <span class="keyword">return</span> torch.tensor(indices)  <span class="comment">#二维列表转化为tensor</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, indices_list, remove_bos=<span class="literal">True</span>, remove_eos=<span class="literal">True</span>, remove_pad=<span class="literal">True</span>, split=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将索引列表转化为文本列表</span></span><br><span class="line"><span class="string">        :param indices_list:某批次的索引列表</span></span><br><span class="line"><span class="string">        :param remove_bos:</span></span><br><span class="line"><span class="string">        :param remove_eos:</span></span><br><span class="line"><span class="string">        :param remove_pad:</span></span><br><span class="line"><span class="string">        :param split:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        text_list = []</span><br><span class="line">        <span class="keyword">for</span> indices <span class="keyword">in</span> indices_list:</span><br><span class="line">            text = []</span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> indices:</span><br><span class="line">                word = self.idx2word.get(index, <span class="string">&quot;[UNK]&quot;</span>)</span><br><span class="line">                <span class="keyword">if</span> remove_bos <span class="keyword">and</span> word == <span class="string">&quot;[BOS]&quot;</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> remove_eos <span class="keyword">and</span> word == <span class="string">&quot;[EOS]&quot;</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> remove_pad <span class="keyword">and</span> word == <span class="string">&quot;[PAD]&quot;</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                text.append(word)</span><br><span class="line">            text_list.append(<span class="string">&quot; &quot;</span>.join(text) <span class="keyword">if</span> <span class="keyword">not</span> split <span class="keyword">else</span> text)</span><br><span class="line">        <span class="keyword">return</span> text_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(word2idx=word2idx, idx2word=idx2word)</span><br><span class="line">raw_text = [<span class="string">&quot;hello world&quot;</span>.split(), <span class="string">&quot;tokenize text datas with batch&quot;</span>.split(), <span class="string">&quot;this is a test&quot;</span>.split()]</span><br><span class="line">indices = tokenizer.encode(raw_text)  <span class="comment">#encode支持批量处理</span></span><br><span class="line">decode_text = tokenizer.decode(indices.tolist(), remove_bos=<span class="literal">False</span>, remove_eos=<span class="literal">False</span>, remove_pad=<span class="literal">False</span>)</span><br><span class="line">print(<span class="string">&quot;raw text&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> raw <span class="keyword">in</span> raw_text:</span><br><span class="line">    print(raw)</span><br><span class="line">print(<span class="string">&quot;indices&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> indices:</span><br><span class="line">    print(index)</span><br><span class="line">print(<span class="string">&quot;decode text&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> decode <span class="keyword">in</span> decode_text:</span><br><span class="line">    print(decode)</span><br></pre></td></tr></table></figure>
<p>对应效果：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/63226/3.png" style="zoom:50%;">

<h4 id="数据集与-DataLoader"><a href="#数据集与-DataLoader" class="headerlink" title="数据集与 DataLoader"></a>数据集与 DataLoader</h4><p>由于是tf的数据集，dataset需要重写：</p>
<ul>
<li>init</li>
<li>getitem</li>
<li>Len</li>
</ul>
<p>并且注意：由于刚导入train_data是包含ids的一系列数据(其中包含词表大小和get_word_index())，每一个样本是不包含结束符的，所以需要先进行decode然后encode为其补上结束符。并且需要对batch进行encode，统一长度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IMDBDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data, labels, remain_length=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> remain_length:  <span class="comment">#字符串输出样本中，是否含有【BOS】和【EOS】，【PAD】</span></span><br><span class="line">            self.data = tokenizer.decode(data, remove_bos=<span class="literal">False</span>, remove_eos=<span class="literal">False</span>, remove_pad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 缩减一下数据</span></span><br><span class="line">            self.data = tokenizer.decode(data)</span><br><span class="line">        self.labels = labels</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        text = self.data[index]</span><br><span class="line">        label = self.labels[index]</span><br><span class="line">        <span class="keyword">return</span> text, label</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fct</span>(<span class="params">batch</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    回调函数：</span></span><br><span class="line"><span class="string">    将batch(一批)数据处理成tensor形式</span></span><br><span class="line"><span class="string">    :param batch:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    text_list = [item[<span class="number">0</span>].split() <span class="keyword">for</span> item <span class="keyword">in</span> batch]  <span class="comment">#batch是128样本，每个样本类型是元组，第一个元素是文本，第二个元素是标签</span></span><br><span class="line">    label_list = [item[<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> batch]</span><br><span class="line">    text_list = tokenizer.encode(text_list).to(dtype=torch.<span class="built_in">int</span>)  <span class="comment"># 文本转化为索引</span></span><br><span class="line">    <span class="keyword">return</span> text_list, torch.tensor(label_list).reshape(-<span class="number">1</span>, <span class="number">1</span>).to(dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_ds = IMDBDataset(train_data, train_labels)</span><br><span class="line">test_ds = IMDBDataset(test_data, test_labels)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=<span class="literal">True</span>, collate_fn=collate_fct)  <span class="comment">#调用回调函数：collate_fn是处理batch的函数！！</span></span><br><span class="line">test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=<span class="literal">False</span>, collate_fn=collate_fct)</span><br></pre></td></tr></table></figure>
<p>此时的train_dl为：[128,500]，为了进一步将其转变为密集向量，将对其转换为one-hot编码：[128,500,10000]，相当于将原本的每条数据中的每个id改为对应的one-hot编码(10000为vocab_size)</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/63226/4.png" style="zoom:60%;">

<p>选择10000*16的矩阵(需要训练，包含权值w)，这样通过矩阵相乘，最后可以得到500x16的矩阵，这样500个词每一个词都又一个16的密集向量所表示。</p>
<h4 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># target output size of 5</span></span><br><span class="line">m = nn.AdaptiveAvgPool1d(<span class="number">1</span>)  <span class="comment"># 自适应平均池化</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">6</span>)</span><br><span class="line">output = m(<span class="built_in">input</span>)</span><br><span class="line">output.size()  <span class="comment">#可以看到最后一维变成了1</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AddingModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embedding_dim=<span class="number">16</span>, hidden_dim=<span class="number">64</span>, vocab_size=vocab_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AddingModel, self).__init__()</span><br><span class="line">        self.embeding = nn.Embedding(vocab_size, embedding_dim)  <span class="comment"># 词嵌入</span></span><br><span class="line">        self.pool = nn.AdaptiveAvgPool1d(<span class="number">1</span>)  <span class="comment"># 自适应平均池化,对应tf是全局平均值池化</span></span><br><span class="line">        self.layer = nn.Linear(embedding_dim, hidden_dim)  <span class="comment"># 全连接层</span></span><br><span class="line">        self.fc = nn.Linear(hidden_dim, <span class="number">1</span>)  <span class="comment"># 全连接层</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># [bs, seq length] [128, 500]  [128,500,10000]---&gt;[128,500,16]</span></span><br><span class="line">        x = self.embeding(x)</span><br><span class="line">        <span class="comment"># [bs, seq length, embedding_dim]--&gt;[bs, embedding_dim, seq length]，尺寸[128,500,16]--》[128,16,500]</span></span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        x = self.pool(x)  <span class="comment"># 每个样本变为一个密集向量，在seq_length维度上进行平均池化，[128,16,500]--&gt;[128,16,1]</span></span><br><span class="line">        x=x.squeeze(<span class="number">2</span>)  <span class="comment"># [bs, embedding_dim, 1] -&gt;[bs, embedding_dim]</span></span><br><span class="line">        <span class="comment"># [bs, embedding_dim] -&gt; [bs, hidden_dim]</span></span><br><span class="line">        x = self.layer(x)</span><br><span class="line">        x = self.fc(x)  <span class="comment"># [bs, hidden_dim] -&gt; [bs, 1]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> AddingModel().named_parameters():</span><br><span class="line">    print(<span class="string">f&quot;<span class="subst">&#123;key:^<span class="number">40</span>&#125;</span>paramerters num: <span class="subst">&#123;np.prod(value.shape)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/63226/5.png" style="zoom:60%;">

<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><ul>
<li>损失函数使用的是<strong>二进制交叉熵损失</strong>：torch.nn.functional.binary_cross_entropy_with_logits, 先sigmoid再计算交叉熵</li>
<li><strong>二进制交叉熵损失</strong>又叫对数似然损失：用于使用一个神经元，做二分类，也即逻辑回归所使用的分类。</li>
</ul>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/63226/6.png" style="zoom:50%;">

<ul>
<li>而多个神经元做多分类的时候，则使用的是交叉熵损失(只有前面一半)</li>
<li>binary_cross_entropy_with_logits与.binary_cross_entropy的区别：在于with_logits会先进行sigmoid，再计算交叉熵。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        model,</span></span></span><br><span class="line"><span class="function"><span class="params">        train_loader,</span></span></span><br><span class="line"><span class="function"><span class="params">        val_loader,</span></span></span><br><span class="line"><span class="function"><span class="params">        epoch,</span></span></span><br><span class="line"><span class="function"><span class="params">        loss_fct,</span></span></span><br><span class="line"><span class="function"><span class="params">        optimizer,</span></span></span><br><span class="line"><span class="function"><span class="params">        tensorboard_callback=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        save_ckpt_callback=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        early_stop_callback=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        eval_step=<span class="number">500</span>,</span></span></span><br><span class="line"><span class="function"><span class="params"></span>):</span></span><br><span class="line">    record_dict = &#123;</span><br><span class="line">        <span class="string">&quot;train&quot;</span>: [],</span><br><span class="line">        <span class="string">&quot;val&quot;</span>: []</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    global_step = <span class="number">0</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">with</span> tqdm(total=epoch * <span class="built_in">len</span>(train_loader)) <span class="keyword">as</span> pbar:</span><br><span class="line">        <span class="keyword">for</span> epoch_id <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">            <span class="comment"># training</span></span><br><span class="line">            <span class="keyword">for</span> datas, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">                datas = datas.to(device)</span><br><span class="line">                labels = labels.to(device)</span><br><span class="line">                <span class="comment"># 梯度清空</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">                <span class="comment"># 模型前向计算</span></span><br><span class="line">                logits = model(datas)</span><br><span class="line">                <span class="comment"># 计算损失</span></span><br><span class="line">                loss = loss_fct(logits, labels)</span><br><span class="line">                <span class="comment"># 梯度回传</span></span><br><span class="line">                loss.backward()</span><br><span class="line">                <span class="comment"># 调整优化器，包括学习率的变动等</span></span><br><span class="line">                optimizer.step()</span><br><span class="line">                preds = logits &gt; <span class="number">0</span> <span class="comment">#当sigmoid输出大于0.5时，预测为1，否则预测为0，这里大于0，刚好sigmoid的值是0.5，预测为1</span></span><br><span class="line"></span><br><span class="line">                acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())</span><br><span class="line">                loss = loss.cpu().item()</span><br><span class="line">                <span class="comment"># record</span></span><br><span class="line"></span><br><span class="line">                record_dict[<span class="string">&quot;train&quot;</span>].append(&#123;</span><br><span class="line">                    <span class="string">&quot;loss&quot;</span>: loss, <span class="string">&quot;acc&quot;</span>: acc, <span class="string">&quot;step&quot;</span>: global_step</span><br><span class="line">                &#125;)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># evaluating</span></span><br><span class="line">                <span class="keyword">if</span> global_step % eval_step == <span class="number">0</span>:</span><br><span class="line">                    model.<span class="built_in">eval</span>()</span><br><span class="line">                    val_loss, val_acc = evaluating(model, val_loader, loss_fct)</span><br><span class="line">                    record_dict[<span class="string">&quot;val&quot;</span>].append(&#123;</span><br><span class="line">                        <span class="string">&quot;loss&quot;</span>: val_loss, <span class="string">&quot;acc&quot;</span>: val_acc, <span class="string">&quot;step&quot;</span>: global_step</span><br><span class="line">                    &#125;)</span><br><span class="line">                    model.train()</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 1. 使用 tensorboard 可视化</span></span><br><span class="line">                    <span class="keyword">if</span> tensorboard_callback <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                        tensorboard_callback(</span><br><span class="line">                            global_step,</span><br><span class="line">                            loss=loss, val_loss=val_loss,</span><br><span class="line">                            acc=acc, val_acc=val_acc,</span><br><span class="line">                            lr=optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>],</span><br><span class="line">                        )</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 2. 保存模型权重 save model checkpoint</span></span><br><span class="line">                    <span class="keyword">if</span> save_ckpt_callback <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                        save_ckpt_callback(global_step, model.state_dict(), metric=val_acc)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 3. 早停 Early Stop</span></span><br><span class="line">                    <span class="keyword">if</span> early_stop_callback <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                        early_stop_callback(val_acc)</span><br><span class="line">                        <span class="keyword">if</span> early_stop_callback.early_stop:</span><br><span class="line">                            print(<span class="string">f&quot;Early stop at epoch <span class="subst">&#123;epoch_id&#125;</span> / global_step <span class="subst">&#123;global_step&#125;</span>&quot;</span>)</span><br><span class="line">                            <span class="keyword">return</span> record_dict</span><br><span class="line"></span><br><span class="line">                <span class="comment"># udate step</span></span><br><span class="line">                global_step += <span class="number">1</span></span><br><span class="line">                pbar.update(<span class="number">1</span>)</span><br><span class="line">                pbar.set_postfix(&#123;<span class="string">&quot;epoch&quot;</span>: epoch_id&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> record_dict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">epoch = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">model = AddingModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 定义损失函数 采用二进制交叉熵损失, 先sigmoid再计算交叉熵</span></span><br><span class="line">loss_fct = F.binary_cross_entropy_with_logits</span><br><span class="line"><span class="comment"># loss_fct =nn.BCEWithLogitsLoss()</span></span><br><span class="line"><span class="comment"># 2. 定义优化器 采用 adam</span></span><br><span class="line"><span class="comment"># Optimizers specified in the torch.optim package</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. tensorboard 可视化</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&quot;runs&quot;</span>):</span><br><span class="line">    os.mkdir(<span class="string">&quot;runs&quot;</span>)</span><br><span class="line">tensorboard_callback = TensorBoardCallback(<span class="string">&quot;runs/imdb-adding&quot;</span>)</span><br><span class="line"><span class="comment"># tensorboard_callback.draw_model(model, [1, MAX_LENGTH])</span></span><br><span class="line"><span class="comment"># 2. save best</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&quot;checkpoints&quot;</span>):</span><br><span class="line">    os.makedirs(<span class="string">&quot;checkpoints&quot;</span>)</span><br><span class="line">save_ckpt_callback = SaveCheckpointsCallback(<span class="string">&quot;checkpoints/imdb-adding&quot;</span>, save_step=<span class="built_in">len</span>(train_dl), save_best_only=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 3. early stop</span></span><br><span class="line">early_stop_callback = EarlyStopCallback(patience=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">model = model.to(device)</span><br><span class="line">record = training(</span><br><span class="line">    model,</span><br><span class="line">    train_dl,</span><br><span class="line">    test_dl,</span><br><span class="line">    epoch,</span><br><span class="line">    loss_fct,</span><br><span class="line">    optimizer,</span><br><span class="line">    tensorboard_callback=tensorboard_callback,</span><br><span class="line">    save_ckpt_callback=save_ckpt_callback,</span><br><span class="line">    early_stop_callback=early_stop_callback,</span><br><span class="line">    eval_step=<span class="built_in">len</span>(train_dl)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<a id="more"></a>

]]></content>
      <categories>
        <category>深度学习</category>
        <category>大模型</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>大模型</tag>
        <tag>Word2Vec</tag>
        <tag>Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title>seq2seq模型</title>
    <url>/34690.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="seq2seq"><a href="#seq2seq" class="headerlink" title="seq2seq"></a>seq2seq</h2><h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/34690/4.png" style="zoom:50%;">

<ul>
<li>效果与LSTM相近，并且参数量比LSTM更少，计算复杂度也更小</li>
</ul>
<h3 id="模型思想-Attention"><a href="#模型思想-Attention" class="headerlink" title="模型思想-Attention"></a>模型思想-Attention</h3><p><a href="https://zhuanlan.zhihu.com/p/394166679">https://zhuanlan.zhihu.com/p/394166679</a></p>
<p>原始的 seq2seq<br>Encoder 和 Decoder 都是循环神经网络实现， 可以是 RNN， LSTM， GRU， CFG 等</p>
<p>GRU 即 Gated Recurrent Unit。 前面说到为了克服 RNN 无法很好处理远距离依赖而提出了LSTM， 而 GRU 则是 LSTM 的一个变体， 当然 LSTM 还有有很多其他的变体。GRU 保持了LSTM的效果同时又使结构更加简单， 所以它也非常流行。</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/34690/1.png" style="zoom:50%;">

<p>隐含状态不断的传递， 得到 s， s 作为 decoder 的初始状态， 15-16 年非常流行的做机器翻译的框架， 不好的地方前面讲了， 离 s 比较远的稀释的比较厉害</p>
<h3 id="基于-attention-的-seq2seq"><a href="#基于-attention-的-seq2seq" class="headerlink" title="基于 attention 的 seq2seq"></a>基于 attention 的 seq2seq</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/34690/2.png" style="zoom:50%;">

<p>seq2seq是第一个提出attention机制的模型。</p>
<ul>
<li>Encoder 每一步的<strong>输出</strong>都会参与到 decoder 每一步的计算中！</li>
<li>我们会拿 <strong>encoder_outputs 和 decoder</strong> 中每一步的 hidden_state 去计算 <strong>attention</strong>。</li>
<li>Attention 理解为向量， 假如如图中所示 encoder_outputs 有 5 个， 那么 Attention 向量长度为 5， 代表输出的权重， 有了这个权重， 我们就可以拿着权重和 encoder_outputs 做加权求和， 得到 <strong>context_vector</strong>， 和 <strong>input</strong> 一块<strong>输入下一步的 decode</strong>r， 用来做词语的生成， 每一个词语都会这么做。</li>
</ul>
<p>思路便是：Encoder的encoder_outputs与decoder生成的每一个部分都加进去计算，生成对应的注意力分数，然后用于decoder的下一步生成</p>
<h3 id="模型中的注意力公式运算"><a href="#模型中的注意力公式运算" class="headerlink" title="模型中的注意力公式运算"></a>模型中的注意力公式运算</h3><ul>
<li>EO: encoder 各个位置的输出</li>
<li>H: decoder 某一步的隐含状态</li>
<li>FC: 全连接层</li>
<li>X: decoder 的一个输入</li>
<li>context: 上下文向量</li>
</ul>
<p>首先我们会得到一个 score（注意力所需要的一个分数） ， score 计算有下面两种方式</p>
<ul>
<li><strong>score = FC(tanh(FC(EO) + FC(H))) —- [Bahdanau 注意力方式]， 我们用这个</strong></li>
</ul>
<p>这里用 tanh 激活函数， 输出有正有负， 会让梯度更新的比较快， 不像 sigmiod 只有正值，<strong>score 是和 EO 长度一样的向量</strong>，tanh <strong>是激活函数</strong></p>
<ul>
<li>另一选项（就是 score 的另一种计算方法） : <strong>score = EO<em>W</em>H—-[luong 注意力]</strong></li>
</ul>
<p><strong>让 score 进行接下来处理</strong></p>
<ul>
<li>attention_weights = softmax(score, axis = 1) –就变为了权重</li>
<li>context = sum(attention_weights * EO, axis = 1) EO 是矩阵， attention_weights 是向量，context 是向量</li>
<li>final_input = concat(context, embed(x)) x 和 context 拼接在一起</li>
</ul>
<h2 id="机器翻译实战"><a href="#机器翻译实战" class="headerlink" title="机器翻译实战"></a>机器翻译实战</h2><p>首先使用一个小数据集<strong>英语与西班牙语翻译</strong>， 总计有 11 万条， 来验证我们的模型</p>
<p>seq2seq_attention 实战(Sequence-to-Sequence)</p>
<h3 id="实战步骤"><a href="#实战步骤" class="headerlink" title="实战步骤"></a>实战步骤</h3><ol>
<li><strong>preprocessing data —数据 id 化和 dataset 生成</strong><br>Tokenizer <strong>word level-Tokenizer</strong></li>
<li><strong>build model</strong><ol>
<li>encoder 构建（使用 GRU）</li>
<li>attention 构建——<strong>实现 Bahdanau —-重点， 难点</strong></li>
<li>decoder 构建：用的 lstm 变种 GRU</li>
<li>loss&amp; optimizer：自定义梯度的更新</li>
<li> train：每次 epoch 调用 train</li>
</ol>
</li>
<li><strong>evaluation （不适合看准确率，使用 bleu）</strong><ol>
<li>given sentence, return translated results</li>
<li>visualize results (attention) <strong>注意力分数的可视化</strong></li>
</ol>
</li>
</ol>
<h4 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h4><ul>
<li>去除西班牙语中的重音</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> unicodedata</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment">#因为西班牙语有一些是特殊字符，所以我们需要unicode转ascii，</span></span><br><span class="line"><span class="comment"># 这样值变小了，因为unicode太大</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unicode_to_ascii</span>(<span class="params">s</span>):</span></span><br><span class="line">    <span class="comment">#NFD是转换方法，把每一个字节拆开，Mn是重音，所以去除</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&#x27;&#x27;</span>.join(c <span class="keyword">for</span> c <span class="keyword">in</span> unicodedata.normalize(<span class="string">&#x27;NFD&#x27;</span>, s) <span class="keyword">if</span> unicodedata.category(c) != <span class="string">&#x27;Mn&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#下面我们找个样本测试一下</span></span><br><span class="line"><span class="comment"># 加u代表对字符串进行unicode编码</span></span><br><span class="line">en_sentence = <span class="string">u&quot;May I borrow this book?&quot;</span></span><br><span class="line">sp_sentence = <span class="string">u&quot;¿Puedo tomar prestado este libro?&quot;</span></span><br><span class="line"></span><br><span class="line">print(unicode_to_ascii(en_sentence))</span><br><span class="line">print(unicode_to_ascii(sp_sentence))</span><br></pre></td></tr></table></figure>
<ul>
<li>数据预处理，控制标点符号与单词分开</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_sentence</span>(<span class="params">w</span>):</span></span><br><span class="line">    <span class="comment">#变为小写，去掉多余的空格，变成小写，id少一些</span></span><br><span class="line">    w = unicode_to_ascii(w.lower().strip())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在单词与跟在其后的标点符号之间插入一个空格</span></span><br><span class="line">    <span class="comment"># eg: &quot;he is a boy.&quot; =&gt; &quot;he is a boy . &quot;</span></span><br><span class="line">    <span class="comment"># Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation</span></span><br><span class="line">    w = re.sub(<span class="string">r&quot;([?.!,¿])&quot;</span>, <span class="string">r&quot; \1 &quot;</span>, w)</span><br><span class="line">    <span class="comment">#因为可能有多余空格，替换为一个空格，所以处理一下</span></span><br><span class="line">    w = re.sub(<span class="string">r&#x27;[&quot; &quot;]+&#x27;</span>, <span class="string">&quot; &quot;</span>, w)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 除了 (a-z, A-Z, &quot;.&quot;, &quot;?&quot;, &quot;!&quot;, &quot;,&quot;)，将所有字符替换为空格</span></span><br><span class="line">    w = re.sub(<span class="string">r&quot;[^a-zA-Z?.!,¿]+&quot;</span>, <span class="string">&quot; &quot;</span>, w)</span><br><span class="line"></span><br><span class="line">    w = w.rstrip().strip()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line">print(preprocess_sentence(en_sentence))</span><br><span class="line">print(preprocess_sentence(sp_sentence))</span><br><span class="line">print(preprocess_sentence(sp_sentence).encode(<span class="string">&#x27;utf-8&#x27;</span>))  <span class="comment">#¿是占用两个字节的</span></span><br></pre></td></tr></table></figure>
<h5 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LangPairDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    fpath = Path(<span class="string">r&quot;./data_spa_en/spa.txt&quot;</span>) <span class="comment">#数据文件路径</span></span><br><span class="line">    cache_path = Path(<span class="string">r&quot;./.cache/lang_pair.npy&quot;</span>) <span class="comment">#缓存文件路径</span></span><br><span class="line">    split_index = np.random.choice(a=[<span class="string">&quot;train&quot;</span>, <span class="string">&quot;test&quot;</span>], replace=<span class="literal">True</span>, p=[<span class="number">0.9</span>, <span class="number">0.1</span>], size=<span class="number">118964</span>) <span class="comment">#按照9:1划分训练集和测试集</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, mode=<span class="string">&quot;train&quot;</span>, cache=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> cache <span class="keyword">or</span> <span class="keyword">not</span> self.cache_path.exists():<span class="comment">#如果没有缓存，或者缓存不存在，就处理一下数据</span></span><br><span class="line">            self.cache_path.parent.mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>) <span class="comment">#创建缓存文件夹，如果存在就忽略</span></span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(self.fpath, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf8&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">                lines = file.readlines()</span><br><span class="line">                lang_pair = [[preprocess_sentence(w) <span class="keyword">for</span> w <span class="keyword">in</span> l.split(<span class="string">&#x27;\t&#x27;</span>)]  <span class="keyword">for</span> l <span class="keyword">in</span> lines] <span class="comment">#处理数据，变成list((src, trg))的形式</span></span><br><span class="line">                trg, src = <span class="built_in">zip</span>(*lang_pair) <span class="comment">#分离出目标语言和源语言</span></span><br><span class="line">                trg=np.array(trg) <span class="comment">#转换为numpy数组</span></span><br><span class="line">                src=np.array(src) <span class="comment">#转换为numpy数组</span></span><br><span class="line">                np.save(self.cache_path, &#123;<span class="string">&quot;trg&quot;</span>: trg, <span class="string">&quot;src&quot;</span>: src&#125;)  <span class="comment">#保存为npy文件,方便下次直接读取,不用再处理</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            lang_pair = np.load(self.cache_path, allow_pickle=<span class="literal">True</span>).item() <span class="comment">#读取npy文件，allow_pickle=True允许读取字典</span></span><br><span class="line">            trg = lang_pair[<span class="string">&quot;trg&quot;</span>]</span><br><span class="line">            src = lang_pair[<span class="string">&quot;src&quot;</span>]</span><br><span class="line"></span><br><span class="line">        self.trg = trg[self.split_index == mode] <span class="comment">#按照index拿到训练集的 标签语言 --英语</span></span><br><span class="line">        self.src = src[self.split_index == mode] <span class="comment">#按照index拿到训练集的源语言 --西班牙</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.src[index], self.trg[index]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.src)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_ds = LangPairDataset(<span class="string">&quot;train&quot;</span>)</span><br><span class="line">test_ds = LangPairDataset(<span class="string">&quot;test&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>使用zip，将生成的list((src, trg))，分离出目标语言和源语言，并放在一起。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#zip例子</span></span><br><span class="line">a = [[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">4</span>,<span class="number">5</span>],[<span class="number">7</span>,<span class="number">8</span>]]</span><br><span class="line">zipped = <span class="built_in">list</span>(<span class="built_in">zip</span>(*a))</span><br><span class="line">print(zipped)</span><br></pre></td></tr></table></figure>
<h5 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a>Tokenizer</h5><p>这里有两种处理方式，分别对应着 encoder 和 decoder 的 word embedding 是否共享，这里实现不共享的方案。</p>
<h6 id="构建词表转换"><a href="#构建词表转换" class="headerlink" title="构建词表转换"></a>构建词表转换</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_word_idx</span>(<span class="params">ds, mode=<span class="string">&quot;src&quot;</span>, threshold=<span class="number">2</span></span>):</span></span><br><span class="line">    <span class="comment">#载入词表，看下词表长度，词表就像英语字典</span></span><br><span class="line">    word2idx = &#123;</span><br><span class="line">        <span class="string">&quot;[PAD]&quot;</span>: <span class="number">0</span>,     <span class="comment"># 填充 token</span></span><br><span class="line">        <span class="string">&quot;[BOS]&quot;</span>: <span class="number">1</span>,     <span class="comment"># begin of sentence</span></span><br><span class="line">        <span class="string">&quot;[UNK]&quot;</span>: <span class="number">2</span>,     <span class="comment"># 未知 token</span></span><br><span class="line">        <span class="string">&quot;[EOS]&quot;</span>: <span class="number">3</span>,     <span class="comment"># end of sentence</span></span><br><span class="line">    &#125;</span><br><span class="line">    idx2word = &#123;value: key <span class="keyword">for</span> key, value <span class="keyword">in</span> word2idx.items()&#125;</span><br><span class="line">    index = <span class="built_in">len</span>(idx2word)</span><br><span class="line">    threshold = <span class="number">1</span>  <span class="comment"># 出现次数低于此的token舍弃</span></span><br><span class="line">    <span class="comment">#如果数据集有很多个G，那是用for循环的，不能&#x27; &#x27;.join</span></span><br><span class="line">    word_list = <span class="string">&quot; &quot;</span>.join([pair[<span class="number">0</span> <span class="keyword">if</span> mode==<span class="string">&quot;src&quot;</span> <span class="keyword">else</span> <span class="number">1</span>] <span class="keyword">for</span> pair <span class="keyword">in</span> ds]).split()</span><br><span class="line">    counter = Counter(word_list) <span class="comment">#统计词频,counter类似字典，key是单词，value是出现次数</span></span><br><span class="line">    print(<span class="string">&quot;word count:&quot;</span>, <span class="built_in">len</span>(counter))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> token, count <span class="keyword">in</span> counter.items():</span><br><span class="line">        <span class="keyword">if</span> count &gt;= threshold:<span class="comment">#出现次数大于阈值的token加入词表</span></span><br><span class="line">            word2idx[token] = index <span class="comment">#加入词表</span></span><br><span class="line">            idx2word[index] = token <span class="comment">#加入反向词表</span></span><br><span class="line">            index += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> word2idx, idx2word</span><br><span class="line"></span><br><span class="line">src_word2idx, src_idx2word = get_word_idx(train_ds, <span class="string">&quot;src&quot;</span>) <span class="comment">#源语言词表</span></span><br><span class="line">trg_word2idx, trg_idx2word = get_word_idx(train_ds, <span class="string">&quot;trg&quot;</span>) <span class="comment">#目标语言词表</span></span><br></pre></td></tr></table></figure>
<h6 id="构建Tokenizer"><a href="#构建Tokenizer" class="headerlink" title="构建Tokenizer"></a>构建Tokenizer</h6><ul>
<li>Tokenizer按语言分开可以减少embedding_dim的大小，对于翻译任务建议分开</li>
<li>而问答任务存在多语言回答，就不能分开。</li>
<li>mask的作用：masks = (input_ids == self.pad_idx).to(dtype=torch.int64)：mask是一个和input_ids一样大小的tensor，0代表token，1代表padding，mask用于去除padding的影响</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tokenizer</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, word2idx, idx2word, max_length=<span class="number">500</span>, pad_idx=<span class="number">0</span>, bos_idx=<span class="number">1</span>, eos_idx=<span class="number">3</span>, unk_idx=<span class="number">2</span></span>):</span></span><br><span class="line">        self.word2idx = word2idx</span><br><span class="line">        self.idx2word = idx2word</span><br><span class="line">        self.max_length = max_length</span><br><span class="line">        self.pad_idx = pad_idx</span><br><span class="line">        self.bos_idx = bos_idx</span><br><span class="line">        self.eos_idx = eos_idx</span><br><span class="line">        self.unk_idx = unk_idx</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, text_list, padding_first=<span class="literal">False</span>, add_bos=<span class="literal">True</span>, add_eos=<span class="literal">True</span>, return_mask=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;如果padding_first == True，则padding加载前面，否则加载后面</span></span><br><span class="line"><span class="string">        return_mask: 是否返回mask(掩码），mask用于指示哪些是padding的，哪些是真实的token</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        max_length = <span class="built_in">min</span>(self.max_length, add_eos + add_bos + <span class="built_in">max</span>([<span class="built_in">len</span>(text) <span class="keyword">for</span> text <span class="keyword">in</span> text_list]))</span><br><span class="line">        indices_list = []</span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> text_list:</span><br><span class="line">            indices = [self.word2idx.get(word, self.unk_idx) <span class="keyword">for</span> word <span class="keyword">in</span> text[:max_length - add_bos - add_eos]] <span class="comment">#如果词表中没有这个词，就用unk_idx代替，indices是一个list,里面是每个词的index,也就是一个样本的index</span></span><br><span class="line">            <span class="keyword">if</span> add_bos:</span><br><span class="line">                indices = [self.bos_idx] + indices</span><br><span class="line">            <span class="keyword">if</span> add_eos:</span><br><span class="line">                indices = indices + [self.eos_idx]</span><br><span class="line">            <span class="keyword">if</span> padding_first:<span class="comment">#padding加载前面，超参可以调</span></span><br><span class="line">                indices = [self.pad_idx] * (max_length - <span class="built_in">len</span>(indices)) + indices</span><br><span class="line">            <span class="keyword">else</span>:<span class="comment">#padding加载后面</span></span><br><span class="line">                indices = indices + [self.pad_idx] * (max_length - <span class="built_in">len</span>(indices))</span><br><span class="line">            indices_list.append(indices)</span><br><span class="line">        input_ids = torch.tensor(indices_list) <span class="comment">#转换为tensor</span></span><br><span class="line">        masks = (input_ids == self.pad_idx).to(dtype=torch.int64) <span class="comment">#mask是一个和input_ids一样大小的tensor，0代表token，1代表padding，mask用于去除padding的影响</span></span><br><span class="line">        <span class="keyword">return</span> input_ids <span class="keyword">if</span> <span class="keyword">not</span> return_mask <span class="keyword">else</span> (input_ids, masks)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, indices_list, remove_bos=<span class="literal">True</span>, remove_eos=<span class="literal">True</span>, remove_pad=<span class="literal">True</span>, split=<span class="literal">False</span></span>):</span></span><br><span class="line">        text_list = []</span><br><span class="line">        <span class="keyword">for</span> indices <span class="keyword">in</span> indices_list:</span><br><span class="line">            text = []</span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> indices:</span><br><span class="line">                word = self.idx2word.get(index, <span class="string">&quot;[UNK]&quot;</span>) <span class="comment">#如果词表中没有这个词，就用unk_idx代替</span></span><br><span class="line">                <span class="keyword">if</span> remove_bos <span class="keyword">and</span> word == <span class="string">&quot;[BOS]&quot;</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> remove_eos <span class="keyword">and</span> word == <span class="string">&quot;[EOS]&quot;</span>:<span class="comment">#如果到达eos，就结束</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> remove_pad <span class="keyword">and</span> word == <span class="string">&quot;[PAD]&quot;</span>:<span class="comment">#如果到达pad，就结束</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                text.append(word) <span class="comment">#单词添加到列表中</span></span><br><span class="line">            text_list.append(<span class="string">&quot; &quot;</span>.join(text) <span class="keyword">if</span> <span class="keyword">not</span> split <span class="keyword">else</span> text) <span class="comment">#把列表中的单词拼接，变为一个句子</span></span><br><span class="line">        <span class="keyword">return</span> text_list</span><br><span class="line"></span><br><span class="line"><span class="comment">#两个相对于1个toknizer的好处是embedding的参数量减少</span></span><br><span class="line">src_tokenizer = Tokenizer(word2idx=src_word2idx, idx2word=src_idx2word) <span class="comment">#源语言tokenizer</span></span><br><span class="line">trg_tokenizer = Tokenizer(word2idx=trg_word2idx, idx2word=trg_idx2word) <span class="comment">#目标语言tokenizer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># trg_tokenizer.encode([[&quot;hello&quot;], [&quot;hello&quot;, &quot;world&quot;]], add_bos=True, add_eos=False,return_mask=True)</span></span><br><span class="line">raw_text = [<span class="string">&quot;hello world&quot;</span>.split(), <span class="string">&quot;tokenize text datas with batch&quot;</span>.split(), <span class="string">&quot;this is a test&quot;</span>.split()]</span><br><span class="line">indices,mask = trg_tokenizer.encode(raw_text, padding_first=<span class="literal">False</span>, add_bos=<span class="literal">True</span>, add_eos=<span class="literal">True</span>,return_mask=<span class="literal">True</span>)</span><br><span class="line">decode_text = trg_tokenizer.decode(indices.tolist(), remove_bos=<span class="literal">False</span>, remove_eos=<span class="literal">False</span>, remove_pad=<span class="literal">False</span>)</span><br><span class="line">print(<span class="string">&quot;raw text&quot;</span>+<span class="string">&#x27;-&#x27;</span>*<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> raw <span class="keyword">in</span> raw_text:</span><br><span class="line">    print(raw)</span><br><span class="line">print(<span class="string">&quot;mask&quot;</span>+<span class="string">&#x27;-&#x27;</span>*<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> mask:</span><br><span class="line">    print(m)</span><br><span class="line">print(<span class="string">&quot;indices&quot;</span>+<span class="string">&#x27;-&#x27;</span>*<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> indices:</span><br><span class="line">    print(index)</span><br><span class="line">print(<span class="string">&quot;decode text&quot;</span>+<span class="string">&#x27;-&#x27;</span>*<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> decode <span class="keyword">in</span> decode_text:</span><br><span class="line">    print(decode)</span><br></pre></td></tr></table></figure>
<h5 id="Datasetloader"><a href="#Datasetloader" class="headerlink" title="Datasetloader"></a>Datasetloader</h5><ul>
<li>训练过程中，decoder处生成一个token后，下一个token是参考的真实标签label，而不是刚刚生成的token，因为是需要计算对应位置的loss。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fct</span>(<span class="params">batch</span>):</span></span><br><span class="line">    src_words = [pair[<span class="number">0</span>].split() <span class="keyword">for</span> pair <span class="keyword">in</span> batch]</span><br><span class="line">    trg_words = [pair[<span class="number">1</span>].split() <span class="keyword">for</span> pair <span class="keyword">in</span> batch]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [PAD] [BOS] src [EOS]</span></span><br><span class="line">    encoder_inputs, encoder_inputs_mask = src_tokenizer.encode(</span><br><span class="line">        src_words, padding_first=<span class="literal">True</span>, add_bos=<span class="literal">True</span>, add_eos=<span class="literal">True</span>, return_mask=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [BOS] trg [PAD]</span></span><br><span class="line">    decoder_inputs = trg_tokenizer.encode(</span><br><span class="line">        trg_words, padding_first=<span class="literal">False</span>, add_bos=<span class="literal">True</span>, add_eos=<span class="literal">False</span>, return_mask=<span class="literal">False</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># trg [EOS] [PAD]</span></span><br><span class="line">    decoder_labels, decoder_labels_mask = trg_tokenizer.encode(</span><br><span class="line">        trg_words, padding_first=<span class="literal">False</span>, add_bos=<span class="literal">False</span>, add_eos=<span class="literal">True</span>, return_mask=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;encoder_inputs&quot;</span>: encoder_inputs.to(device=device),</span><br><span class="line">        <span class="string">&quot;encoder_inputs_mask&quot;</span>: encoder_inputs_mask.to(device=device),</span><br><span class="line">        <span class="string">&quot;decoder_inputs&quot;</span>: decoder_inputs.to(device=device),</span><br><span class="line">        <span class="string">&quot;decoder_labels&quot;</span>: decoder_labels.to(device=device),</span><br><span class="line">        <span class="string">&quot;decoder_labels_mask&quot;</span>: decoder_labels_mask.to(device=device),</span><br><span class="line">    &#125; <span class="comment">#当返回的数据较多时，用dict返回比较合理</span></span><br><span class="line">    </span><br><span class="line">sample_dl = DataLoader(train_ds, batch_size=<span class="number">2</span>, shuffle=<span class="literal">True</span>, collate_fn=collate_fct)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> sample_dl:</span><br><span class="line">    <span class="keyword">for</span> key, value <span class="keyword">in</span> batch.items():</span><br><span class="line">        print(key)</span><br><span class="line">        print(value)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>上述decoder_inputs = trg_tokenizer.encode和decoder_labels, decoder_labels_mask = trg_tokenizer.encode分别对应decoder的输入输出：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/34690/3.png" style="zoom:50%;">

<h4 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">        embedding_dim=<span class="number">256</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        hidden_dim=<span class="number">1024</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        num_layers=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        </span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, encoder_inputs</span>):</span></span><br><span class="line">        <span class="comment"># encoder_inputs.shape = [batch size, sequence length]</span></span><br><span class="line">        <span class="comment"># bs, seq_len = encoder_inputs.shape</span></span><br><span class="line">        embeds = self.embedding(encoder_inputs)</span><br><span class="line">        <span class="comment"># embeds.shape = [batch size, sequence length, embedding_dim]-&gt;[batch size, sequence length, hidden_dim]</span></span><br><span class="line">        seq_output, hidden = self.gru(embeds)</span><br><span class="line">        <span class="comment"># seq_output.shape = [batch size, sequence length, hidden_dim]，hidden.shape [ num_layers, batch size, hidden_dim]</span></span><br><span class="line">        <span class="keyword">return</span> seq_output, hidden</span><br><span class="line">      </span><br><span class="line"><span class="comment">#把上面的Encoder写一个例子，看看输出的shape</span></span><br><span class="line">encoder = Encoder(vocab_size=<span class="number">100</span>, embedding_dim=<span class="number">256</span>, hidden_dim=<span class="number">1024</span>, num_layers=<span class="number">4</span>)</span><br><span class="line">encoder_inputs = torch.randint(<span class="number">0</span>, <span class="number">100</span>, (<span class="number">2</span>, <span class="number">50</span>))</span><br><span class="line">encoder_outputs, hidden = encoder(encoder_inputs)</span><br><span class="line">print(encoder_outputs.shape)</span><br><span class="line">print(hidden.shape)</span><br><span class="line">print(encoder_outputs[:,-<span class="number">1</span>,:])</span><br><span class="line">print(hidden[-<span class="number">1</span>,:,:]) <span class="comment">#取最后一层的hidden</span></span><br></pre></td></tr></table></figure>
<ul>
<li>注意每一层计算的输出内容：def forward(self, encoder_inputs)中的各项。</li>
</ul>
<h4 id="实现-Bahdanau"><a href="#实现-Bahdanau" class="headerlink" title="实现 Bahdanau"></a>实现 Bahdanau</h4><p>Wk、Wq、V分别对应上述公式的 **score = FC(tanh(FC(EO) + FC(H))) —- [Bahdanau 注意力方式]**的 FC(EO)、FC(H)、和 FC(tanh(FC(EO)</p>
<p> 正向传播<br>        :param query: decoder的hidden state，是decoder的隐藏状态，shape = [batch size, hidden_dim] ，多层decoder时也只会拿最后一层的 [batch size, hidden_dim]<br>        :param keys: EO  [batch size, sequence length, hidden_dim]<br>        :param values: EO  [batch size, sequence length, hidden_dim]<br>        :param attn_mask:[batch size, sequence length]<br>        :return:</p>
<ul>
<li> scores = self.V(F.tanh(self.Wk(keys) + self.Wq(query.unsqueeze(-2))))</li>
<li>全连接层只会对你的最后一维做矩阵运算</li>
<li> score.shape = [batch size, sequence length, 1]：意味着对于每个句子（batch size）中的每个单词（sequence length），我们都有一个<strong>得分</strong>。每个单词的得分是通过某些机制（如注意力机制）计算出来的，可以理解为该单词对最终结果的重要性。</li>
<li><strong>values</strong>：表示与输入序列中每个单词对应的值，通常是隐藏状态向量或编码器的输出。它的形状为 <code>[batch size, sequence length, hidden_dim]</code>，其中 <code>hidden_dim</code> 是每个单词的表示维度（如 GRU 或 Transformer 中的隐藏层大小）。</li>
<li><strong>torch.mul(scores, values)**：是逐元素的乘法操作，将每个单词的 **score</strong> 与对应的 <strong>value</strong> 相乘。结果的形状将是 <code>[batch size, sequence length, hidden_dim]</code>，即对每个时间步，score 和 value 之间的关系已经建立。</li>
<li><strong>.sum(dim=-2)*<em>：是对 **scores \</em> values</strong> 的结果沿着 <strong>sequence length（即-2维度）</strong> 进行求和，得到一个 <strong>context_vector</strong>。形状变为 <code>[batch size, hidden_dim]</code>。<ul>
<li><strong>dim=-2</strong> 表示按序列的维度（时间步）求和。这意味着所有的单词的加权值（根据得分的权重）都会合并到一个向量中，即我们得到的 <strong>context_vector</strong> 是每个句子的加权平均向量。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BahdanauAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_dim=<span class="number">1024</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.Wk = nn.Linear(hidden_dim, hidden_dim) <span class="comment">#对keys做运算，encoder的输出EO</span></span><br><span class="line">        self.Wq = nn.Linear(hidden_dim, hidden_dim) <span class="comment">#对query做运算，decoder的隐藏状态</span></span><br><span class="line">        self.V = nn.Linear(hidden_dim, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, query, keys, values, attn_mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        正向传播</span></span><br><span class="line"><span class="string">        :param query: hidden state，是decoder的隐藏状态，shape = [batch size, hidden_dim]</span></span><br><span class="line"><span class="string">        :param keys: EO  [batch size, sequence length, hidden_dim]</span></span><br><span class="line"><span class="string">        :param values: EO  [batch size, sequence length, hidden_dim]</span></span><br><span class="line"><span class="string">        :param attn_mask:[batch size, sequence length]</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># query.shape = [batch size, hidden_dim] --&gt;通过unsqueeze(-2)增加维度 [batch size, 1, hidden_dim]</span></span><br><span class="line">        <span class="comment"># keys.shape = [batch size, sequence length, hidden_dim]</span></span><br><span class="line">        <span class="comment"># values.shape = [batch size, sequence length, hidden_dim]</span></span><br><span class="line">        scores = self.V(F.tanh(self.Wk(keys) + self.Wq(query.unsqueeze(-<span class="number">2</span>)))) <span class="comment">#unsqueeze(-2)增加维度</span></span><br><span class="line">        <span class="comment"># score.shape = [batch size, sequence length, 1]</span></span><br><span class="line">        <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment">#这个mask是encoder_inputs_mask，用来mask掉padding的部分,让padding部分socres为0</span></span><br><span class="line">            <span class="comment"># attn_mask is a matrix of 0/1 element,</span></span><br><span class="line">            <span class="comment"># 1 means to mask logits while 0 means do nothing</span></span><br><span class="line">            <span class="comment"># here we add -inf to the element while mask == 1</span></span><br><span class="line">            attn_mask = (attn_mask.unsqueeze(-<span class="number">1</span>)) * -<span class="number">1e16</span> <span class="comment">#在最后增加一个维度，[batch size, sequence length] --&gt; [batch size, sequence length, 1]</span></span><br><span class="line">            scores += attn_mask</span><br><span class="line">        scores = F.softmax(scores, dim=-<span class="number">2</span>) <span class="comment">#对每一个词的score做softmax</span></span><br><span class="line">        <span class="comment"># score.shape = [batch size, sequence length, 1]</span></span><br><span class="line">        context_vector = torch.mul(scores, values).<span class="built_in">sum</span>(dim=-<span class="number">2</span>) <span class="comment">#对每一个词的score和对应的value做乘法，然后在seq_len维度上求和，得到context_vector</span></span><br><span class="line">        <span class="comment"># context_vector.shape = [batch size, hidden_dim]</span></span><br><span class="line">        <span class="comment">#socres用于最后的画图</span></span><br><span class="line">        <span class="keyword">return</span> context_vector, scores</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<a id="more"></a>

]]></content>
      <categories>
        <category>GPU</category>
        <category>seq2seq</category>
        <category>attention</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>seq2seq</tag>
        <tag>attention</tag>
      </tags>
  </entry>
  <entry>
    <title>循环神经网络</title>
    <url>/7224.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="循环神经网络-RNN"><a href="#循环神经网络-RNN" class="headerlink" title="循环神经网络(RNN)"></a>循环神经网络(RNN)</h2><h3 id="为什么需要RNN"><a href="#为什么需要RNN" class="headerlink" title="为什么需要RNN"></a>为什么需要RNN</h3><p>合并+ padding 的缺点</p>
<ul>
<li>信息丢失<ul>
<li>多个 embedding 合并 （求均值）</li>
<li>Pad 噪音、 无主次 （padding 太多， 即便没有 padding， 一些重要的表达情感的词语， 没有主次）</li>
</ul>
</li>
<li>无效计算太多,低效<ul>
<li>太多的 padding</li>
</ul>
</li>
<li>序列问题：由于普通的神经网络(比如Embedding)是将多个句子词语通过全局平均池化合并成一个向量，来表示文章的积极和消极，这就导致信息丢失，除此之外，全局平均池化也没有考虑句子的语序问题，一个词语出现的先后效果也是不同的。</li>
</ul>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/7224/1.png" style="zoom:40%;">

<ul>
<li>图 1： 普通神经网络( Vanilla Neural Networks ) 一个样本对应一个输出（卷积神经网络，全连接神经网络）</li>
<li>图 2： 1 对多:图片 生成描述</li>
<li>图 3： 多对 1:文本分类(文本情感分析)</li>
<li>图 4： 多对多: encoding-decoding,机器翻译， 需要在输入最后一个结束后， 才能进行输出，是非实时的</li>
<li>图 5： 实时多对多:视频解说</li>
</ul>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/7224/2.png" style="zoom:40%;">

<p>核心： 维护一个状态作为下一步的额外输入<br>每一步使用同样的激活函数和参数 也就是 RNN 的 w 是不变的， 如下图所示</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/7224/3.png" style="zoom:30%;">

<p>这里有知乎的解释 <a href="https://zhuanlan.zhihu.com/p/30844905">https://zhuanlan.zhihu.com/p/30844905</a></p>
<h3 id="SimpleRNN"><a href="#SimpleRNN" class="headerlink" title="SimpleRNN"></a>SimpleRNN</h3><p>是一个单向单层循环神经网络</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/7224/4.png" style="zoom:40%;">

<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/7224/5.png" style="zoom:80%;">

<ul>
<li>这里的<strong>（16,64）</strong>就是对应的<strong>（embedding dim, hidden dim）</strong></li>
<li><strong>W就是（64，64），U就是（16，64）这两部份就是RNN的参数</strong></li>
</ul>
<h3 id="单向双层RNN"><a href="#单向双层RNN" class="headerlink" title="单向双层RNN"></a>单向双层RNN</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/7224/8.png" style="zoom:55%;">

<ul>
<li>RNN(num_layers=2)：num_layers置为2</li>
</ul>
<h3 id="双向单层RNN"><a href="#双向单层RNN" class="headerlink" title="双向单层RNN"></a>双向单层RNN</h3><p>单向进行的时候，最前面的词往往遗忘的更快，也即衰减的更多，所以为了保持前后的影响一致，从后到前再做一个</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/7224/7.png" style="zoom:50%;">

<ul>
<li>RNN(bidirectional=True)，bidirectional参数置为True</li>
<li>双向参数量翻倍， 用另外一组参数做逆向</li>
<li>因为是相当于使用同样的一层的参数做了一次逆向，然后将他们二者concat在一起，所以输出层就为两倍的hidden_dim</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=<span class="literal">True</span>, bidirectional=bidirectional)</span><br><span class="line">        self.layer = nn.Linear(hidden_dim * (<span class="number">2</span> <span class="keyword">if</span> bidirectional <span class="keyword">else</span> <span class="number">1</span>), hidden_dim)</span><br></pre></td></tr></table></figure>
<h3 id="双向双层RNN"><a href="#双向双层RNN" class="headerlink" title="双向双层RNN"></a>双向双层RNN</h3><ul>
<li>上面的情况相加：bidirectional参数置为True ，同时num_layers置为2</li>
</ul>
<h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><ul>
<li>这里的seq_output, final_hidden = self.rnn(x)：<ul>
<li> final_hidden为最后一个的状态输出即（1，128，64）（这里为了进行上述的矩阵相乘，是先将（128，500，16）–&gt; （500，128，16）之后进行的运算，所以最后一个状态即为（1，128，64））</li>
<li>seq_output：是上述每个过程的所有状态，即Yt-1，Yt和Yt+1所有的都在，所以其最后一个就是final_hidden，为（128，500，64）即：seq_output[:，-1，:].squeeze() == final_hidden.squeeze() </li>
<li>batch_first=True参数表示当前输入为（batch, seq，feature）,如果输入做了交换，输入为（seq，batch，feature）则应为False</li>
<li>bidirectional=False</li>
<li>num_layers=1</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embedding_dim=<span class="number">16</span>, hidden_dim=<span class="number">64</span>, vocab_size=vocab_size, num_layers=<span class="number">1</span>, bidirectional=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(RNN, self).__init__()</span><br><span class="line">        self.embeding = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=<span class="literal">True</span>, bidirectional=bidirectional)</span><br><span class="line">        self.layer = nn.Linear(hidden_dim * (<span class="number">2</span> <span class="keyword">if</span> bidirectional <span class="keyword">else</span> <span class="number">1</span>), hidden_dim)</span><br><span class="line">        self.fc = nn.Linear(hidden_dim, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># [bs, seq length]</span></span><br><span class="line">        x = self.embeding(x)</span><br><span class="line">        <span class="comment"># [bs, seq length, embedding_dim] -&gt; shape [bs, embedding_dim, seq length]</span></span><br><span class="line">        seq_output, final_hidden = self.rnn(x)</span><br><span class="line">        <span class="comment"># [bs, seq length, hidden_dim], [*, bs, hidden_dim]</span></span><br><span class="line">        x = seq_output[:, -<span class="number">1</span>, :]</span><br><span class="line">        <span class="comment"># 取最后一个时间步的输出 (这也是为什么要设置padding_first=True的原因)</span></span><br><span class="line">        x = self.layer(x)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">sample_inputs = torch.randint(<span class="number">0</span>, vocab_size, (<span class="number">2</span>, <span class="number">128</span>))</span><br><span class="line">    </span><br><span class="line">print(<span class="string">&quot;&#123;:=^80&#125;&quot;</span>.<span class="built_in">format</span>(<span class="string">&quot; 一层单向 RNN &quot;</span>))       </span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> RNN().named_parameters():</span><br><span class="line">    print(<span class="string">f&quot;<span class="subst">&#123;key:^<span class="number">40</span>&#125;</span>paramerters num: <span class="subst">&#123;np.prod(value.shape)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">print(<span class="string">&quot;&#123;:=^80&#125;&quot;</span>.<span class="built_in">format</span>(<span class="string">&quot; 一层双向 RNN &quot;</span>))       </span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> RNN(bidirectional=<span class="literal">True</span>).named_parameters():</span><br><span class="line">    print(<span class="string">f&quot;<span class="subst">&#123;key:^<span class="number">40</span>&#125;</span>paramerters num: <span class="subst">&#123;np.prod(value.shape)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">print(<span class="string">&quot;&#123;:=^80&#125;&quot;</span>.<span class="built_in">format</span>(<span class="string">&quot; 俩层单向 RNN &quot;</span>))       </span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> RNN(num_layers=<span class="number">2</span>).named_parameters():</span><br><span class="line">    print(<span class="string">f&quot;<span class="subst">&#123;key:^<span class="number">40</span>&#125;</span>paramerters num: <span class="subst">&#123;np.prod(value.shape)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/7224/6.png" style="zoom:50%;">

<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/7224/9.png" style="zoom:50%;">

<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/7224/10.png" style="zoom:50%;">

<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/7224/11.png" style="zoom:50%;">

<ul>
<li>（128，500，16）——&gt; （128，64）x（64，64）+<strong>（128，16）x（16，64）</strong></li>
</ul>
<p>​    <img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/7224/12.png" style="zoom:50%;">    </p>
<p>​            </p>
<ul>
<li>8192的来源：上述（128，64）变为（128，64）x（64，64）+<strong>（128，128）x（128，64）</strong>，所以128x64=8,192 </li>
<li>4096的来源：64x64=4,096 </li>
</ul>
<h3 id="实战分析"><a href="#实战分析" class="headerlink" title="实战分析"></a>实战分析</h3><h4 id="1-构建-rnn-模型"><a href="#1-构建-rnn-模型" class="headerlink" title="1 构建 rnn 模型"></a>1 构建 rnn 模型</h4><p>当考虑神经网络层的参数量时， 计算方法如下：</p>
<ol>
<li><p>Embedding 层：</p>
<ul>
<li>vocab_size 是词汇表大小。</li>
<li>embedding_dim 是嵌入维度。</li>
<li>参数量为 vocab_size * embedding_dim， 因为每个词汇都有一个对应的嵌入向量。</li>
</ul>
</li>
<li><p>RNN 层：</p>
<ul>
<li><p>embedding_dim 是输入维度。</p>
</li>
<li><p>hidden_dim 是隐藏层维度。</p>
</li>
<li><p>RNN 权重参数由两部分组成：</p>
<ul>
<li>weight_ih： 输入到隐藏层的权重， 大小为 (hidden_dim, embedding_dim)。</li>
<li>weight_hh： 隐藏层到隐藏层的权重， 大小为 (hidden_dim, hidden_dim)。</li>
</ul>
</li>
<li><p>RNN 偏置参数由两部分组成：</p>
<ul>
<li>bias_ih： 输入到隐藏层的偏置， 大小为 (hidden_dim,)。</li>
<li>bias_hh： 隐藏层到隐藏层的偏置， 大小为 (hidden_dim,)。</li>
</ul>
</li>
<li><p>总参数量为 hidden_dim * (hidden_dim + embedding_dim) + 2 * hidden_dim。</p>
</li>
</ul>
</li>
<li><p>全连接层 fc：</p>
<ul>
<li><p>输入维度是 hidden_dim， 输出维度是 vocab_size。</p>
</li>
<li><p>权重参数大小为 (vocab_size, hidden_dim)。</p>
</li>
<li><p>偏置参数大小为 (vocab_size,)。</p>
</li>
<li><p>参数量为 vocab_size * hidden_dim + vocab_size。</p>
</li>
</ul>
</li>
</ol>
<p>结合你提供的参数数量和上述计算方法：</p>
<ul>
<li>embedding.weight： vocab_size * embedding_dim， 即 65 * 256 = 16640。</li>
<li>rnn.weight_ih_l0： hidden_dim * embedding_dim， 即 1024 * 256 = 262144。</li>
<li>rnn.weight_hh_l0： hidden_dim * hidden_dim， 即 1024 * 1024 = 1048576。</li>
<li>rnn.bias_ih_l0 和 rnn.bias_hh_l0： 每个都有 hidden_dim 个参数， 即 1024 + 1024 = 2048。</li>
<li>fc.weight： vocab_size * hidden_dim， 即 65 * 1024 = 66560。</li>
<li>fc.bias： vocab_size， 即 65</li>
</ul>
<h4 id="BRNN-的应用包括："><a href="#BRNN-的应用包括：" class="headerlink" title="BRNN 的应用包括："></a>BRNN 的应用包括：</h4><p>语音识别（与长时记忆结合）、翻译、手写识别、蛋白质结构预测、词性标记、依赖解析、实体提取</p>
<h3 id="文本生成"><a href="#文本生成" class="headerlink" title="文本生成"></a>文本生成</h3><p>使用莎士比亚文集做文本生成</p>
<h4 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!wget https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./shakespeare.txt&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf8&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    text = file.read()</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;length&quot;</span>, <span class="built_in">len</span>(text))</span><br><span class="line">print(text[<span class="number">0</span>:<span class="number">100</span>])</span><br></pre></td></tr></table></figure>
<h4 id="构造字典"><a href="#构造字典" class="headerlink" title="构造字典"></a>构造字典</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. generate vocab</span></span><br><span class="line"><span class="comment"># 2. build mapping char-&gt;id</span></span><br><span class="line"><span class="comment"># 3. data -&gt; id_data  把数据都转为id</span></span><br><span class="line"><span class="comment"># 4. a b c d [EOS] -&gt; [BOS] b c d  预测下一个字符生成的模型，也就是输入是a，输出就是b</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#去重，留下独立字符，并排序（排序是为了好看）</span></span><br><span class="line">vocab = <span class="built_in">sorted</span>(<span class="built_in">set</span>(text))</span><br><span class="line">print(<span class="built_in">len</span>(vocab))</span><br><span class="line">print(vocab)</span><br><span class="line"></span><br><span class="line"><span class="comment">#每个字符都编好号，enumerate对每一个位置编号，生成的是列表中是元组，下面字典生成式</span></span><br><span class="line">char2idx = &#123;char:idx <span class="keyword">for</span> idx, char <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br><span class="line">print(char2idx)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把vocab从列表变为ndarray</span></span><br><span class="line">idx2char = np.array(vocab)</span><br><span class="line">print(idx2char)</span><br><span class="line"></span><br><span class="line"><span class="comment">#把字符都转换为id</span></span><br><span class="line">text_as_int = np.array([char2idx[c] <span class="keyword">for</span> c <span class="keyword">in</span> text])</span><br><span class="line">print(text_as_int.shape)</span><br><span class="line">print(<span class="built_in">len</span>(text_as_int))</span><br><span class="line">print(text_as_int[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line">print(text[<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<h4 id="Dataset与Dataloader的构建"><a href="#Dataset与Dataloader的构建" class="headerlink" title="Dataset与Dataloader的构建"></a>Dataset与Dataloader的构建</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CharDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="comment">#text_as_int是字符的id列表，seq_length是每个样本的长度</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, text_as_int, seq_length</span>):</span></span><br><span class="line">        self.sub_len = seq_length + <span class="number">1</span> <span class="comment">#一个样本的长度</span></span><br><span class="line">        self.text_as_int = text_as_int</span><br><span class="line">        self.num_seq = <span class="built_in">len</span>(text_as_int) // self.sub_len <span class="comment">#样本的个数</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span><span class="comment">#index是样本的索引，返回的是一个样本，比如第一个，就是0-100的字符,总计101个字符</span></span><br><span class="line">        <span class="keyword">return</span> self.text_as_int[index * self.sub_len: (index + <span class="number">1</span>) * self.sub_len]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span> <span class="comment">#返回样本的个数</span></span><br><span class="line">        <span class="keyword">return</span> self.num_seq</span><br><span class="line"></span><br><span class="line"><span class="comment">#batch是一个列表，列表中的每一个元素是一个样本，有101个字符，前100个是输入，后100个是输出</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collat_fct</span>(<span class="params">batch</span>):</span></span><br><span class="line">    src_list = [] <span class="comment">#输入</span></span><br><span class="line">    trg_list = [] <span class="comment">#输出</span></span><br><span class="line">    <span class="keyword">for</span> part <span class="keyword">in</span> batch:</span><br><span class="line">        src_list.append(part[:-<span class="number">1</span>]) <span class="comment">#输入</span></span><br><span class="line">        trg_list.append(part[<span class="number">1</span>:]) <span class="comment">#输出</span></span><br><span class="line">        </span><br><span class="line">    src_list = np.array(src_list) <span class="comment">#把列表转换为ndarray</span></span><br><span class="line">    trg_list = np.array(trg_list) <span class="comment">#把列表转换为ndarray</span></span><br><span class="line">    <span class="keyword">return</span> torch.Tensor(src_list).to(dtype=torch.int64), torch.Tensor(trg_list).to(dtype=torch.int64) <span class="comment">#返回的是一个元组，元组中的每一个元素是一个torch.Tensor</span></span><br><span class="line">        </span><br><span class="line"><span class="comment">#每个样本的长度是101，也就是100个字符+1个结束符</span></span><br><span class="line">train_ds = CharDataset(text_as_int, <span class="number">100</span>)</span><br><span class="line">train_dl = DataLoader(train_ds, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>, collate_fn=collat_fct)</span><br></pre></td></tr></table></figure>
<h4 id="定义模型-1"><a href="#定义模型-1" class="headerlink" title="定义模型"></a>定义模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CharRNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim=<span class="number">256</span>, hidden_dim=<span class="number">1024</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CharRNN, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        <span class="comment">#batch_first=True,输入的数据格式是(batch_size, seq_len, embedding_dim)</span></span><br><span class="line">        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.fc = nn.Linear(hidden_dim, vocab_size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, hidden=<span class="literal">None</span></span>):</span></span><br><span class="line">        x = self.embedding(x) <span class="comment">#(batch_size, seq_len) -&gt; (batch_size, seq_len, embedding_dim) (64, 100, 256)</span></span><br><span class="line">        <span class="comment">#这里和02的差异是没有只拿最后一个输出，而是把所有的输出都拿出来了</span></span><br><span class="line">        <span class="comment">#(batch_size, seq_len, embedding_dim)-&gt;(batch_size, seq_len, hidden_dim)(64, 100, 1024)</span></span><br><span class="line">        output, hidden = self.rnn(x, hidden)</span><br><span class="line">        x = self.fc(output) <span class="comment">#[bs, seq_len, hidden_dim]---&gt;[bs, seq_len, vocab_size] (64, 100,65)</span></span><br><span class="line">        <span class="keyword">return</span> x, hidden <span class="comment">#x的shape是(batch_size, seq_len, vocab_size)</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">vocab_size = <span class="built_in">len</span>(vocab)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;&#123;:=^80&#125;&quot;</span>.<span class="built_in">format</span>(<span class="string">&quot; 一层单向 RNN &quot;</span>))       </span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> CharRNN(vocab_size).named_parameters():</span><br><span class="line">    print(<span class="string">f&quot;<span class="subst">&#123;key:^<span class="number">40</span>&#125;</span>paramerters num: <span class="subst">&#123;np.prod(value.shape)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/7224/13.png" style="zoom:60%;">

<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">    model, </span></span></span><br><span class="line"><span class="function"><span class="params">    train_loader, </span></span></span><br><span class="line"><span class="function"><span class="params">    epoch, </span></span></span><br><span class="line"><span class="function"><span class="params">    loss_fct, </span></span></span><br><span class="line"><span class="function"><span class="params">    optimizer, </span></span></span><br><span class="line"><span class="function"><span class="params">    save_ckpt_callback=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    stateful=<span class="literal">False</span>      <span class="comment"># 想用stateful，batch里的数据就必须连续，不能打乱</span></span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">    record_dict = &#123;</span><br><span class="line">        <span class="string">&quot;train&quot;</span>: [],</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    global_step = <span class="number">0</span></span><br><span class="line">    model.train()</span><br><span class="line">    hidden = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">with</span> tqdm(total=epoch * <span class="built_in">len</span>(train_loader)) <span class="keyword">as</span> pbar:</span><br><span class="line">        <span class="keyword">for</span> epoch_id <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">            <span class="comment"># training</span></span><br><span class="line">            <span class="keyword">for</span> datas, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">                datas = datas.to(device)</span><br><span class="line">                labels = labels.to(device)</span><br><span class="line">                <span class="comment"># 梯度清空</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">                <span class="comment"># 模型前向计算,如果数据集打乱了，stateful=False，hidden就要清空</span></span><br><span class="line">                <span class="comment"># 如果数据集没有打乱，stateful=True，hidden就不需要清空</span></span><br><span class="line">                logits, hidden = model(datas, hidden=hidden <span class="keyword">if</span> stateful <span class="keyword">else</span> <span class="literal">None</span>)</span><br><span class="line">                <span class="comment"># 计算损失,交叉熵损失第一个参数要是二阶张量，第二个参数要是一阶张量，所以要reshape</span></span><br><span class="line">                loss = loss_fct(logits.reshape(-<span class="number">1</span>, vocab_size), labels.reshape(-<span class="number">1</span>))</span><br><span class="line">                <span class="comment"># 梯度回传</span></span><br><span class="line">                loss.backward()</span><br><span class="line">                <span class="comment"># 调整优化器，包括学习率的变动等</span></span><br><span class="line">                optimizer.step()</span><br><span class="line"> </span><br><span class="line">                loss = loss.cpu().item()</span><br><span class="line">                <span class="comment"># record</span></span><br><span class="line">                </span><br><span class="line">                record_dict[<span class="string">&quot;train&quot;</span>].append(&#123;</span><br><span class="line">                    <span class="string">&quot;loss&quot;</span>: loss, <span class="string">&quot;step&quot;</span>: global_step</span><br><span class="line">                &#125;)</span><br><span class="line">   </span><br><span class="line">                <span class="comment"># 保存模型权重 save model checkpoint</span></span><br><span class="line">                <span class="keyword">if</span> save_ckpt_callback <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    save_ckpt_callback(global_step, model.state_dict(), metric=-loss)</span><br><span class="line">                <span class="comment"># udate step</span></span><br><span class="line">                global_step += <span class="number">1</span></span><br><span class="line">                pbar.update(<span class="number">1</span>)</span><br><span class="line">                pbar.set_postfix(&#123;<span class="string">&quot;epoch&quot;</span>: epoch_id&#125;)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> record_dict</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">epoch = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">model = CharRNN(vocab_size=vocab_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 定义损失函数 采用交叉熵损失 </span></span><br><span class="line">loss_fct = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 2. 定义优化器 采用 adam</span></span><br><span class="line"><span class="comment"># Optimizers specified in the torch.optim package</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># save best</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&quot;checkpoints&quot;</span>):</span><br><span class="line">    os.makedirs(<span class="string">&quot;checkpoints&quot;</span>)</span><br><span class="line">save_ckpt_callback = SaveCheckpointsCallback(<span class="string">&quot;checkpoints/text_generation&quot;</span>, save_step=<span class="number">1000</span>, save_best_only=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = model.to(device)</span><br><span class="line"></span><br><span class="line">record = training(</span><br><span class="line">    model,</span><br><span class="line">    train_dl,</span><br><span class="line">    epoch,</span><br><span class="line">    loss_fct,</span><br><span class="line">    optimizer,</span><br><span class="line">    save_ckpt_callback=save_ckpt_callback,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<ul>
<li>如果数据集没有打乱，stateful=True，hidden就不需要清空</li>
<li>计算损失,交叉熵损失第一个参数要是二阶张量，第二个参数要是一阶张量，所以要reshape：<ul>
<li>第一个参数将原本的（3，100，64）——&gt;（300，64）</li>
<li>第二个参数将原本的（64，100）——&gt;（6400）</li>
</ul>
</li>
</ul>
<h4 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个概率分布，表示每个类别被选中的概率</span></span><br><span class="line"><span class="comment"># 这里我们有一个简单的四个类别的概率分布</span></span><br><span class="line">prob_dist = torch.tensor([<span class="number">0.1</span>, <span class="number">0.45</span>, <span class="number">0.35</span>, <span class="number">0.1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 multinomial 进行抽样</span></span><br><span class="line"><span class="comment"># num_samples 表示要抽取的样本数量</span></span><br><span class="line">num_samples = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 抽取样本，随机抽样，概率越高，抽到的概率就越高</span></span><br><span class="line">samples = torch.multinomial(prob_dist, <span class="number">1</span>, replacement=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;概率分布:&quot;</span>, prob_dist)</span><br><span class="line">print(<span class="string">&quot;抽取的样本索引:&quot;</span>, samples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示每个样本对应的概率</span></span><br><span class="line">print(<span class="string">&quot;每个样本对应的概率:&quot;</span>, prob_dist[samples])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_text</span>(<span class="params">model, start_string, max_len=<span class="number">1000</span>, temperature=<span class="number">1.0</span>, stream=<span class="literal">True</span></span>):</span></span><br><span class="line">    input_eval = torch.Tensor([char2idx[char] <span class="keyword">for</span> char <span class="keyword">in</span> start_string]).to(dtype=torch.int64, device=device).reshape(<span class="number">1</span>, -<span class="number">1</span>) <span class="comment">#bacth_size=1, seq_len长度是多少都可以 （1,5）</span></span><br><span class="line">    hidden = <span class="literal">None</span></span><br><span class="line">    text_generated = [] <span class="comment">#用来保存生成的文本</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    pbar = tqdm(<span class="built_in">range</span>(max_len)) <span class="comment"># 进度条</span></span><br><span class="line">    print(start_string, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line">    <span class="comment"># no_grad是一个上下文管理器，用于指定在其中的代码块中不需要计算梯度。在这个区域内，不会记录梯度信息，用于在生成文本时不影响模型权重。</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> pbar:<span class="comment">#控制进度条</span></span><br><span class="line">            logits, hidden = model(input_eval, hidden=hidden)</span><br><span class="line">            <span class="comment"># 温度采样，较高的温度会增加预测结果的多样性，较低的温度则更加保守。</span></span><br><span class="line">            <span class="comment">#取-1的目的是只要最后，拼到原有的输入上</span></span><br><span class="line">            logits = logits[<span class="number">0</span>, -<span class="number">1</span>, :] / temperature</span><br><span class="line">            <span class="comment"># using multinomial to sampling</span></span><br><span class="line">            probs = F.softmax(logits, dim=-<span class="number">1</span>) <span class="comment">#算为概率分布</span></span><br><span class="line">            idx = torch.multinomial(probs, <span class="number">1</span>).item() <span class="comment">#从概率分布中抽取一个样本,取概率较大的那些</span></span><br><span class="line">            input_eval = torch.Tensor([idx]).to(dtype=torch.int64, device=device).reshape(<span class="number">1</span>, -<span class="number">1</span>) <span class="comment">#把idx转为tensor</span></span><br><span class="line">            text_generated.append(idx)</span><br><span class="line">            <span class="keyword">if</span> stream:</span><br><span class="line">                print(idx2char[idx], end=<span class="string">&quot;&quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join([idx2char[i] <span class="keyword">for</span> i <span class="keyword">in</span> text_generated])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># load checkpoints</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&quot;checkpoints/text_generation/best.ckpt&quot;</span>, map_location=<span class="string">&quot;cpu&quot;</span>))</span><br><span class="line">start_string = <span class="string">&quot;All: &quot;</span> <span class="comment">#这里就是开头，什么都可以</span></span><br><span class="line">res = generate_text(model, start_string, max_len=<span class="number">1000</span>, temperature=<span class="number">0.5</span>, stream=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>logits = logits[0, -1, :] / temperature：</p>
<ul>
<li><p>除 temperature之后，softmax的概率值越接近，每个生成越随机。</p>
</li>
<li><p>当 temperature 较小（接近 0）时，模型生成的输出会更加 <strong>确定</strong> 和 <strong>保守</strong>。</p>
<p>模型会倾向于选择 <strong>概率较高的单词</strong>，而 <strong>低概率的单词</strong> 基本不会被选中。</p>
</li>
<li><p>当 temperature 较大（比如大于 1）时，模型生成的输出会更加 <strong>多样化</strong> 和 <strong>随机</strong>。</p>
<p>温度较大时，生成时概率分布变得更加平滑，低概率的单词也有机会被选择，从而 <strong>增加了生成文本的多样性</strong>。</p>
</li>
</ul>
</li>
</ul>
<a id="more"></a>

]]></content>
      <categories>
        <category>深度学习</category>
        <category>RNN</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>大模型</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title>长短期记忆网络LSTM</title>
    <url>/21398.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="长短期记忆网络-LSTM"><a href="#长短期记忆网络-LSTM" class="headerlink" title="长短期记忆网络 LSTM"></a>长短期记忆网络 LSTM</h2><h3 id="为什么需要-LSTM-（Long-short-term-memory）"><a href="#为什么需要-LSTM-（Long-short-term-memory）" class="headerlink" title="为什么需要 LSTM （Long short-term memory）"></a>为什么需要 LSTM （Long short-term memory）</h3><p>普通 RNN 的信息不能长久传播(存在于理论上)—原因是针对结尾较远的信息被稀释的比较厉害</p>
<ul>
<li><p><strong>引入选择性机制（门机制）</strong></p>
<ul>
<li><p>选择性输入</p>
</li>
<li><p>选择性遗忘</p>
</li>
<li><p>选择性输出</p>
</li>
</ul>
</li>
</ul>
<p>选择性机制实现的原理 ——&gt; 门：Sigmoid 函数: [0,1]</p>
<p>LSTM（长短时记忆） 是一种循环神经网络， 具有门控结构， 用于处理序列数据。其运算过程可以概括为以下几个步骤：</p>
<ol>
<li>输入门（Input Gate） ： 计算当前输入和前一时刻的输出是否应该被记忆。</li>
<li>遗忘门（Forget Gate） ： 决定前一时刻输出， 结合当前的输入， 哪些记忆是否被保留。</li>
<li>记忆单元（Memory Cell） ： 根据输入门和遗忘门的结果更新记忆状态。</li>
<li>输出门（Output Gate） ： 基于当前输入和记忆状态计算当前时刻的输出。</li>
</ol>
<p><a href="https://www.bilibili.com/video/BV1Z34y1k7mc/?spm_id_from=333.1387.search.video_card.click&amp;vd_source=c9b1c252315e6753ab148ae6b39a7dc3">https://www.bilibili.com/video/BV1Z34y1k7mc/?spm_id_from=333.1387.search.video_card.click&amp;vd_source=c9b1c252315e6753ab148ae6b39a7dc3</a></p>
<p><a href="https://blog.csdn.net/weixin_44162104/article/details/88660003">https://blog.csdn.net/weixin_44162104/article/details/88660003</a></p>
<h3 id="LSTM-图示"><a href="#LSTM-图示" class="headerlink" title="LSTM 图示"></a>LSTM 图示</h3><p>以下图来进行理解：</p>
<ul>
<li>遗忘门：用于使用sigmoid，取值在（0，1），删去Ct-1中的取值为0的元素，相当于选择性遗忘了部分记忆</li>
<li>输入门：第一部份进行选择，然后使用tanh，取值在（-1，1），不是遗忘而是进行梳理归纳，并写入C中</li>
<li>Ct=f1 * Ct-1 + f2：先相乘再相加，更新了Ct。用于传递以及更新St等到Yt</li>
<li><strong>可以理解为在短期记忆的RNN上（St这条竖线），加上了一个长期记忆的节点（Ct这条竖线线）中间的交互即为，遗忘门和输入门。负责删去和写入内容。并对Yt进行影响</strong></li>
</ul>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/21398/1.png" style="zoom:40%;">

<p>对应的图示：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/21398/2.png" style="zoom:40%;">

<h3 id="LSTM-公式"><a href="#LSTM-公式" class="headerlink" title="LSTM 公式"></a>LSTM 公式</h3><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/21398/3.png" style="zoom:50%;">

<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/21398/4.png" style="zoom:50%;">

<ul>
<li>从上面公式可以看出， 参数量是 rnn 的 4 倍</li>
<li>Ht-1 和 xt 拼接是直接加起来的</li>
<li><a href="https://www.cnblogs.com/picassooo/p/13504533.html">LSTM的计算过程，权重参数数目，weight_ih_l0，weight_hh_l0</a></li>
<li>源码 lstm 是继承 rnn 实现的</li>
<li>RNN 没有细胞状态； LSTM 通过细胞状态记忆信息。RNN 激活函数只有 tanh； LSTM 通过输入门、 遗忘门、 输出门引入 sigmoid 函数并结合 tanh 函数， 添加求和操作， 减少梯度消失和梯度爆炸的可能性。RNN 只能够处理短期依赖问题； LSTM 既能够处理短期依赖问题， 又能够处理长期依赖问题。</li>
</ul>
<h3 id="文本分类"><a href="#文本分类" class="headerlink" title="文本分类"></a>文本分类</h3><ul>
<li>代码和RNN相同，只是在nn.RNN改为nn.LSTM。embedding_dim和hidden_dim和RNN一样。</li>
<li> <strong>参数量是 RNN 的 4 倍</strong>：对应的四个W[ht-1,xt] （4x16x64）（4x64x64） 和 四个b </li>
</ul>
<p>让我们来解析这些参数的数量和计算方式：</p>
<ol>
<li><p>embedding.weight：</p>
<ul>
<li><p>vocab_size 是词汇表大小。</p>
</li>
<li><p>embedding_dim 是嵌入维度。</p>
</li>
<li><p>参数量为 vocab_size * embedding_dim</p>
</li>
</ul>
</li>
<li><p>lstm.weight_ih_l0 和 lstm.weight_hh_l0：</p>
<ul>
<li>embedding_dim 是输入维度。</li>
<li>hidden_dim 是 LSTM 隐藏层的维度。</li>
<li>对于单向 LSTM：<ul>
<li>weight_ih 是输入到隐藏层的权重， 大小为 (4 * hidden_dim, embedding_dim)。</li>
<li>weight_hh 是隐藏层到隐藏层的权重， 大小为 (4 * hidden_dim, hidden_dim)。</li>
</ul>
</li>
<li>对于双向 LSTM：<ul>
<li>weight_ih 是输入到隐藏层的权重， 大小为 (4 * hidden_dim, embedding_dim)。</li>
<li>weight_hh 是隐藏层到隐藏层的权重， 大小为 (4 * hidden_dim, hidden_dim)。</li>
</ul>
</li>
<li>参数量计算为 (4 * hidden_dim * (embedding_dim + hidden_dim))。</li>
</ul>
</li>
<li><p>lstm.bias_ih_l0 和 lstm.bias_hh_l0：</p>
<ul>
<li>LSTM 层的偏置参数。</li>
<li>对于单向 LSTM， 每个都有 4 * hidden_dim 个参数。</li>
<li>对于双向 LSTM， 每个都有 8 * hidden_dim 个参数。</li>
<li>参数量计算为 4 * hidden_dim 或 8 * hidden_dim。</li>
</ul>
</li>
<li><p>layer.weight：</p>
<ul>
<li>将隐藏状态维度从 hidden_dim * (2 if bidirectional else 1) 转换为 hidden_dim。</li>
<li>参数量计算为 hidden_dim * hidden_dim。</li>
</ul>
</li>
<li><p>layer.bias：</p>
<ul>
<li>线性层的偏置参数。</li>
<li>参数量计算为 hidden_dim。</li>
</ul>
</li>
<li><p>fc.weight：</p>
<ul>
<li>输入维度是 hidden_dim， 输出维度是 1。</li>
<li>参数量计算为 hidden_dim * 1 = hidden_dim。</li>
</ul>
</li>
<li><p>fc.bias：</p>
<ul>
<li>输出层的偏置参数。</li>
<li>参数量计算为 1</li>
</ul>
</li>
</ol>
<p>根据提供的参数数量和上述计算方式：</p>
<ul>
<li>embedding.weight： vocab_size * embedding_dim = 10000 * 16 = 160000。</li>
<li><strong>lstm.weight_ih_l0</strong> 和 <strong>lstm.weight_hh_l0</strong> ： 4 * hidden_dim * (embedding_dim + hidden_dim)， 即 4 * 64 * 16 = 4096 和 4 * 64 * 64 = 16384。</li>
<li><strong>lstm.bias_ih_l0</strong> 和 <strong>lstm.bias_hh_l0</strong>： 4 * hidden_dim = 256。</li>
<li>layer.weight： hidden_dim * hidden_dim = 64 * 64 = 4096。</li>
<li>layer.bias： hidden_dim = 64。</li>
<li>fc.weight： hidden_dim = 64。</li>
<li>fc.bias： 1。</li>
</ul>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><h4 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="comment">#用karas有的数据集imdb，电影分类,分电影是积极的，还是消极的</span></span><br><span class="line">imdb = keras.datasets.imdb</span><br><span class="line"><span class="comment">#载入数据使用下面两个参数</span></span><br><span class="line">vocab_size = <span class="number">10000</span>  <span class="comment">#词典大小，仅保留训练数据中前10000个最经常出现的单词，低频单词被舍弃</span></span><br><span class="line">index_from = <span class="number">3</span>  <span class="comment">#0,1,2,3空出来做别的事</span></span><br><span class="line"><span class="comment">#前一万个词出现词频最高的会保留下来进行处理，后面的作为特殊字符处理，</span></span><br><span class="line"><span class="comment"># 小于3的id都是特殊字符，下面代码有写</span></span><br><span class="line"><span class="comment"># 需要注意的一点是取出来的词表还是从1开始的，需要做处理</span></span><br><span class="line">(train_data, train_labels), (test_data, test_labels) = imdb.load_data(</span><br><span class="line">    num_words = vocab_size, index_from = index_from)</span><br><span class="line"></span><br><span class="line"><span class="comment">#载入词表，看下词表长度，词表就像英语字典</span></span><br><span class="line">word_index = imdb.get_word_index()</span><br><span class="line">print(<span class="built_in">len</span>(word_index))</span><br><span class="line">print(<span class="built_in">type</span>(word_index))</span><br><span class="line"><span class="comment">#词表虽然有8万多，但是我们只载入了最高频的1万词！！！！</span></span><br></pre></td></tr></table></figure>
<h4 id="构造-word2idx-和-idx2word"><a href="#构造-word2idx-和-idx2word" class="headerlink" title="构造 word2idx 和 idx2word"></a>构造 word2idx 和 idx2word</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word2idx = &#123;word: idx + <span class="number">3</span> <span class="keyword">for</span> word, idx <span class="keyword">in</span> word_index.items()&#125;</span><br><span class="line">word2idx.update(&#123;</span><br><span class="line">    <span class="string">&quot;[PAD]&quot;</span>: <span class="number">0</span>,     <span class="comment"># 填充 token</span></span><br><span class="line">    <span class="string">&quot;[BOS]&quot;</span>: <span class="number">1</span>,     <span class="comment"># begin of sentence</span></span><br><span class="line">    <span class="string">&quot;[UNK]&quot;</span>: <span class="number">2</span>,     <span class="comment"># 未知 token</span></span><br><span class="line">    <span class="string">&quot;[EOS]&quot;</span>: <span class="number">3</span>,     <span class="comment"># end of sentence</span></span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line">idx2word = &#123;idx: word <span class="keyword">for</span> word, idx <span class="keyword">in</span> word2idx.items()&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择 max_length</span></span><br><span class="line">length_collect = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> train_data:</span><br><span class="line">    length = <span class="built_in">len</span>(text)</span><br><span class="line">    length_collect[length] = length_collect.get(length, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">MAX_LENGTH = <span class="number">500</span></span><br><span class="line">plt.bar(length_collect.keys(), length_collect.values())</span><br><span class="line">plt.axvline(MAX_LENGTH, label=<span class="string">&quot;max length&quot;</span>, c=<span class="string">&quot;gray&quot;</span>, ls=<span class="string">&quot;:&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h4 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a>Tokenizer</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tokenizer</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, word2idx, idx2word, max_length=<span class="number">500</span>, pad_idx=<span class="number">0</span>, bos_idx=<span class="number">1</span>, eos_idx=<span class="number">3</span>, unk_idx=<span class="number">2</span></span>):</span></span><br><span class="line">        self.word2idx = word2idx</span><br><span class="line">        self.idx2word = idx2word</span><br><span class="line">        self.max_length = max_length</span><br><span class="line">        self.pad_idx = pad_idx</span><br><span class="line">        self.bos_idx = bos_idx</span><br><span class="line">        self.eos_idx = eos_idx</span><br><span class="line">        self.unk_idx = unk_idx</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, text_list, padding_first=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;如果padding_first == True，则padding加载前面，否则加载后面&quot;&quot;&quot;</span></span><br><span class="line">        max_length = <span class="built_in">min</span>(self.max_length, <span class="number">2</span> + <span class="built_in">max</span>([<span class="built_in">len</span>(text) <span class="keyword">for</span> text <span class="keyword">in</span> text_list]))</span><br><span class="line">        indices_list = []</span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> text_list:</span><br><span class="line">            indices = [self.bos_idx] + [self.word2idx.get(word, self.unk_idx) <span class="keyword">for</span> word <span class="keyword">in</span> text[:max_length-<span class="number">2</span>]] + [self.eos_idx]</span><br><span class="line">            <span class="keyword">if</span> padding_first:</span><br><span class="line">                indices = [self.pad_idx] * (max_length - <span class="built_in">len</span>(indices)) + indices</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                indices = indices + [self.pad_idx] * (max_length - <span class="built_in">len</span>(indices))</span><br><span class="line">            indices_list.append(indices)</span><br><span class="line">        <span class="keyword">return</span> torch.tensor(indices_list)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, indices_list, remove_bos=<span class="literal">True</span>, remove_eos=<span class="literal">True</span>, remove_pad=<span class="literal">True</span>, split=<span class="literal">False</span></span>):</span></span><br><span class="line">        text_list = []</span><br><span class="line">        <span class="keyword">for</span> indices <span class="keyword">in</span> indices_list:</span><br><span class="line">            text = []</span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> indices:</span><br><span class="line">                word = self.idx2word.get(index, <span class="string">&quot;[UNK]&quot;</span>)</span><br><span class="line">                <span class="keyword">if</span> remove_bos <span class="keyword">and</span> word == <span class="string">&quot;[BOS]&quot;</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> remove_eos <span class="keyword">and</span> word == <span class="string">&quot;[EOS]&quot;</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> remove_pad <span class="keyword">and</span> word == <span class="string">&quot;[PAD]&quot;</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                text.append(word)</span><br><span class="line">            text_list.append(<span class="string">&quot; &quot;</span>.join(text) <span class="keyword">if</span> <span class="keyword">not</span> split <span class="keyword">else</span> text)</span><br><span class="line">        <span class="keyword">return</span> text_list</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer(word2idx=word2idx, idx2word=idx2word)</span><br><span class="line">raw_text = [<span class="string">&quot;hello world&quot;</span>.split(), <span class="string">&quot;tokenize text datas with batch&quot;</span>.split(), <span class="string">&quot;this is a test&quot;</span>.split()]</span><br><span class="line">indices = tokenizer.encode(raw_text, padding_first=<span class="literal">True</span>)</span><br><span class="line">decode_text = tokenizer.decode(indices.tolist(), remove_bos=<span class="literal">False</span>, remove_eos=<span class="literal">False</span>, remove_pad=<span class="literal">False</span>)</span><br><span class="line">print(<span class="string">&quot;raw text&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> raw <span class="keyword">in</span> raw_text:</span><br><span class="line">    print(raw)</span><br><span class="line">print(<span class="string">&quot;indices&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> indices:</span><br><span class="line">    print(index)</span><br><span class="line">print(<span class="string">&quot;decode text&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> decode <span class="keyword">in</span> decode_text:</span><br><span class="line">    print(decode)</span><br></pre></td></tr></table></figure>
<h4 id="数据集与-DataLoader"><a href="#数据集与-DataLoader" class="headerlink" title="数据集与 DataLoader"></a>数据集与 DataLoader</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IMDBDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, data, labels, remain_length=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> remain_length:</span><br><span class="line">            self.data = tokenizer.decode(data, remove_bos=<span class="literal">False</span>, remove_eos=<span class="literal">False</span>, remove_pad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 缩减一下数据</span></span><br><span class="line">            self.data = tokenizer.decode(data)</span><br><span class="line">        self.labels = labels</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        text = self.data[index]</span><br><span class="line">        label = self.labels[index]</span><br><span class="line">        <span class="keyword">return</span> text, label</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fct</span>(<span class="params">batch</span>):</span></span><br><span class="line">    text_list = [item[<span class="number">0</span>].split() <span class="keyword">for</span> item <span class="keyword">in</span> batch]</span><br><span class="line">    label_list = [item[<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> batch]</span><br><span class="line">    <span class="comment"># 这里使用 padding first</span></span><br><span class="line">    text_list = tokenizer.encode(text_list, padding_first=<span class="literal">True</span>).to(dtype=torch.<span class="built_in">int</span>)</span><br><span class="line">    <span class="keyword">return</span> text_list, torch.tensor(label_list).reshape(-<span class="number">1</span>, <span class="number">1</span>).to(dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用RNN，缩短序列长度</span></span><br><span class="line">train_ds = IMDBDataset(train_data, train_labels, remain_length=<span class="literal">False</span>)</span><br><span class="line">test_ds = IMDBDataset(test_data, test_labels, remain_length=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=<span class="literal">True</span>, collate_fn=collate_fct)</span><br><span class="line">test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=<span class="literal">False</span>, collate_fn=collate_fct)</span><br></pre></td></tr></table></figure>
<h4 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTM</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embedding_dim=<span class="number">16</span>, hidden_dim=<span class="number">64</span>, vocab_size=vocab_size, num_layers=<span class="number">1</span>, bidirectional=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LSTM, self).__init__()</span><br><span class="line">        self.embeding = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=<span class="literal">True</span>, bidirectional=bidirectional)</span><br><span class="line">        self.layer = nn.Linear(hidden_dim * (<span class="number">2</span> <span class="keyword">if</span> bidirectional <span class="keyword">else</span> <span class="number">1</span>), hidden_dim)</span><br><span class="line">        self.fc = nn.Linear(hidden_dim, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># [bs, seq length]</span></span><br><span class="line">        x = self.embeding(x)</span><br><span class="line">        <span class="comment"># [bs, seq length, embedding_dim] -&gt; shape [bs, embedding_dim, seq length]</span></span><br><span class="line">        seq_output, (hidden, cell) = self.lstm(x)</span><br><span class="line">        <span class="comment"># [bs, seq length, hidden_dim], [*, bs, hidden_dim]</span></span><br><span class="line">        x = seq_output[:, -<span class="number">1</span>, :]</span><br><span class="line">        <span class="comment"># 取最后一个时间步的输出 (这也是为什么要设置padding_first=True的原因)</span></span><br><span class="line">        x = self.layer(x)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">sample_inputs = torch.randint(<span class="number">0</span>, vocab_size, (<span class="number">2</span>, <span class="number">128</span>))</span><br><span class="line">    </span><br><span class="line">print(<span class="string">&quot;&#123;:=^80&#125;&quot;</span>.<span class="built_in">format</span>(<span class="string">&quot; 一层单向 LSTM &quot;</span>))       </span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> LSTM().named_parameters():</span><br><span class="line">    print(<span class="string">f&quot;<span class="subst">&#123;key:^<span class="number">40</span>&#125;</span>paramerters num: <span class="subst">&#123;np.prod(value.shape)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">print(<span class="string">&quot;&#123;:=^80&#125;&quot;</span>.<span class="built_in">format</span>(<span class="string">&quot; 一层双向 LSTM &quot;</span>))       </span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> LSTM(bidirectional=<span class="literal">True</span>).named_parameters():</span><br><span class="line">    print(<span class="string">f&quot;<span class="subst">&#123;key:^<span class="number">40</span>&#125;</span>paramerters num: <span class="subst">&#123;np.prod(value.shape)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">print(<span class="string">&quot;&#123;:=^80&#125;&quot;</span>.<span class="built_in">format</span>(<span class="string">&quot; 两层单向 LSTM &quot;</span>))       </span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> LSTM(num_layers=<span class="number">2</span>).named_parameters():</span><br><span class="line">    print(<span class="string">f&quot;<span class="subst">&#123;key:^<span class="number">40</span>&#125;</span>paramerters num: <span class="subst">&#123;np.prod(value.shape)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/21398/5.png" style="zoom:60%;">

<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluating</span>(<span class="params">model, dataloader, loss_fct</span>):</span></span><br><span class="line">    loss_list = []</span><br><span class="line">    pred_list = []</span><br><span class="line">    label_list = []</span><br><span class="line">    <span class="keyword">for</span> datas, labels <span class="keyword">in</span> dataloader:</span><br><span class="line">        datas = datas.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line">        <span class="comment"># 前向计算</span></span><br><span class="line">        logits = model(datas)</span><br><span class="line">        loss = loss_fct(logits, labels)         <span class="comment"># 验证集损失</span></span><br><span class="line">        loss_list.append(loss.item())</span><br><span class="line">        <span class="comment"># 二分类</span></span><br><span class="line">        preds = logits &gt; <span class="number">0</span></span><br><span class="line">        pred_list.extend(preds.cpu().numpy().tolist())</span><br><span class="line">        label_list.extend(labels.cpu().numpy().tolist())</span><br><span class="line">        </span><br><span class="line">    acc = accuracy_score(label_list, pred_list)</span><br><span class="line">    <span class="keyword">return</span> np.mean(loss_list), acc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">    model, </span></span></span><br><span class="line"><span class="function"><span class="params">    train_loader, </span></span></span><br><span class="line"><span class="function"><span class="params">    val_loader, </span></span></span><br><span class="line"><span class="function"><span class="params">    epoch, </span></span></span><br><span class="line"><span class="function"><span class="params">    loss_fct, </span></span></span><br><span class="line"><span class="function"><span class="params">    optimizer, </span></span></span><br><span class="line"><span class="function"><span class="params">    tensorboard_callback=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    save_ckpt_callback=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    early_stop_callback=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    eval_step=<span class="number">500</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">    record_dict = &#123;</span><br><span class="line">        <span class="string">&quot;train&quot;</span>: [],</span><br><span class="line">        <span class="string">&quot;val&quot;</span>: []</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    global_step = <span class="number">0</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">with</span> tqdm(total=epoch * <span class="built_in">len</span>(train_loader)) <span class="keyword">as</span> pbar:</span><br><span class="line">        <span class="keyword">for</span> epoch_id <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">            <span class="comment"># training</span></span><br><span class="line">            <span class="keyword">for</span> datas, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">                datas = datas.to(device)</span><br><span class="line">                labels = labels.to(device)</span><br><span class="line">                <span class="comment"># 梯度清空</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">                <span class="comment"># 模型前向计算</span></span><br><span class="line">                logits = model(datas)</span><br><span class="line">                <span class="comment"># 计算损失</span></span><br><span class="line">                loss = loss_fct(logits, labels)</span><br><span class="line">                <span class="comment"># 梯度回传</span></span><br><span class="line">                loss.backward()</span><br><span class="line">                <span class="comment"># 调整优化器，包括学习率的变动等</span></span><br><span class="line">                optimizer.step()</span><br><span class="line">                preds = logits &gt; <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">                acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())    </span><br><span class="line">                loss = loss.cpu().item()</span><br><span class="line">                <span class="comment"># record</span></span><br><span class="line">                </span><br><span class="line">                record_dict[<span class="string">&quot;train&quot;</span>].append(&#123;</span><br><span class="line">                    <span class="string">&quot;loss&quot;</span>: loss, <span class="string">&quot;acc&quot;</span>: acc, <span class="string">&quot;step&quot;</span>: global_step</span><br><span class="line">                &#125;)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># evaluating</span></span><br><span class="line">                <span class="keyword">if</span> global_step % eval_step == <span class="number">0</span>:</span><br><span class="line">                    model.<span class="built_in">eval</span>()</span><br><span class="line">                    val_loss, val_acc = evaluating(model, val_loader, loss_fct)</span><br><span class="line">                    record_dict[<span class="string">&quot;val&quot;</span>].append(&#123;</span><br><span class="line">                        <span class="string">&quot;loss&quot;</span>: val_loss, <span class="string">&quot;acc&quot;</span>: val_acc, <span class="string">&quot;step&quot;</span>: global_step</span><br><span class="line">                    &#125;)</span><br><span class="line">                    model.train()</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 1. 使用 tensorboard 可视化</span></span><br><span class="line">                    <span class="keyword">if</span> tensorboard_callback <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                        tensorboard_callback(</span><br><span class="line">                            global_step, </span><br><span class="line">                            loss=loss, val_loss=val_loss,</span><br><span class="line">                            acc=acc, val_acc=val_acc,</span><br><span class="line">                            lr=optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>],</span><br><span class="line">                            )</span><br><span class="line">                </span><br><span class="line">                    <span class="comment"># 2. 保存模型权重 save model checkpoint</span></span><br><span class="line">                    <span class="keyword">if</span> save_ckpt_callback <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                        save_ckpt_callback(global_step, model.state_dict(), metric=val_acc)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 3. 早停 Early Stop</span></span><br><span class="line">                    <span class="keyword">if</span> early_stop_callback <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                        early_stop_callback(val_acc)</span><br><span class="line">                        <span class="keyword">if</span> early_stop_callback.early_stop:</span><br><span class="line">                            print(<span class="string">f&quot;Early stop at epoch <span class="subst">&#123;epoch_id&#125;</span> / global_step <span class="subst">&#123;global_step&#125;</span>&quot;</span>)</span><br><span class="line">                            <span class="keyword">return</span> record_dict</span><br><span class="line">                    </span><br><span class="line">                <span class="comment"># udate step</span></span><br><span class="line">                global_step += <span class="number">1</span></span><br><span class="line">                pbar.update(<span class="number">1</span>)</span><br><span class="line">                pbar.set_postfix(&#123;<span class="string">&quot;epoch&quot;</span>: epoch_id&#125;)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> record_dict</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">epoch = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">model = LSTM()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 定义损失函数 采用交叉熵损失 (但是二分类)</span></span><br><span class="line">loss_fct = F.binary_cross_entropy_with_logits</span><br><span class="line"><span class="comment"># 2. 定义优化器 采用 adam</span></span><br><span class="line"><span class="comment"># Optimizers specified in the torch.optim package</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. tensorboard 可视化</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&quot;runs&quot;</span>):</span><br><span class="line">    os.mkdir(<span class="string">&quot;runs&quot;</span>)</span><br><span class="line">tensorboard_callback = TensorBoardCallback(<span class="string">&quot;runs/imdb-lstm&quot;</span>)</span><br><span class="line"><span class="comment"># tensorboard_callback.draw_model(model, [1, MAX_LENGTH])</span></span><br><span class="line"><span class="comment"># 2. save best</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&quot;checkpoints&quot;</span>):</span><br><span class="line">    os.makedirs(<span class="string">&quot;checkpoints&quot;</span>)</span><br><span class="line">save_ckpt_callback = SaveCheckpointsCallback(<span class="string">&quot;checkpoints/imdb-lstm&quot;</span>, save_step=<span class="built_in">len</span>(train_dl), save_best_only=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 3. early stop</span></span><br><span class="line">early_stop_callback = EarlyStopCallback(patience=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">model = model.to(device)</span><br><span class="line">record = training(</span><br><span class="line">    model, </span><br><span class="line">    train_dl, </span><br><span class="line">    test_dl, </span><br><span class="line">    epoch, </span><br><span class="line">    loss_fct, </span><br><span class="line">    optimizer, </span><br><span class="line">    tensorboard_callback=tensorboard_callback,</span><br><span class="line">    save_ckpt_callback=save_ckpt_callback,</span><br><span class="line">    early_stop_callback=early_stop_callback,</span><br><span class="line">    eval_step=<span class="built_in">len</span>(train_dl)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>


<a id="more"></a>

]]></content>
      <categories>
        <category>深度学习</category>
        <category>LSTM</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>大模型</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习-——卷积神经网络</title>
    <url>/45354.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="为什么要使用卷积"><a href="#为什么要使用卷积" class="headerlink" title="为什么要使用卷积"></a>为什么要使用卷积</h2><h3 id="神经网络遇到的问题-1"><a href="#神经网络遇到的问题-1" class="headerlink" title="神经网络遇到的问题 1"></a>神经网络遇到的问题 1</h3><ul>
<li><p>参数过多内存装不下<br>举例:</p>
<ul>
<li><p>图像大小 1000*1000<br>一层神经元数目为 10^6</p>
</li>
<li><p><em>全连接参数为 1000</em>1000*10^ 6=10^12<br>一层就是 1 万亿个参数， 内存装不下</p>
</li>
</ul>
</li>
</ul>
<h3 id="神经网络遇到的问题-2"><a href="#神经网络遇到的问题-2" class="headerlink" title="神经网络遇到的问题 2"></a>神经网络遇到的问题 2</h3><ul>
<li>参数过多容易过拟合<br>◆计算资源不足<br>◆容易过拟合,发生过拟合， 我们就需要更多训练数据， 但是很多时候我们没有更多的数据， 因为获取数据需要成本</li>
</ul>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/45354/1.png" style="zoom:50%;">

<h2 id="为什么我们可以这么做-卷积为什么有作用-呢？"><a href="#为什么我们可以这么做-卷积为什么有作用-呢？" class="headerlink" title="为什么我们可以这么做(卷积为什么有作用)呢？"></a>为什么我们可以这么做(卷积为什么有作用)呢？</h2><ul>
<li>局部连接<br><strong>图像的区域性</strong>—爱因斯坦的嘴唇附近的色彩等是相似的</li>
<li>参数共享与平移不变性<br><strong>图像特征与位置无关</strong>—左边是脸， 右边也是脸， 这样无论脸放在什么地方都检查出来，刚好可以解决过拟合的问题（否则脸放到其他地方就检测不出来）</li>
</ul>
<p>参数共享与平移不变性<br>可以参考下面链接—重点看<br><a href="https://blog.csdn.net/weixin_44177568/article/details/102812050?utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~all~first_rank_v2~rank_v25-3-102812050.nonecase&amp;utm_term=%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%81%9A%E5%8D%B7%E7%A7%AF">https://blog.csdn.net/weixin_44177568/article/details/102812050?utm_medium=distribute.pc_aggpage_search_result.none-task-blog-2~all~first_rank_v2~rank_v25-3-102812050.nonecase&amp;utm_term=%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%81%9A%E5%8D%B7%E7%A7%AF</a><br><a href="https://zhuanlan.zhihu.com/p/41696749">https://zhuanlan.zhihu.com/p/41696749</a></p>
<h2 id="卷积操作"><a href="#卷积操作" class="headerlink" title="卷积操作"></a>卷积操作</h2><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/45354/2.png" style="zoom:50%;">

<p>具体我们看下如何做卷积， 其实是一个把<strong>卷积核和它相对应部分做乘法</strong>， 然后再<strong>求和</strong></p>
<p>图卷积核重合区域内相对应的每一个像素值乘卷积核 、 内相对应点的权重， 然后求和， 再加上偏置后， 最后得到输出图片中的一个像素值</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/45354/3.png" style="zoom:50%;">

<p>卷积核的值就是w权重，我们通过训练得到。初始化权值为：<strong>nn.init.xavier_normal(**格罗特初始化随机权值)或者</strong>init.kaiming.uniform_(self.weight, a=math.sqrt(5))**</p>
<p><strong>总结：!! 卷积输出图像尺寸 1+(n-k)//s（s 是步长， n 是输入尺寸， k 是卷积核）</strong></p>
<h2 id="padding"><a href="#padding" class="headerlink" title="padding"></a>padding</h2><p>那有没有操作让大小保持不变呢？采用如下 padding 手法即可：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/45354/4.png" style="zoom:50%;">

<p><strong>提取图像的高阶特征</strong>：因为padding后图像大小虽然没变，但是提取到的都是图像的高阶特征</p>
<h2 id="卷积神经网络结构"><a href="#卷积神经网络结构" class="headerlink" title="卷积神经网络结构"></a>卷积神经网络结构</h2><p>◆卷积神经网络<br>    ◆(卷积层+(可选)池化层)<em>N+全连接层</em>M<br>N&gt;=1,M&gt;=0<br>卷积层的输入和输出都是矩阵， 全连接层的输入和输出都是向量</p>
<p>矩阵和向量的区分， 一维就是向量， 二维就是矩阵<br><a href="https://blog.csdn.net/liuxiangxxl/article/details/82253497">https://blog.csdn.net/liuxiangxxl/article/details/82253497</a></p>
<p>在最后一层的卷积上， 把它做一个展平， 这样就可以和全连接层进行运算了为什么卷积要放到前面， 因为展平丧失了维度信息， 因此全连接层后面不能再放卷积层</p>
<h3 id="卷积——多通道-channels"><a href="#卷积——多通道-channels" class="headerlink" title="卷积——多通道(channels)"></a>卷积——多通道(channels)</h3><p>面的卷积都是单通道的， 多通道， 比如 rgb 三色， 我们相当于有一个厚度， 分别去处理， 然后再加起来， 最后就变为 1 层了</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/45354/5.png" style="zoom:50%;">

<h3 id="卷积——多个卷积核-channels"><a href="#卷积——多个卷积核-channels" class="headerlink" title="卷积——多个卷积核(channels)"></a>卷积——多个卷积核(channels)</h3><p> 多个卷积核可以从不同角度提取学习图像的高阶特征</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/45354/6.png" style="zoom:50%;">

<h2 id="池化-pooling"><a href="#池化-pooling" class="headerlink" title="池化(pooling)"></a>池化(pooling)</h2><p>池化核：没有参数，一般步长是2，移动不到的部分则丢弃。选择核中<strong>最大或者均值</strong></p>
<p><strong>池化作用</strong>： 池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。本质是 <strong>降采样</strong>，<strong>可以大幅减少网络的参数量</strong>。<strong>卷积是获取高阶特征，池化才是降采样</strong></p>
<p><strong>池化技术的本质</strong>： 在尽可能保留图片空间信息的前提下， <strong>降低图片的尺寸</strong>， 增大卷积核感受视野， 提取高层特征， 同时<strong>减少网络参数量， 预防过拟合</strong>。</p>
<p>简单来说： 等比例缩小图片， 图片的主体内容丢失不多， 依然具有平移， 旋转，尺度的不变性， 简单来说就是图片的主体内容依旧保存着原来大部分的空间信息</p>
<h3 id="最大值池化"><a href="#最大值池化" class="headerlink" title="最大值池化"></a>最大值池化</h3><p>能够<strong>抑制网络参数误差</strong>造成的<strong>估计均值偏移</strong>的现象</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/45354/7.png" style="zoom:50%;">

<h3 id="平均值池化"><a href="#平均值池化" class="headerlink" title="平均值池化"></a>平均值池化</h3><p>主要用来抑制邻域值之间差别过大，造成的方差过大。</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/45354/8.png" style="zoom:50%;">

<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><p>​    ◆ 常使用不重叠（池化核的大小与步长相等） 、 不补零（padding 为 valid）<br>​    ◆ 没有用于求导的参数（没有需要训练的参数， 参数个数为 0）<br>​    ◆ 池化层的超参数为步长和池化核大小<br>​    ◆ 用于减少图像尺寸,从而减少计算量<br>​    ◆ 一定程度平移鲁棒<br>​    比如一只猫移动了一个像素的另外一张图片， 我们先做池化， 再做卷积， 那么最终还是可以识别这个猫<br>​    ◆ 损失了空间位置精度</p>
<h2 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h2><ul>
<li>每次图像<strong>尺寸变小</strong>，需要<strong>更多的卷积核</strong>来学习高阶特征，所以pool变小一倍以后，卷积核数量翻一倍</li>
<li>MaxPool2d(2,2) #池化核大小为(2*2)步长为2</li>
<li>卷积核都为奇数的nxn，没有偶数的</li>
<li>每个卷积核都有一个偏置bias</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, activation=<span class="string">&quot;relu&quot;</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.activation = F.relu <span class="keyword">if</span> activation == <span class="string">&quot;relu&quot;</span> <span class="keyword">else</span> F.selu</span><br><span class="line">        <span class="comment">#输入通道数，图片是灰度图，所以是1，图片是彩色图，就是3，输出通道数，就是卷积核的个数(32,1,28,28）</span></span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#输入x(32,32,28,28)输出x(32,32,28,28)</span></span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>) <span class="comment">#池化核大小为(2*2)步长为2</span></span><br><span class="line">        self.conv3 = nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv4 = nn.Conv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv5 = nn.Conv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv6 = nn.Conv2d(in_channels=<span class="number">128</span>, out_channels=<span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        <span class="comment"># input shape is (28, 28, 1) so the fc1 layer in_features is 128 * 3 * 3</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">128</span> * <span class="number">3</span> * <span class="number">3</span>, <span class="number">128</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">        self.init_weights()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;使用 xavier 均匀分布来初始化全连接层、卷积层的权重 W&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, (nn.Linear, nn.Conv2d)):</span><br><span class="line">                nn.init.xavier_uniform_(m.weight)</span><br><span class="line">                nn.init.zeros_(m.bias)</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        act = self.activation</span><br><span class="line">        x = self.pool(act(self.conv2(act(self.conv1(x))))) <span class="comment"># 1 * 28 * 28 -&gt; 32 * 14 * 14</span></span><br><span class="line">        x = self.pool(act(self.conv4(act(self.conv3(x))))) <span class="comment"># 32 * 14 * 14 -&gt; 64 * 7 * 7</span></span><br><span class="line">        x = self.pool(act(self.conv6(act(self.conv5(x))))) <span class="comment"># 64 * 7 * 7 -&gt; 128 * 3 * 3</span></span><br><span class="line">        x = self.flatten(x) <span class="comment"># 128 * 3 * 3 -&gt;1152 展平</span></span><br><span class="line">        x = act(self.fc1(x)) <span class="comment"># 1152 -&gt; 128</span></span><br><span class="line">        x = self.fc2(x) <span class="comment"># 128 -&gt; 10</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">        </span><br><span class="line"><span class="keyword">for</span> idx, (key, value) <span class="keyword">in</span> <span class="built_in">enumerate</span>(CNN().named_parameters()):</span><br><span class="line">    print(<span class="string">f&quot;<span class="subst">&#123;key&#125;</span>\tparamerters num: <span class="subst">&#123;np.prod(value.shape)&#125;</span>&quot;</span>) <span class="comment"># 打印模型的参数信息</span></span><br></pre></td></tr></table></figure>
<h3 id="参数量计算："><a href="#参数量计算：" class="headerlink" title="参数量计算："></a>参数量计算：</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">f&#x27;conv1 - <span class="subst">&#123;<span class="number">1</span>*<span class="number">3</span>*<span class="number">3</span>*<span class="number">32</span>&#125;</span>&#x27;</span>) <span class="comment"># 32个卷积核，每个卷积核大小为1*3*3</span></span><br><span class="line">print(<span class="string">f&#x27;conv2 - <span class="subst">&#123;<span class="number">32</span>*<span class="number">3</span>*<span class="number">3</span>*<span class="number">32</span>&#125;</span>&#x27;</span>) <span class="comment"># 32个卷积核，每个卷积核大小为32*3*3</span></span><br><span class="line">print(<span class="string">f&#x27;conv3 - <span class="subst">&#123;<span class="number">32</span>*<span class="number">3</span>*<span class="number">3</span>*<span class="number">64</span>&#125;</span>&#x27;</span>)</span><br><span class="line">print(<span class="string">f&#x27;conv4 - <span class="subst">&#123;<span class="number">64</span>*<span class="number">3</span>*<span class="number">3</span>*<span class="number">64</span>&#125;</span>&#x27;</span>)</span><br><span class="line">print(<span class="string">f&#x27;conv5 - <span class="subst">&#123;<span class="number">64</span>*<span class="number">3</span>*<span class="number">3</span>*<span class="number">128</span>&#125;</span>&#x27;</span>)</span><br><span class="line">print(<span class="string">f&#x27;conv6 - <span class="subst">&#123;<span class="number">128</span>*<span class="number">3</span>*<span class="number">3</span>*<span class="number">128</span>&#125;</span>&#x27;</span>)</span><br><span class="line">print(<span class="string">f&#x27;fc1 - <span class="subst">&#123;<span class="number">1152</span>*<span class="number">128</span>&#125;</span>&#x27;</span>)</span><br><span class="line">print(<span class="string">f&#x27;fc2 - <span class="subst">&#123;<span class="number">128</span>*<span class="number">10</span>&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#对上面求和，总参数数目为：</span></span><br><span class="line"><span class="number">1</span>*<span class="number">3</span>*<span class="number">3</span>*<span class="number">32</span> +<span class="number">32</span>+ <span class="number">32</span>*<span class="number">3</span>*<span class="number">3</span>*<span class="number">32</span> +<span class="number">32</span>+ <span class="number">32</span>*<span class="number">3</span>*<span class="number">3</span>*<span class="number">64</span> +<span class="number">64</span>+ <span class="number">64</span>*<span class="number">3</span>*<span class="number">3</span>*<span class="number">64</span> +<span class="number">64</span>+ <span class="number">64</span>*<span class="number">3</span>*<span class="number">3</span>*<span class="number">128</span> +<span class="number">128</span>+ <span class="number">128</span>*<span class="number">3</span>*<span class="number">3</span>*<span class="number">128</span> + <span class="number">128</span>*<span class="number">3</span>*<span class="number">3</span>*<span class="number">128</span> +<span class="number">128</span>+ <span class="number">128</span>*<span class="number">10</span> = <span class="number">434720</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">    model, </span></span></span><br><span class="line"><span class="function"><span class="params">    train_loader, </span></span></span><br><span class="line"><span class="function"><span class="params">    val_loader, </span></span></span><br><span class="line"><span class="function"><span class="params">    epoch, </span></span></span><br><span class="line"><span class="function"><span class="params">    loss_fct, </span></span></span><br><span class="line"><span class="function"><span class="params">    optimizer, </span></span></span><br><span class="line"><span class="function"><span class="params">    tensorboard_callback=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    save_ckpt_callback=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    early_stop_callback=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    eval_step=<span class="number">500</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br><span class="line">    record_dict = &#123;</span><br><span class="line">        <span class="string">&quot;train&quot;</span>: [],</span><br><span class="line">        <span class="string">&quot;val&quot;</span>: []</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    global_step = <span class="number">0</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">with</span> tqdm(total=epoch * <span class="built_in">len</span>(train_loader)) <span class="keyword">as</span> pbar:</span><br><span class="line">        <span class="keyword">for</span> epoch_id <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">            <span class="comment"># training</span></span><br><span class="line">            <span class="keyword">for</span> datas, labels <span class="keyword">in</span> train_loader:</span><br><span class="line">                datas = datas.to(device)</span><br><span class="line">                labels = labels.to(device)</span><br><span class="line">                <span class="comment"># 梯度清空</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">                <span class="comment"># 模型前向计算</span></span><br><span class="line">                logits = model(datas)</span><br><span class="line">                <span class="comment"># 计算损失</span></span><br><span class="line">                loss = loss_fct(logits, labels)</span><br><span class="line">                <span class="comment"># 梯度回传</span></span><br><span class="line">                loss.backward()</span><br><span class="line">                <span class="comment"># 调整优化器，包括学习率的变动等</span></span><br><span class="line">                optimizer.step()</span><br><span class="line">                preds = logits.argmax(axis=-<span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">                acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())    </span><br><span class="line">                loss = loss.cpu().item()</span><br><span class="line">                <span class="comment"># record</span></span><br><span class="line">                </span><br><span class="line">                record_dict[<span class="string">&quot;train&quot;</span>].append(&#123;</span><br><span class="line">                    <span class="string">&quot;loss&quot;</span>: loss, <span class="string">&quot;acc&quot;</span>: acc, <span class="string">&quot;step&quot;</span>: global_step</span><br><span class="line">                &#125;)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># evaluating</span></span><br><span class="line">                <span class="keyword">if</span> global_step % eval_step == <span class="number">0</span>:</span><br><span class="line">                    model.<span class="built_in">eval</span>()</span><br><span class="line">                    val_loss, val_acc = evaluating(model, val_loader, loss_fct)</span><br><span class="line">                    record_dict[<span class="string">&quot;val&quot;</span>].append(&#123;</span><br><span class="line">                        <span class="string">&quot;loss&quot;</span>: val_loss, <span class="string">&quot;acc&quot;</span>: val_acc, <span class="string">&quot;step&quot;</span>: global_step</span><br><span class="line">                    &#125;)</span><br><span class="line">                    model.train()</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 1. 使用 tensorboard 可视化</span></span><br><span class="line">                    <span class="keyword">if</span> tensorboard_callback <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                        tensorboard_callback(</span><br><span class="line">                            global_step, </span><br><span class="line">                            loss=loss, val_loss=val_loss,</span><br><span class="line">                            acc=acc, val_acc=val_acc,</span><br><span class="line">                            lr=optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>],</span><br><span class="line">                            )</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 2. 保存模型权重 save model checkpoint</span></span><br><span class="line">                    <span class="keyword">if</span> save_ckpt_callback <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                        save_ckpt_callback(global_step, model.state_dict(), metric=val_acc)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 3. 早停 Early Stop</span></span><br><span class="line">                    <span class="keyword">if</span> early_stop_callback <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                        early_stop_callback(val_acc)</span><br><span class="line">                        <span class="keyword">if</span> early_stop_callback.early_stop:</span><br><span class="line">                            print(<span class="string">f&quot;Early stop at epoch <span class="subst">&#123;epoch_id&#125;</span> / global_step <span class="subst">&#123;global_step&#125;</span>&quot;</span>)</span><br><span class="line">                            <span class="keyword">return</span> record_dict</span><br><span class="line">                    </span><br><span class="line">                <span class="comment"># udate step</span></span><br><span class="line">                global_step += <span class="number">1</span></span><br><span class="line">                pbar.update(<span class="number">1</span>)</span><br><span class="line">                pbar.set_postfix(&#123;<span class="string">&quot;epoch&quot;</span>: epoch_id&#125;)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> record_dict</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">epoch = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">activation = <span class="string">&quot;relu&quot;</span></span><br><span class="line">model = CNN(activation)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 定义损失函数 采用交叉熵损失</span></span><br><span class="line">loss_fct = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 2. 定义优化器 采用SGD</span></span><br><span class="line"><span class="comment"># Optimizers specified in the torch.optim package</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. tensorboard 可视化</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&quot;runs&quot;</span>):</span><br><span class="line">    os.mkdir(<span class="string">&quot;runs&quot;</span>)</span><br><span class="line">tensorboard_callback = TensorBoardCallback(<span class="string">f&quot;runs/cnn-<span class="subst">&#123;activation&#125;</span>&quot;</span>)</span><br><span class="line">tensorboard_callback.draw_model(model, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"><span class="comment"># 2. save best</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&quot;checkpoints&quot;</span>):</span><br><span class="line">    os.makedirs(<span class="string">&quot;checkpoints&quot;</span>)</span><br><span class="line">save_ckpt_callback = SaveCheckpointsCallback(<span class="string">f&quot;checkpoints/cnn-<span class="subst">&#123;activation&#125;</span>&quot;</span>, save_best_only=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 3. early stop</span></span><br><span class="line">early_stop_callback = EarlyStopCallback(patience=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h2 id="视野域"><a href="#视野域" class="headerlink" title="视野域"></a>视野域</h2><img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/45354/9.png" style="zoom:50%;">

<p>经过<strong>两层3x3</strong>之后一个视野域相当于<strong>一个5x5的视野域</strong></p>
<p>一层5x5的卷积核有25个参数，而两层3x3 有 9+9 = 18个参数。<strong>同样视野域下参数更少</strong></p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>从relu换成selu后，能有好的效果提升，因为selu能在一定程度上缓解梯度消失和梯度爆炸的问题。</p>
<h2 id="10-Monkeys"><a href="#10-Monkeys" class="headerlink" title="10-Monkeys"></a>10-Monkeys</h2><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, Resize, Compose, ConvertImageDtype, Normalize</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line">DATA_DIR = Path(<span class="string">&quot;./archive/&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MonkeyDataset</span>(<span class="params">datasets.ImageFolder</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, mode, transform=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&quot;train&quot;</span>:</span><br><span class="line">            root = DATA_DIR / <span class="string">&quot;training&quot;</span></span><br><span class="line">        <span class="keyword">elif</span> mode == <span class="string">&quot;val&quot;</span>:</span><br><span class="line">            root = DATA_DIR / <span class="string">&quot;validation&quot;</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;mode should be one of the following: train, val, but got &#123;&#125;&quot;</span>.<span class="built_in">format</span>(mode))</span><br><span class="line">        <span class="built_in">super</span>().__init__(root, transform) <span class="comment"># 调用父类init方法 root：路径，transform：图片处理方法 </span></span><br><span class="line">        self.imgs = self.samples <span class="comment"># self.samples里边是图片路径及标签[(path,label),(path,label),...]</span></span><br><span class="line">        self.targets = [s[<span class="number">1</span>] <span class="keyword">for</span> s <span class="keyword">in</span> self.samples] <span class="comment"># 标签取出来</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 预先设定的图片尺寸</span></span><br><span class="line">img_h, img_w = <span class="number">128</span>, <span class="number">128</span></span><br><span class="line">transform = Compose([</span><br><span class="line">    Resize((img_h, img_w)), <span class="comment"># 图片缩放</span></span><br><span class="line">    ToTensor(),</span><br><span class="line">    <span class="comment"># 预先统计的</span></span><br><span class="line">    Normalize([<span class="number">0.4363</span>, <span class="number">0.4328</span>, <span class="number">0.3291</span>], [<span class="number">0.2085</span>, <span class="number">0.2032</span>, <span class="number">0.1988</span>]),</span><br><span class="line">    ConvertImageDtype(torch.<span class="built_in">float</span>), <span class="comment"># 转换为float类型</span></span><br><span class="line">]) <span class="comment">#数据预处理</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_ds = MonkeyDataset(<span class="string">&quot;train&quot;</span>, transform=transform)</span><br><span class="line">val_ds = MonkeyDataset(<span class="string">&quot;val&quot;</span>, transform=transform)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;load &#123;&#125; images from training dataset&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(train_ds)))</span><br><span class="line">print(<span class="string">&quot;load &#123;&#125; images from validation dataset&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(val_ds)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.dataloader <span class="keyword">import</span> DataLoader    </span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line"><span class="comment">#从数据集到dataloader，num_workers参数不能加，否则会报错</span></span><br><span class="line">train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>10-Monkeys：不同类别的图片放在不同的文件夹，继承自datasets.ImageFolder</li>
<li>Resize((img_h, img_w))：图片缩放，将图片尺寸进行调整，可能像素会改变(比如用均值拟合)，但整体不会改变。<strong>尺寸需要统一</strong>，不能尺寸变化</li>
<li>Tran_ds.classes：[‘n0’,’n1’’n2’,’n3’,’n4’,’n5’,’nó’,’n7’,’n8’,’n9’]</li>
<li>Tran_ds.class_to_idx：{..’n2’:2,’n3’:3,’n4’:4,’n5’: 5,’n6’:6,.’n7’:7 …’n9’:9}</li>
<li>Normalize([0.4363, 0.4328, 0.3291], [0.2085, 0.2032, 0.1988])：三通道的均值和方差（均值为0，方差为1，证明：再进行一次标准化，可以得出）</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_classes=<span class="number">10</span>, activation=<span class="string">&quot;relu&quot;</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.activation = F.relu <span class="keyword">if</span> activation == <span class="string">&quot;relu&quot;</span> <span class="keyword">else</span> F.selu</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>)</span><br><span class="line">        self.conv4 = nn.Conv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>)</span><br><span class="line">        self.conv5 = nn.Conv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>)</span><br><span class="line">        self.conv6 = nn.Conv2d(in_channels=<span class="number">128</span>, out_channels=<span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>)</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        <span class="comment"># input shape is (3, 128, 128) so the flatten output shape is 128 * 16 * 16</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">128</span> * <span class="number">16</span> * <span class="number">16</span>, <span class="number">128</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">128</span>, num_classes)</span><br><span class="line">        </span><br><span class="line">        self.init_weights()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;使用 xavier 均匀分布来初始化全连接层、卷积层的权重 W&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, (nn.Linear, nn.Conv2d)):</span><br><span class="line">                nn.init.xavier_uniform_(m.weight)</span><br><span class="line">                nn.init.zeros_(m.bias)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        act = self.activation</span><br><span class="line">        x = self.pool(act(self.conv2(act(self.conv1(x)))))</span><br><span class="line">        x = self.pool(act(self.conv4(act(self.conv3(x)))))</span><br><span class="line">        x = self.pool(act(self.conv6(act(self.conv5(x)))))</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = act(self.fc1(x))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx, (key, value) <span class="keyword">in</span> <span class="built_in">enumerate</span>(CNN().named_parameters()):</span><br><span class="line">    print(<span class="string">f&quot;<span class="subst">&#123;key&#125;</span>\tparamerters num: <span class="subst">&#123;np.prod(value.shape)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>padding=”same”：保证步长为1的时候，输入输出的维度一致</li>
</ul>
<a id="more"></a>

]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>卷积神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习-1-——神经网络、softmax以及交叉熵损失</title>
    <url>/39756.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="深度学习的定义"><a href="#深度学习的定义" class="headerlink" title="深度学习的定义"></a>深度学习的定义</h2><p>深度学习是<a href="https://www.oracle.com/cn/data-science/machine-learning/what-is-machine-learning/">机器学习 (ML)</a> 的一个子集，指人工神经网络（由算法建模而成，能够像人的大脑一样工作）学习大量数据。</p>
<h2 id="深度学习的工作原理是什么？"><a href="#深度学习的工作原理是什么？" class="headerlink" title="深度学习的工作原理是什么？"></a>深度学习的工作原理是什么？</h2><p>深度学习由<a href="https://blogs.oracle.com/bigdata/post/neural-networks-in-deep-learning">神经网络</a>层驱动。神经网络由一系列算法按照人类大脑的工作方式松散建模而成，而使用大量数据进行训练，即对神经网络的神经进行配置。经过训练后，深度学习模型可以处理新数据，能够摄取并实时分析多个来源的数据，无需人为干预。在深度学习中，图形处理单元 (GPU) 可以同时处理多个计算，以优化方式训练深度学习模型。</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/39756/1.png" style="zoom:50%;">

<p><strong>深度学习的概念源于对<a href="https://zhida.zhihu.com/search?content_id=359980856&content_type=Answer&match_order=1&q=%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&zhida_source=entity">人工神经网络</a>的研究，很多深度学习算法都使用神经网络进行表示，因为神经网络的性能精度和通用效果都非常好，于是业界习惯性地把深度学习算法等同于AI。</strong></p>
<h2 id="目标函数softmax"><a href="#目标函数softmax" class="headerlink" title="目标函数softmax"></a>目标函数softmax</h2><p>在使用深度学习来处理分类问题的时候，需要衡量目标类别与当前预测的差距，目标函数可以帮助衡量模型的好坏，softmax 就是一个目标函数： 讲结果变为一个概率值</p>
<p>Softmax函数，又称 <strong>归一化指数函数</strong> 。</p>
<p>该函数 是最流行的 <a href="https://blog.csdn.net/jningwei/article/details/79232031">分类任务目标函数</a>，也是 Sigmoid函数 的一种 推广。可转换为<a href="https://blog.csdn.net/JNingWei/article/details/79843205">交叉熵误差 (CE) </a>。</p>
<p>它将原始的实数输入（通常是神经网络的输出）转换成概率值。具体来说，Softmax 操作会把一个向量的每个元素转换为 [0, 1] 范围内的值，并且所有输出的和为 1，从而可以将它们解释为概率。</p>
<p>给定一个向量 z=[z1,z2,…,zn]z=[z1,z2,…,zn] 作为输入，Softmax 函数输出一个与输入向量形状相同的概率分布：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/39756/2.png" style="zoom:50%;">

<ul>
<li>zi 是输入向量中的第 ii 个元素。</li>
<li>输出的每个 Softmax(zi)Softmax(zi) 是一个概率值，所有概率值的和为 1。</li>
</ul>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/39756/3.png" style="zoom:50%;">

<p>例如：输入向量 [ 1 , 2 , 3 , 4 , 1 , 2 , 3 ] 对应的Softmax函数的值为 [ 0.024 , 0.064 , 0.175 , 0.475 , 0.024 , 0.064 , 0.175 。输出向量中拥有最大权重的项对应着输入向量中的最大值“4”。这也显示了这个函数通常的意义：</p>
<p><strong>对向量进行归一化，凸显其中最大的值并抑制远低于最大值的其他分量。</strong> </p>
<h2 id="交叉熵损失"><a href="#交叉熵损失" class="headerlink" title="交叉熵损失"></a><strong>交叉熵损失</strong></h2><p>交叉熵损失函数（Cross-Entropy Loss）用来计算 <strong>分类任务中</strong> 预测概率分布与真实标签分布之间的差异。交叉熵损失衡量了两个概率分布之间的距离，特别是在分类任务中，衡量了预测类别分布和实际标签分布的差异。</p>
<h4 id="交叉熵损失的公式："><a href="#交叉熵损失的公式：" class="headerlink" title="交叉熵损失的公式："></a>交叉熵损失的公式：</h4><p>给定真实标签分布 y=[y1,y2,…,yn]y=[y1,y2,…,yn] 和模型的预测输出概率分布 p=[p1,p2,…,pn]p=[p1,p2,…,pn]，交叉熵损失定义为：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/39756/4.png" style="zoom:50%;">

<ul>
<li>yi 是真实标签（通常是一个 one-hot 向量，表示类别标签）。</li>
<li>pii 是模型预测的概率值，通常由 Softmax 得到。</li>
</ul>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/39756/5.png" style="zoom:40%;">

<h4 id="Softmax-和交叉熵损失的结合："><a href="#Softmax-和交叉熵损失的结合：" class="headerlink" title="Softmax 和交叉熵损失的结合："></a><strong>Softmax 和交叉熵损失的结合</strong>：</h4><p>在多分类问题中，通常会先通过 Softmax 转换模型的输出为概率分布，再使用交叉熵损失计算损失值。因此，Softmax 和交叉熵损失经常是紧密配合使用的，但它们的功能和作用是不同的。</p>
<ul>
<li><strong>Softmax</strong>：将神经网络的原始输出（通常是未归一化的 logits）转换为概率分布。</li>
<li><strong>交叉熵损失</strong>：计算模型预测的概率分布与真实标签之间的差异。</li>
</ul>
<h4 id="交叉熵损失和均方误差损失对比分析"><a href="#交叉熵损失和均方误差损失对比分析" class="headerlink" title="交叉熵损失和均方误差损失对比分析"></a>交叉熵损失和均方误差损失对比分析</h4><p>均方误差</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/39756/6.png" style="zoom:50%;">

<ul>
<li>cross_entropy 具有 rmse 不具有的优点：避免学习速率降低的情况。(注意这种效果仅限于输出层，隐藏层的学习速率与其使用的激活函数密切相关。)</li>
<li>主要原因是逻辑回归配合 MSE 损失函数时，采用梯度下降法进行学习时，会出现模型一开始训练时，学习速率非常慢的情况（MSE 损失函数）</li>
<li>下面这个更清晰<br><a href="https://zhuanlan.zhihu.com/p/35709485">https://zhuanlan.zhihu.com/p/35709485</a>  <strong>交叉熵损失 更能捕捉到预测效果的差异</strong></li>
</ul>
<p>举例：模型预测输出：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/39756/7.png" style="zoom:50%;">

<p>均方误差损失的结果：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/39756/8.png" style="zoom:50%;">

<p>交叉熵损失的结果：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/39756/9.png" style="zoom:50%;">

<ul>
<li>使用逻辑函数得到概率，并结合交叉熵当损失函数时，在模型效果差的时候学习速度比较快，在模型效果好的时候学习速度变慢。</li>
<li><strong>均方损失：假设误差是正态分布，适用于线性的输出(如回归问题)，特点是对于与真实结果差别越大，则惩罚力度越大，这并不适用于分类问题</strong></li>
<li><strong>交叉熵损失：假设误差是二值分布，可以视为预测概率分布和真实概率分布的相似程度。在分类问题中有良好的应用。</strong></li>
</ul>
<h2 id="w的初始值分布"><a href="#w的初始值分布" class="headerlink" title="w的初始值分布"></a>w的初始值分布</h2><p><code>Xavier Normal</code>（也称作 <strong>Glorot Normal</strong>）是 <strong>权重初始化</strong> 的一种方法，旨在避免深度神经网络中训练过程中的梯度消失和梯度爆炸问题。</p>
<p>在训练深度神经网络时，如果权重初始化得不好，可能会导致训练难以进行。特别是在网络层数很深的情况下，梯度在反向传播时可能会变得非常小（梯度消失）或非常大（梯度爆炸），这会导致训练过程不稳定或收敛困难。</p>
<p><strong>Xavier 初始化</strong>（也叫 <strong>Glorot 初始化</strong>）是为了解决这个问题提出的一种权重初始化方法，尤其适用于 <strong>sigmoid</strong> 和 <strong>tanh</strong> 等激活函数的神经网络。<code>Xavier Normal</code> 是 <strong>正态分布</strong>（也叫高斯分布）初始化方法的一种。它的关键思想是，<strong>根据神经网络的输入和输出层的大小来决定初始化权重的标准差，从而保持信号在网络中的传播不会太大或太小。</strong></p>
<h4 id="公式："><a href="#公式：" class="headerlink" title="公式："></a>公式：</h4><p>对于层 ll 的权重初始化，<code>Xavier Normal</code> 的标准差（σσ）是根据输入单元数（ninnin）和输出单元数（noutnout）来计算的，标准差的公式如下：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/39756/10.png" style="zoom:60%;">

<p>其中：</p>
<ul>
<li>Nin 是当前层的输入神经元数量。</li>
<li>Nout 是当前层的输出神经元数量。</li>
</ul>
<p>因此，每一层的权重矩阵会从一个均值为 0，标准差为 σ 的 <strong>正态分布</strong> 中随机采样。</p>
<p>比如 784+300， 就是 np.sqrt(2/1084)</p>
<h3 id="为什么是这种初始化方法？"><a href="#为什么是这种初始化方法？" class="headerlink" title="为什么是这种初始化方法？"></a><strong>为什么是这种初始化方法？</strong></h3><p>Xavier 初始化方法的目标是让每一层的 <strong>输入信号</strong> 和 <strong>梯度信号</strong> 都能通过网络正常传播而不会衰减（梯度消失）或爆炸（梯度爆炸）。</p>
<h4 id="a-激活值的分布："><a href="#a-激活值的分布：" class="headerlink" title="a. 激活值的分布："></a>a. <strong>激活值的分布</strong>：</h4><p>如果初始化权重过大，经过激活函数（如 <code>sigmoid</code> 或 <code>tanh</code>）的输出可能会饱和（即接近 0 或 1），导致梯度接近于 0，这就会导致梯度消失问题。反之，如果权重过小，信号在通过网络时会过度缩小，可能导致训练非常缓慢，甚至无法收敛。</p>
<p>Xavier 初始化通过调节权重的标准差，使得每层的输入和输出信号在初始化时有适当的方差，从而避免了过度压缩或爆炸的情况。</p>
<h4 id="b-避免梯度消失或爆炸："><a href="#b-避免梯度消失或爆炸：" class="headerlink" title="b. 避免梯度消失或爆炸："></a>b. <strong>避免梯度消失或爆炸</strong>：</h4><ul>
<li>通过选择适当的初始化标准差，Xavier 方法帮助保持梯度在网络中传播时不会迅速消失或变得太大，尤其在深层网络中非常重要。</li>
<li>在激活函数为 <code>sigmoid</code> 或 <code>tanh</code> 时，Xavier 初始化能使得每层的激活值分布保持在合理的范围内，从而避免梯度消失问题。</li>
</ul>
<h3 id="如何在-PyTorch-中使用-Xavier-Normal-初始化："><a href="#如何在-PyTorch-中使用-Xavier-Normal-初始化：" class="headerlink" title="如何在 PyTorch 中使用 Xavier Normal 初始化："></a><strong>如何在 PyTorch 中使用 Xavier Normal 初始化：</strong></h3><p>在 PyTorch 中，你可以使用 <code>torch.nn.init.xavier_normal_()</code> 来应用 Xavier Normal 初始化。该方法会将张量的权重初始化为符合 Xavier Normal 的分布。</p>
<p>例如，初始化一个卷积层或全连接层的权重：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.init <span class="keyword">as</span> init</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们有一个卷积层</span></span><br><span class="line">conv = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 Xavier Normal 初始化卷积层的权重</span></span><br><span class="line">init.xavier_normal_(conv.weight)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果有偏置项，一般初始化为零</span></span><br><span class="line"><span class="keyword">if</span> conv.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    init.zeros_(conv.bias)</span><br></pre></td></tr></table></figure>
<h3 id="5-与其他初始化方法的对比："><a href="#5-与其他初始化方法的对比：" class="headerlink" title="5. 与其他初始化方法的对比："></a>5. <strong>与其他初始化方法的对比：</strong></h3><ul>
<li><strong>Xavier Normal</strong>：适用于 <code>tanh</code> 和 <code>sigmoid</code> 激活函数的网络。通过 <code>n_in + n_out</code> 来平衡输入输出的方差，适合解决梯度消失/爆炸问题。</li>
<li><strong>He 初始化</strong>：类似于 Xavier 初始化，但适用于 <strong>ReLU</strong> 激活函数，标准差为 2ninnin2。ReLU 激活函数的输出通常会有更多的零值（因为负数的输出被“抑制”），因此 He 初始化会稍微增大标准差。</li>
<li><strong>均匀初始化</strong>：与 Xavier 相比，均匀分布初始化是从一个均匀分布中采样，而不是正态分布。这个方法通常会导致训练的不稳定，尤其是对于深层网络。</li>
</ul>
<h2 id="深度学习实现分类问题"><a href="#深度学习实现分类问题" class="headerlink" title="深度学习实现分类问题"></a>深度学习实现分类问题</h2><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">print(sys.version_info)</span><br><span class="line"><span class="keyword">for</span> module <span class="keyword">in</span> mpl, np, pd, sklearn, torch:</span><br><span class="line">    print(module.__name__, module.__version__)</span><br><span class="line">    </span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span>  torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">print(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="comment">#from torchvision.transforms import ToTensor</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义数据集的变换</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),<span class="comment"># 转换为tensor，进行归一化</span></span><br><span class="line">    <span class="comment"># transforms.Normalize(mean, std) # 标准化，mean和std是数据集的均值和方差</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># fashion_mnist图像分类数据集，衣服分类，60000张训练图片，10000张测试图片</span></span><br><span class="line">train_ds = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train= <span class="literal">True</span>,</span><br><span class="line">    download=<span class="literal">False</span>,</span><br><span class="line">    transform=transform</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_ds = datasets.FashionMNIST(</span><br><span class="line">    root=<span class="string">&quot;data&quot;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">    transform=transform</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># torchvision 数据集里没有提供训练集和验证集的划分</span></span><br><span class="line"><span class="comment"># 当然也可以用 torch.utils.data.Dataset 实现人为划分</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#计算均值和方差</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_mean_std</span>(<span class="params">ds</span>):</span></span><br><span class="line">    mean = <span class="number">0.</span></span><br><span class="line">    std = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> img, _ <span class="keyword">in</span> ds: <span class="comment"># 遍历每张图片,img.shape=[1,28,28]</span></span><br><span class="line">        mean += img.mean(dim=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        std += img.std(dim=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">    mean /= <span class="built_in">len</span>(ds)</span><br><span class="line">    std /= <span class="built_in">len</span>(ds)</span><br><span class="line">    <span class="keyword">return</span> mean, std</span><br><span class="line">    </span><br><span class="line">print(cal_mean_std(train_ds))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从数据集到dataloader</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_ds, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>) <span class="comment">#batch_size分批，shuffle洗牌</span></span><br><span class="line">val_loader = torch.utils.data.DataLoader(test_ds, batch_size=<span class="number">32</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在PyTorch中，<code>DataLoader</code>是一个迭代器，它封装了数据的加载和预处理过程，使得在训练机器学习模型时可以方便地批量加载数据。<code>DataLoader</code>主要负责以下几个方面：</p>
<ol>
<li><p><strong>批量加载数据</strong>：<code>DataLoader</code>可以将数据集（Dataset）切分为更小的批次（batch），每次迭代提供一小批量数据，而不是单个数据点。这有助于模型学习数据中的统计依赖性，并且可以更高效地利用GPU等硬件的并行计算能力。</p>
</li>
<li><p><strong>数据打乱</strong>：默认情况下，<code>DataLoader</code>会在每个epoch（训练周期）开始时打乱数据的顺序。这有助于模型训练时避免陷入局部最优解，并且可以提高模型的泛化能力。</p>
</li>
<li><p><strong>多线程数据加载</strong>：<code>DataLoader</code>支持多线程（通过参数<code>num_workers</code>）来并行地加载数据，这可以显著减少训练过程中的等待时间，尤其是在处理大规模数据集时。</p>
</li>
<li><p><strong>数据预处理</strong>：<code>DataLoader</code>可以与<code>transforms</code>结合使用，对加载的数据进行预处理，如归一化、标准化、数据增强等操作。</p>
</li>
<li><p><strong>内存管理</strong>：<code>DataLoader</code>负责管理数据的内存使用，确保在训练过程中不会耗尽内存资源。</p>
</li>
<li><p><strong>易用性</strong>：<code>DataLoader</code>提供了一个简单的接口，可以很容易地集成到训练循环中。</p>
</li>
</ol>
<h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNetwork</span>(<span class="params">nn.Module</span>):</span> <span class="comment"># 都是继承自nn.Module</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span> </span><br><span class="line">        <span class="built_in">super</span>().__init__() <span class="comment"># 继承父类的初始化方法，子类有父类的属性</span></span><br><span class="line">        self.flatten = nn.Flatten() <span class="comment"># 展平层</span></span><br><span class="line">        self.linear_relu_stack = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">784</span>, <span class="number">300</span>),  <span class="comment"># in_features=784, out_features=300, 784是输入特征数，300是输出特征数 Linear：全连接层</span></span><br><span class="line">            nn.ReLU(), <span class="comment"># 激活函数</span></span><br><span class="line">            nn.Linear(<span class="number">300</span>, <span class="number">100</span>),<span class="comment">#隐藏层神经元数100</span></span><br><span class="line">            nn.ReLU(), <span class="comment"># 激活函数</span></span><br><span class="line">            nn.Linear(<span class="number">100</span>, <span class="number">10</span>),<span class="comment">#输出层神经元数10</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span>  <span class="comment"># 前向计算 必须实现</span></span><br><span class="line">        <span class="comment"># x.shape [batch size, 1, 28, 28]</span></span><br><span class="line">        x = self.flatten(x)  </span><br><span class="line">        <span class="comment"># 展平后 x.shape [batch size, 784]</span></span><br><span class="line">        logits = self.linear_relu_stack(x)</span><br><span class="line">        <span class="comment"># logits.shape [batch size, 10]</span></span><br><span class="line">        <span class="keyword">return</span> logits <span class="comment">#没有经过softmax,称为logits</span></span><br><span class="line">    </span><br><span class="line">model = NeuralNetwork()</span><br><span class="line">        </span><br></pre></td></tr></table></figure>
<p>模型结构如下：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/39756/11.png" style="zoom:50%;">

<h4 id="model-named-parameters"><a href="#model-named-parameters" class="headerlink" title="model.named_parameters()"></a><strong><code>model.named_parameters()</code></strong></h4><p>这是查看模型中所有 <strong>可训练参数</strong> 的方法。它会返回一个生成器，包含每个参数的名称和形状。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters(): <span class="comment"># 打印模型参数</span></span><br><span class="line">      print(name, param.shape)</span><br></pre></td></tr></table></figure>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/39756/12.png" style="zoom:50%;">

<h4 id="model-state-dict"><a href="#model-state-dict" class="headerlink" title="model.state_dict()"></a><code>model.state_dict()</code></h4><p><code>state_dict()</code> 方法返回一个字典，其中包含模型的所有参数（权重和偏置），以及优化器的状态（如果有的话）。这个字典中的键是参数的名称，值是参数的值（tensor）。包括所有的权重信息，会很复杂</p>
<p><strong>这种方法用于保存模型参数，能看见参数属于模型的哪一部分</strong></p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/39756/13.png" style="zoom:40%;">

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 看看模型参数</span></span><br><span class="line"><span class="built_in">list</span>(model.parameters())  <span class="comment"># 这种方法拿到模型的所有可学习参数,requires_grad=True</span></span><br></pre></td></tr></table></figure>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>pytorch的训练需要自行实现，包括</p>
<ol>
<li>定义损失函数</li>
<li>定义优化器</li>
<li>定义训练步</li>
<li>训练</li>
</ol>
<p>注意： </p>
<ol>
<li>logits = model(datas) 的时候会调用进入到forward，进行前向计算，会生成计算得到的逻辑值logits，</li>
<li>再进入计算损失，使用交叉熵损失计算前会softmax，之后对其生成的概率值进行计算交叉熵</li>
<li>优化器Optimizer需要将模型参数都写入model.parameters()：模型的所有可学习参数,requires_grad=True</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1. 定义损失函数 采用交叉熵损失</span></span><br><span class="line">loss_fct = nn.CrossEntropyLoss() <span class="comment">#内部先做softmax，然后计算交叉熵</span></span><br><span class="line"><span class="comment"># 2. 定义优化器 采用SGD</span></span><br><span class="line"><span class="comment"># Optimizers specified in the torch.optim package,随机梯度下降</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">training</span>(<span class="params">model, train_loader, val_loader, epoch, loss_fct, optimizer, eval_step=<span class="number">500</span></span>):</span></span><br><span class="line">    record_dict = &#123;</span><br><span class="line">        <span class="string">&quot;train&quot;</span>: [],</span><br><span class="line">        <span class="string">&quot;val&quot;</span>: []</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    global_step = <span class="number">0</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">with</span> tqdm(total=epoch * <span class="built_in">len</span>(train_loader)) <span class="keyword">as</span> pbar: <span class="comment"># 进度条 1875*20,60000/32=1875</span></span><br><span class="line">        <span class="keyword">for</span> epoch_id <span class="keyword">in</span> <span class="built_in">range</span>(epoch): <span class="comment"># 训练epoch次</span></span><br><span class="line">            <span class="comment"># training</span></span><br><span class="line">            <span class="keyword">for</span> datas, labels <span class="keyword">in</span> train_loader: <span class="comment">#执行次数是60000/32=1875</span></span><br><span class="line">                datas = datas.to(device) <span class="comment">#datas尺寸是[batch_size,1,28,28]</span></span><br><span class="line">                labels = labels.to(device) <span class="comment">#labels尺寸是[batch_size]</span></span><br><span class="line">                <span class="comment"># 梯度清空</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">                <span class="comment"># 模型前向计算</span></span><br><span class="line">                logits = model(datas) </span><br><span class="line">                <span class="comment"># 计算损失</span></span><br><span class="line">                loss = loss_fct(logits, labels)</span><br><span class="line">                <span class="comment"># 梯度回传，loss.backward()会计算梯度，loss对模型参数求导</span></span><br><span class="line">                loss.backward()</span><br><span class="line">                <span class="comment"># 调整优化器，包括学习率的变动等,优化器的学习率会随着训练的进行而减小，更新w,b</span></span><br><span class="line">                optimizer.step() <span class="comment">#梯度是计算并存储在模型参数的 .grad 属性中，优化器使用这些存储的梯度来更新模型参数</span></span><br><span class="line"></span><br><span class="line">                preds = logits.argmax(axis=-<span class="number">1</span>) <span class="comment"># 训练集预测</span></span><br><span class="line">                acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())   <span class="comment"># 计算准确率，numpy可以</span></span><br><span class="line">                loss = loss.cpu().item() <span class="comment"># 损失转到CPU，item()取值,一个数值</span></span><br><span class="line">                <span class="comment"># record</span></span><br><span class="line">                </span><br><span class="line">                record_dict[<span class="string">&quot;train&quot;</span>].append(&#123;</span><br><span class="line">                    <span class="string">&quot;loss&quot;</span>: loss, <span class="string">&quot;acc&quot;</span>: acc, <span class="string">&quot;step&quot;</span>: global_step</span><br><span class="line">                &#125;) <span class="comment"># 记录训练集信息，每一步的损失，准确率，步数</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># evaluating</span></span><br><span class="line">                <span class="keyword">if</span> global_step % eval_step == <span class="number">0</span>:</span><br><span class="line">                    model.<span class="built_in">eval</span>() <span class="comment"># 进入评估模式</span></span><br><span class="line">                    val_loss, val_acc = evaluating(model, val_loader, loss_fct)</span><br><span class="line">                    record_dict[<span class="string">&quot;val&quot;</span>].append(&#123;</span><br><span class="line">                        <span class="string">&quot;loss&quot;</span>: val_loss, <span class="string">&quot;acc&quot;</span>: val_acc, <span class="string">&quot;step&quot;</span>: global_step</span><br><span class="line">                    &#125;)</span><br><span class="line">                    model.train() <span class="comment"># 进入训练模式</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># udate step</span></span><br><span class="line">                global_step += <span class="number">1</span> <span class="comment"># 全局步数加1</span></span><br><span class="line">                pbar.update(<span class="number">1</span>) <span class="comment"># 更新进度条</span></span><br><span class="line">                pbar.set_postfix(&#123;<span class="string">&quot;epoch&quot;</span>: epoch_id&#125;) <span class="comment"># 设置进度条显示信息</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> record_dict</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">epoch = <span class="number">20</span> <span class="comment">#改为40</span></span><br><span class="line">model = model.to(device)</span><br><span class="line">record = training(model, train_loader, val_loader, epoch, loss_fct, optimizer, eval_step=<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<p><code>loss.backward()</code> 是 PyTorch 中用于计算梯度的方法。 它是在进行反向传播（backpropagation） 过程中的一个关键步骤。 下面是 <code>loss.backward()</code> 方法的主要作用：</p>
<ol>
<li>根据计算图： PyTorch 中的计算图是由前向传播过程中的张量操作构建的。 当调用 loss.backward()` 时， 它会遵循计算图中的连接关系， 从损失节点开始向后传播， 计算每个相关参数的梯度。</li>
<li>梯度计算： <code>loss.backward()</code> 方法会根据链式法则自动计算每个参数的梯度。 它会沿着计算图反向传播梯度， 将梯度值累积到每个参数的 <code>.grad</code> 属性中。</li>
<li>梯度累积： 如果在调用 <code>loss.backward()</code> 前进行了多次前向传播和损失计算， 那么每次调用 <code>loss.backward()</code> 时， 梯度将被累积到参数的 <code>.grad</code> 属性中。 这通常用于在训练过程中使用小批量样本进行梯度更新</li>
<li>参数更新： 在计算完梯度后， 可以使用优化器（如 <code>torch.optim</code> 中的优化器） 来更新模型的参数， 将梯度信息应用于参数更新规则。</li>
</ol>
<p>总而言之， <code>loss.backward()</code> 的作用是根据<strong>计算图和损失函数</strong>， <strong>计算模型参数的梯度</strong>。 这些<br>梯度可以用于更新模型参数， 以便在训练过程中最小化损失函数。</p>
<h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><p>注意：当进行评估时，模型需要先进入评估模式：model.eval()# 进入评估模式</p>
<p>以确保不会进行梯度计算，以及dropout的丢失损失的情况。并且在下一次进入训练模式的时候也需要先进入训练模式：model.train() # 进入训练模式   以开启反向传播</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.no_grad() </span><span class="comment"># 装饰器，禁止反向传播，节省内存</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluating</span>(<span class="params">model, dataloader, loss_fct</span>):</span></span><br><span class="line">    loss_list = [] <span class="comment"># 记录损失</span></span><br><span class="line">    pred_list = [] <span class="comment"># 记录预测</span></span><br><span class="line">    label_list = [] <span class="comment"># 记录标签</span></span><br><span class="line">    <span class="keyword">for</span> datas, labels <span class="keyword">in</span> dataloader:<span class="comment">#10000/32=312</span></span><br><span class="line">        datas = datas.to(device) <span class="comment"># 转到GPU</span></span><br><span class="line">        labels = labels.to(device) <span class="comment"># 转到GPU</span></span><br><span class="line">        <span class="comment"># 前向计算</span></span><br><span class="line">        logits = model(datas)</span><br><span class="line">        loss = loss_fct(logits, labels)         <span class="comment"># 验证集损失</span></span><br><span class="line">        loss_list.append(loss.item()) <span class="comment"># 记录损失</span></span><br><span class="line">        </span><br><span class="line">        preds = logits.argmax(axis=-<span class="number">1</span>)    <span class="comment"># 验证集预测,argmax返回最大值索引</span></span><br><span class="line">        <span class="comment"># print(preds)</span></span><br><span class="line">        pred_list.extend(preds.cpu().numpy().tolist())<span class="comment">#将PyTorch张量转换为NumPy数组。只有当张量在CPU上时，这个转换才是合法的</span></span><br><span class="line">        <span class="comment"># print(preds.cpu().numpy().tolist())</span></span><br><span class="line">        label_list.extend(labels.cpu().numpy().tolist())</span><br><span class="line">        </span><br><span class="line">    acc = accuracy_score(label_list, pred_list) <span class="comment"># 计算准确率</span></span><br><span class="line">    <span class="keyword">return</span> np.mean(loss_list), acc</span><br><span class="line"></span><br><span class="line"><span class="comment"># dataload for evaluating</span></span><br><span class="line">model.<span class="built_in">eval</span>()<span class="comment"># 进入评估模式</span></span><br><span class="line">loss, acc = evaluating(model, val_loader, loss_fct)</span><br><span class="line">print(<span class="string">f&quot;loss:     <span class="subst">&#123;loss:<span class="number">.4</span>f&#125;</span>\naccuracy: <span class="subst">&#123;acc:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="画图"><a href="#画图" class="headerlink" title="画图"></a>画图</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#画线要注意的是损失是不一定在零到1之间的</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learning_curves</span>(<span class="params">record_dict, sample_step=<span class="number">1000</span></span>):</span></span><br><span class="line">    <span class="comment"># build DataFrame</span></span><br><span class="line">    train_df = pd.DataFrame(record_dict[<span class="string">&quot;train&quot;</span>]).set_index(<span class="string">&quot;step&quot;</span>).iloc[::sample_step]</span><br><span class="line">    val_df = pd.DataFrame(record_dict[<span class="string">&quot;val&quot;</span>]).set_index(<span class="string">&quot;step&quot;</span>)</span><br><span class="line">    last_step = train_df.index[-<span class="number">1</span>] <span class="comment"># 最后一步的步数</span></span><br><span class="line">    <span class="comment"># print(train_df.columns)</span></span><br><span class="line">    print(train_df[<span class="string">&#x27;acc&#x27;</span>])</span><br><span class="line">    print(val_df[<span class="string">&#x27;acc&#x27;</span>])</span><br><span class="line">    <span class="comment"># plot</span></span><br><span class="line">    fig_num = <span class="built_in">len</span>(train_df.columns) <span class="comment"># 画几张图,分别是损失和准确率</span></span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>, fig_num, figsize=(<span class="number">5</span> * fig_num, <span class="number">5</span>))</span><br><span class="line">    <span class="keyword">for</span> idx, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_df.columns):</span><br><span class="line">        <span class="comment"># print(train_df[item].values)</span></span><br><span class="line">        axs[idx].plot(train_df.index, train_df[item], label=<span class="string">f&quot;train_<span class="subst">&#123;item&#125;</span>&quot;</span>)</span><br><span class="line">        axs[idx].plot(val_df.index, val_df[item], label=<span class="string">f&quot;val_<span class="subst">&#123;item&#125;</span>&quot;</span>)</span><br><span class="line">        axs[idx].grid() <span class="comment"># 显示网格</span></span><br><span class="line">        axs[idx].legend() <span class="comment"># 显示图例</span></span><br><span class="line">        axs[idx].set_xticks(<span class="built_in">range</span>(<span class="number">0</span>, train_df.index[-<span class="number">1</span>], <span class="number">5000</span>)) <span class="comment"># 设置x轴刻度</span></span><br><span class="line">        axs[idx].set_xticklabels(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: <span class="string">f&quot;<span class="subst">&#123;<span class="built_in">int</span>(x/<span class="number">1000</span>)&#125;</span>k&quot;</span>, <span class="built_in">range</span>(<span class="number">0</span>, last_step, <span class="number">5000</span>))) <span class="comment"># 设置x轴标签</span></span><br><span class="line">        axs[idx].set_xlabel(<span class="string">&quot;step&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot_learning_curves(record)  <span class="comment">#横坐标是 steps</span></span><br></pre></td></tr></table></figure>


<a id="more"></a>

]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
        <tag>softmax</tag>
        <tag>交叉熵</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习-——深度可分离卷积</title>
    <url>/63103.html</url>
    <content><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.2.1/css/hint.min.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a>深度可分离卷积</h2><p>通过引入深度可分离卷积，能很好的减少参数量，但效果下降更少，用于将模型降小。</p>
<p>第一步 depthwise feature：一般卷积网络会在卷积后三个通道求和，但深度可分离卷积不会求和，三个通道分别继续卷积：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/63103/1.png" style="zoom:50%;">

<p>第二步 pointwise feature：使用一个1x1的卷积核，将多通道求和：</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/63103/2.png" style="zoom:50%;">

<h2 id="对比计算量"><a href="#对比计算量" class="headerlink" title="对比计算量"></a>对比计算量</h2><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p><strong>◆ 普通卷积计算量</strong><br>    Dk. Dk.M .N . DF. DF (各项相乘)<br>加法操作， 复杂度小， 忽略掉<br>其中：DKxDK 是输入和卷积核相乘， DF*DF 是滑动次数， 为什么乘以 M？ M 是通道数目， N 是卷积核的数目(可以理解 M 是输入通道数目， N 是输出通道数目)</p>
<p><strong>◆ 深度可分离卷积计算量</strong><br>    ◆深度可分离<br>        DK.DK.M.DF.DF depthwise 计算量<br>    ◆1*1 卷积<br>        M.N.DF.DF pointwise 计算量</p>
<img src= "https://cdn.jsdelivr.net/gh/weilining/img@main/img/loading.gif" data-lazy-src="/63103/3.png" style="zoom:50%;">

<p>1/卷积核数目+1/卷积核 size 的平方</p>
<p><a href="https://www.cnblogs.com/hellcat/p/9726528.html">https://www.cnblogs.com/hellcat/p/9726528.html</a></p>
<p>参数量减少比例 ：Dk x Dk x M+M x N / Dk x Dk x M x N</p>
<h2 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DepthWiseConv2d</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DepthWiseConv2d, self).__init__()<span class="comment">#这里写为super().__init__()，等价的</span></span><br><span class="line">        self.depthwise_conv = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, groups=in_channels, bias=<span class="literal">False</span>) <span class="comment">#groups参数表示一个卷积核的每个通道分别进行运算</span></span><br><span class="line">        self.pointwise_conv = nn.Conv2d(in_channels, out_channels, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, bias=bias)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.depthwise_conv(x)</span><br><span class="line">        x = self.pointwise_conv(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>需要使用<strong>groups参数</strong>表示一个卷积核的每个通道分别进行运算，否则会直接卷积求和。</p>
<p>pointwise_conv：使用是1x1的卷积核</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, activation=<span class="string">&quot;relu&quot;</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CNN, self).__init__()</span><br><span class="line">        self.activation = F.relu <span class="keyword">if</span> activation == <span class="string">&quot;relu&quot;</span> <span class="keyword">else</span> F.selu</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>)</span><br><span class="line">        self.conv2 = DepthWiseConv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv3 = DepthWiseConv2d(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>)</span><br><span class="line">        self.conv4 = DepthWiseConv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>)</span><br><span class="line">        self.conv5 = DepthWiseConv2d(in_channels=<span class="number">64</span>, out_channels=<span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>)</span><br><span class="line">        self.conv6 = DepthWiseConv2d(in_channels=<span class="number">128</span>, out_channels=<span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="string">&quot;same&quot;</span>)</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        <span class="comment"># input shape is (28, 28, 1) so the fc1 layer in_features is 128 * 3 * 3</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">128</span> * <span class="number">3</span> * <span class="number">3</span>, <span class="number">128</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">        self.init_weights()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;使用 xavier 均匀分布来初始化全连接层、卷积层的权重 W&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, (nn.Linear, nn.Conv2d)):</span><br><span class="line">                nn.init.xavier_uniform_(m.weight)</span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    nn.init.zeros_(m.bias)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        act = self.activation</span><br><span class="line">        x = self.pool(act(self.conv2(act(self.conv1(x)))))<span class="comment">#(batch_size，32，14，14)</span></span><br><span class="line">        x = self.pool(act(self.conv4(act(self.conv3(x)))))<span class="comment">#(batch_size，64，7,7)</span></span><br><span class="line">        x = self.pool(act(self.conv6(act(self.conv5(x)))))<span class="comment">#(batch_size，128，3，3)</span></span><br><span class="line">        x = self.flatten(x)<span class="comment">#(batch_size，128，3，3)</span></span><br><span class="line">        x = act(self.fc1(x))<span class="comment">#(batch_size，128)</span></span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx, (key, value) <span class="keyword">in</span> <span class="built_in">enumerate</span>(CNN().named_parameters()):</span><br><span class="line">    print(<span class="string">f&quot;<span class="subst">&#123;key&#125;</span>\tparamerters num: <span class="subst">&#123;np.prod(value.shape)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>收敛速度会比卷积网络更慢</p>
<a id="more"></a>

]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>深度可分离卷积</tag>
      </tags>
  </entry>
</search>
